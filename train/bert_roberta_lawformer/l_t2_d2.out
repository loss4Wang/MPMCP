01/22/2024 00:17:43 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
01/22/2024 00:17:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=7e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/runs/Jan22_00-17-41_tony-4,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=200.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
01/22/2024 00:17:43 - INFO - __main__ - load a local file for train: /local/xiaowang/LJP Task/Data/Datasets/train_data2.csv
01/22/2024 00:17:43 - INFO - __main__ - load a local file for validation: /local/xiaowang/LJP Task/Data/Datasets/val_data2.csv
01/22/2024 00:17:43 - INFO - __main__ - load a local file for test: /local/xiaowang/LJP Task/Data/Datasets/test_data2.csv
Using custom data configuration default-ff3709bd4e83e3e3
01/22/2024 00:17:44 - INFO - datasets.builder - Using custom data configuration default-ff3709bd4e83e3e3
Loading Dataset Infos from /local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/datasets/packaged_modules/csv
01/22/2024 00:17:44 - INFO - datasets.info - Loading Dataset Infos from /local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/datasets/packaged_modules/csv
Generating dataset csv (/home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
01/22/2024 00:17:44 - INFO - datasets.builder - Generating dataset csv (/home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
Downloading and preparing dataset csv/default to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...
01/22/2024 00:17:44 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 26269.13it/s]
Downloading took 0.0 min
01/22/2024 00:17:44 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
01/22/2024 00:17:44 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2183.78it/s]
Generating train split
01/22/2024 00:17:44 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4000 examples [00:01, 2542.32 examples/s]Generating train split: 4000 examples [00:01, 2527.98 examples/s]
Generating validation split
01/22/2024 00:17:45 - INFO - datasets.builder - Generating validation split
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 500 examples [00:00, 2680.31 examples/s]Generating validation split: 500 examples [00:00, 2653.65 examples/s]
Generating test split
01/22/2024 00:17:46 - INFO - datasets.builder - Generating test split
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 2128.44 examples/s]Generating test split: 500 examples [00:00, 2113.42 examples/s]
Unable to verify splits sizes.
01/22/2024 00:17:46 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.
01/22/2024 00:17:46 - INFO - datasets.builder - Dataset csv downloaded and prepared to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.
Map:   0%|          | 0/4000 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-d2efeab5193485a8.arrow
01/22/2024 00:17:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-d2efeab5193485a8.arrow
Map:  25%|██▌       | 1000/4000 [00:00<00:00, 6591.54 examples/s]Map:  50%|█████     | 2000/4000 [00:00<00:00, 6835.42 examples/s]Map:  75%|███████▌  | 3000/4000 [00:00<00:00, 4503.45 examples/s]Map: 100%|██████████| 4000/4000 [00:00<00:00, 5188.92 examples/s]Map: 100%|██████████| 4000/4000 [00:00<00:00, 5075.36 examples/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-fe4162ca3f1135e6.arrow
01/22/2024 00:17:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-fe4162ca3f1135e6.arrow
Map: 100%|██████████| 500/500 [00:00<00:00, 4992.86 examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 4872.12 examples/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-a26855df202e702d.arrow
01/22/2024 00:17:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-a26855df202e702d.arrow
Map: 100%|██████████| 500/500 [00:00<00:00, 6696.06 examples/s]
01/22/2024 00:17:47 - WARNING - __main__ - Labels {"['抢劫', '盗窃', '寻衅滋事']", "['诈骗', '强奸', '抢劫']", "['贪污', '挪用资金']", "['非法吸收公众存款', '故意伤害']", "['强迫交易', '妨害公务']", "['容留他人吸毒', '妨害公务']", "['金融凭证诈骗', '诈骗']", "['抢劫', '招摇撞骗', '妨害公务']", "['窝藏、包庇', '诈骗']", "['非法处置查封、扣押、冻结的财产', '危险驾驶']", "['敲诈勒索', '寻衅滋事', '盗窃', '诈骗']", "['走私、贩卖、运输、制造毒品', '引诱、容留、介绍卖淫']", "['非法持有毒品', '污染环境']", "['非法拘禁', '提供伪造、变造的出入境证件']", "['打击报复证人', '寻衅滋事']", "['妨害公务', '阻碍军人执行职务']", "['抢劫', '盗伐林木']", "['敲诈勒索', '非法持有毒品']", "['拒不支付劳动报酬', '危险驾驶']", "['抢劫', '强制猥亵、侮辱妇女']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '非法拘禁']", "['帮助信息网络犯罪活动', '侵犯公民个人信息']", "['抢夺', '招摇撞骗']", "['非法狩猎', '非法持有、私藏枪支、弹药']", "['非法收购、运输、出售珍贵、濒危野生动物、珍贵、濒危野生动物制品', '非法经营']", "['赌博', '信用卡诈骗']", "['盗窃', '诈骗', '危险驾驶']", "['投放危险物质', '寻衅滋事']", "['非法经营', '生产、销售假药']", "['非国家工作人员受贿', '对非国家工作人员行贿']", "['非法拘禁', '寻衅滋事', '强奸']", "['非法持有、私藏枪支、弹药', '寻衅滋事']", "['组织、领导传销活动', '抢劫', '非法拘禁']", "['敲诈勒索', '合同诈骗']", "['行贿', '非国家工作人员受贿']", "['伪造、变造、买卖身份证件', '伪造、变造、买卖国家机关公文、证件、印章']", "['故意伤害', '走私、贩卖、运输、制造毒品', '容留他人吸毒']", "['非法持有、私藏枪支、弹药', '窝藏、包庇']", "['诈骗', '伪证']", "['招摇撞骗', '危险驾驶']", "['寻衅滋事', '聚众斗殴', '走私、贩卖、运输、制造毒品', '容留他人吸毒']", "['敲诈勒索', '抢劫']", "['强奸', '强制猥亵、侮辱']", "['非法收购、运输、出售珍贵、濒危野生动物、珍贵、濒危野生动物制品', '掩饰、隐瞒犯罪所得、犯罪所得收益']", "['组织卖淫', '引诱、容留、介绍卖淫']", "['强迫交易', '职务侵占', '敲诈勒索']", "['诈骗', '盗窃', '伪造公司、企业、事业单位、人民团体印章']", "['贪污', '巨额财产来源不明']", "['假冒注册商标', '伪造公司、企业、事业单位、人民团体印章']", "['非法持有、私藏枪支、弹药', '非法占用农用地']", "['聚众斗殴', '非法拘禁', '故意伤害']", "['受贿', '高利转贷']", "['盗窃', '诈骗', '敲诈勒索']", "['职务侵占', '合同诈骗', '诈骗']", "['组织、领导、参加黑社会性质组织', '协助组织卖淫']", "['诈骗', '非法占用农用地', '拒不执行判决、裁定']", "['抢夺', '抢劫', '盗窃', '脱逃']", "['骗取贷款、票据承兑、金融票证', '拒不支付劳动报酬']", "['组织、领导、参加黑社会性质组织', '聚众斗殴', '敲诈勒索']", "['贪污', '职务侵占', '非国家工作人员受贿']", "['强奸', '妨害作证']", "['重婚', '诈骗']", "['抢劫', '破坏电力设备']", "['帮助毁灭、伪造证据', '掩饰、隐瞒犯罪所得、犯罪所得收益']", "['非法出售发票', '非法制造、出售非法制造的发票']", "['滥伐林木', '掩饰、隐瞒犯罪所得、犯罪所得收益']", "['故意伤害', '过失致人重伤']", "['抢劫', '绑架']", "['非法吸收公众存款', '开设赌场']", "['职务侵占', '故意伤害']", "['信用卡诈骗', '盗窃', '诈骗']", "['贷款诈骗', '盗窃', '诈骗']", "['集资诈骗', '非法占用农用地']", "['寻衅滋事', '非法持有毒品']", "['寻衅滋事', '高利转贷']", "['诈骗', '传授犯罪方法']", "['非法占用农用地', '重大责任事故']", "['非国家工作人员受贿', '挪用资金']", "['敲诈勒索', '妨害动植物防疫、检疫']", "['合同诈骗', '贷款诈骗', '使用虚假身份证件、盗用身份证件']", "['开设赌场', '妨害公务']", "['强奸', '寻衅滋事']", "['故意伤害', '重婚']", "['窝藏、转移、隐瞒毒品、毒赃', '容留他人吸毒']", "['骗取贷款、票据承兑、金融票证', '贷款诈骗', '非法占用农用地']", "['受贿', '放纵走私']", "['受贿', '贪污', '国有公司、企业、事业单位人员失职']", "['贪污', '诈骗', '行贿']", "['危险驾驶', '过失致人重伤']", "['生产、销售假药', '生产、销售不符合安全标准的食品']", "['组织、领导、参加黑社会性质组织', '寻衅滋事', '非法侵入住宅', '故意伤害']", "['贷款诈骗', '诈骗', '挪用公款']"} in validation set but not in training set, adding them to the label list
01/22/2024 00:17:47 - WARNING - __main__ - Labels {"['赌博', '开设赌场']", "['非法拘禁', '受贿', '挪用公款']", "['诈骗', '交通肇事']", "['组织卖淫', '伪造、变造居民身份证']", "['非法转让、倒卖土地使用权', '行贿']", "['组织、领导、参加黑社会性质组织', '聚众斗殴', '故意伤害', '开设赌场']", "['组织、领导传销活动', '非法吸收公众存款']", "['私分国有资产', '挪用公款', '受贿']", "['故意伤害', '故意杀人']", "['强迫交易', '破坏生产经营', '伪造、变造、买卖国家机关公文、证件、印章']", "['故意伤害', '敲诈勒索', '盗窃']", "['制作、复制、出版、贩卖、传播淫秽物品牟利', '开设赌场']", "['非法出售发票', '虚开发票']", "['开设赌场', '寻衅滋事', '强迫交易']", "['走私、贩卖、运输、制造毒品', '容留他人吸毒', '聚众斗殴']", "['妨害公务', '非法持有、私藏枪支、弹药']", "['生产、销售伪劣产品', '敲诈勒索']", "['走私、贩卖、运输、制造毒品', '非法利用信息网络']", "['敲诈勒索', '妨害公务']", "['妨害信用卡管理', '非法拘禁']", "['挪用资金', '交通肇事']", "['非法收购、运输、出售珍贵、濒危野生动物、珍贵、濒危野生动物制品', '非法持有、私藏枪支、弹药']", "['组织卖淫', '开设赌场']", "['非法持有、私藏枪支、弹药', '故意杀人']", "['盗窃', '窃取、收买、非法提供信用卡信息']", "['盗窃', '提供侵入、非法控制计算机信息系统程序、工具']", "['非法经营', '诈骗']", "['容留他人吸毒', '非法拘禁']", "['非法采矿', '非法占用农用地']", "['受贿', '滥用职权', '玩忽职守']", "['职务侵占', '寻衅滋事']", "['破坏电力设备', '非法侵入住宅', '盗窃']", "['合同诈骗', '妨害公务', '伪造、变造、买卖国家机关公文、证件、印章']", "['包庇、纵容黑社会性质组织', '介绍贿赂']", "['非法拘禁', '污染环境']", "['故意杀人', '开设赌场']", "['故意杀人', '故意伤害', '非法拘禁']", "['保险诈骗', '合同诈骗']", "['行贿', '受贿', '虚开发票']", "['信用卡诈骗', '贷款诈骗']", "['伪造、变造金融票证', '诈骗']", "['故意伤害', '敲诈勒索', '故意毁坏财物']", "['寻衅滋事', '聚众斗殴', '故意伤害']", "['妨害信用卡管理', '开设赌场']", "['盗伐林木', '破坏生产经营']", "['引诱、容留、介绍卖淫', '危险驾驶']", "['组织、领导、参加黑社会性质组织', '寻衅滋事', '抢劫', '脱逃']", "['走私废物', '虚开增值税专用发票、用于骗取出口退税、抵扣税款发票']", "['故意伤害', '抢夺']", "['故意杀人', '盗窃']", "['组织、利用会道门、邪教组织、利用迷信破坏法律实施', '诈骗']", "['寻衅滋事', '危险驾驶', '妨害公务']", "['职务侵占', '虚开发票']", "['危险驾驶', '寻衅滋事', '诈骗']", "['非法拘禁', '诬告陷害']", "['抢劫', '非法持有毒品']", "['聚众斗殴', '非法拘禁', '非法侵入住宅']", "['徇私枉法', '滥用职权']", "['故意伤害', '职务侵占', '挪用资金']", "['组织、领导传销活动', '销售假冒注册商标的商品']", "['强迫交易', '聚众扰乱社会秩序']", "['职务侵占', '对非国家工作人员行贿', '寻衅滋事']", "['窝藏、包庇', '开设赌场']", "['对非国家工作人员行贿', '行贿']", "['非法狩猎', '非法猎捕、杀害珍贵、濒危野生动物']", "['徇私枉法', '贪污', '挪用公款']", "['合同诈骗', '票据诈骗']", "['诈骗', '挪用公款']", "['聚众斗殴', '故意毁坏财物', '窝藏、包庇']", "['非法拘禁', '盗窃']", "['受贿', '非法持有、私藏枪支、弹药', '徇私枉法']", "['高利转贷', '滥用职权']", "['危险驾驶', '寻衅滋事', '妨害公务']", "['敲诈勒索', '放火', '寻衅滋事']", "['敲诈勒索', '开设赌场']", "['破坏电力设备', '破坏广播电视设施、公用电信设施']", "['组织、领导、参加黑社会性质组织', '非法拘禁', '寻衅滋事', '故意毁坏财物', '赌博', '强迫交易', '聚众斗殴']"} in test set but not in training set, adding them to the label list
config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]config.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 9.98MB/s]
[INFO|configuration_utils.py:717] 2024-01-22 00:17:47,615 >> loading configuration file config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/config.json
[INFO|configuration_utils.py:777] 2024-01-22 00:17:47,624 >> Model config LongformerConfig {
  "_name_or_path": "thunlp/Lawformer",
  "architectures": [
    "LongformerForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "finetuning_task": "text-classification",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95",
    "96": "LABEL_96",
    "97": "LABEL_97",
    "98": "LABEL_98",
    "99": "LABEL_99",
    "100": "LABEL_100",
    "101": "LABEL_101",
    "102": "LABEL_102",
    "103": "LABEL_103",
    "104": "LABEL_104",
    "105": "LABEL_105",
    "106": "LABEL_106",
    "107": "LABEL_107",
    "108": "LABEL_108",
    "109": "LABEL_109",
    "110": "LABEL_110",
    "111": "LABEL_111",
    "112": "LABEL_112",
    "113": "LABEL_113",
    "114": "LABEL_114",
    "115": "LABEL_115",
    "116": "LABEL_116",
    "117": "LABEL_117",
    "118": "LABEL_118",
    "119": "LABEL_119",
    "120": "LABEL_120",
    "121": "LABEL_121",
    "122": "LABEL_122",
    "123": "LABEL_123",
    "124": "LABEL_124",
    "125": "LABEL_125",
    "126": "LABEL_126",
    "127": "LABEL_127",
    "128": "LABEL_128",
    "129": "LABEL_129",
    "130": "LABEL_130",
    "131": "LABEL_131",
    "132": "LABEL_132",
    "133": "LABEL_133",
    "134": "LABEL_134",
    "135": "LABEL_135",
    "136": "LABEL_136",
    "137": "LABEL_137",
    "138": "LABEL_138",
    "139": "LABEL_139",
    "140": "LABEL_140",
    "141": "LABEL_141",
    "142": "LABEL_142",
    "143": "LABEL_143",
    "144": "LABEL_144",
    "145": "LABEL_145",
    "146": "LABEL_146",
    "147": "LABEL_147",
    "148": "LABEL_148",
    "149": "LABEL_149",
    "150": "LABEL_150",
    "151": "LABEL_151",
    "152": "LABEL_152",
    "153": "LABEL_153",
    "154": "LABEL_154",
    "155": "LABEL_155",
    "156": "LABEL_156",
    "157": "LABEL_157",
    "158": "LABEL_158",
    "159": "LABEL_159",
    "160": "LABEL_160",
    "161": "LABEL_161",
    "162": "LABEL_162",
    "163": "LABEL_163",
    "164": "LABEL_164",
    "165": "LABEL_165",
    "166": "LABEL_166",
    "167": "LABEL_167",
    "168": "LABEL_168",
    "169": "LABEL_169",
    "170": "LABEL_170",
    "171": "LABEL_171",
    "172": "LABEL_172",
    "173": "LABEL_173",
    "174": "LABEL_174",
    "175": "LABEL_175",
    "176": "LABEL_176",
    "177": "LABEL_177",
    "178": "LABEL_178",
    "179": "LABEL_179",
    "180": "LABEL_180",
    "181": "LABEL_181",
    "182": "LABEL_182",
    "183": "LABEL_183",
    "184": "LABEL_184",
    "185": "LABEL_185",
    "186": "LABEL_186",
    "187": "LABEL_187",
    "188": "LABEL_188",
    "189": "LABEL_189",
    "190": "LABEL_190",
    "191": "LABEL_191",
    "192": "LABEL_192",
    "193": "LABEL_193",
    "194": "LABEL_194",
    "195": "LABEL_195",
    "196": "LABEL_196",
    "197": "LABEL_197",
    "198": "LABEL_198",
    "199": "LABEL_199",
    "200": "LABEL_200",
    "201": "LABEL_201",
    "202": "LABEL_202",
    "203": "LABEL_203",
    "204": "LABEL_204",
    "205": "LABEL_205",
    "206": "LABEL_206",
    "207": "LABEL_207",
    "208": "LABEL_208",
    "209": "LABEL_209",
    "210": "LABEL_210",
    "211": "LABEL_211",
    "212": "LABEL_212",
    "213": "LABEL_213",
    "214": "LABEL_214",
    "215": "LABEL_215",
    "216": "LABEL_216",
    "217": "LABEL_217",
    "218": "LABEL_218",
    "219": "LABEL_219",
    "220": "LABEL_220",
    "221": "LABEL_221",
    "222": "LABEL_222",
    "223": "LABEL_223",
    "224": "LABEL_224",
    "225": "LABEL_225",
    "226": "LABEL_226",
    "227": "LABEL_227",
    "228": "LABEL_228",
    "229": "LABEL_229",
    "230": "LABEL_230",
    "231": "LABEL_231",
    "232": "LABEL_232",
    "233": "LABEL_233",
    "234": "LABEL_234",
    "235": "LABEL_235",
    "236": "LABEL_236",
    "237": "LABEL_237",
    "238": "LABEL_238",
    "239": "LABEL_239",
    "240": "LABEL_240",
    "241": "LABEL_241",
    "242": "LABEL_242",
    "243": "LABEL_243",
    "244": "LABEL_244",
    "245": "LABEL_245",
    "246": "LABEL_246",
    "247": "LABEL_247",
    "248": "LABEL_248",
    "249": "LABEL_249",
    "250": "LABEL_250",
    "251": "LABEL_251",
    "252": "LABEL_252",
    "253": "LABEL_253",
    "254": "LABEL_254",
    "255": "LABEL_255",
    "256": "LABEL_256",
    "257": "LABEL_257",
    "258": "LABEL_258",
    "259": "LABEL_259",
    "260": "LABEL_260",
    "261": "LABEL_261",
    "262": "LABEL_262",
    "263": "LABEL_263",
    "264": "LABEL_264",
    "265": "LABEL_265",
    "266": "LABEL_266",
    "267": "LABEL_267",
    "268": "LABEL_268",
    "269": "LABEL_269",
    "270": "LABEL_270",
    "271": "LABEL_271",
    "272": "LABEL_272",
    "273": "LABEL_273",
    "274": "LABEL_274",
    "275": "LABEL_275",
    "276": "LABEL_276",
    "277": "LABEL_277",
    "278": "LABEL_278",
    "279": "LABEL_279",
    "280": "LABEL_280",
    "281": "LABEL_281",
    "282": "LABEL_282",
    "283": "LABEL_283",
    "284": "LABEL_284",
    "285": "LABEL_285",
    "286": "LABEL_286",
    "287": "LABEL_287",
    "288": "LABEL_288",
    "289": "LABEL_289",
    "290": "LABEL_290",
    "291": "LABEL_291",
    "292": "LABEL_292",
    "293": "LABEL_293",
    "294": "LABEL_294",
    "295": "LABEL_295",
    "296": "LABEL_296",
    "297": "LABEL_297",
    "298": "LABEL_298",
    "299": "LABEL_299",
    "300": "LABEL_300",
    "301": "LABEL_301",
    "302": "LABEL_302",
    "303": "LABEL_303",
    "304": "LABEL_304",
    "305": "LABEL_305",
    "306": "LABEL_306",
    "307": "LABEL_307",
    "308": "LABEL_308",
    "309": "LABEL_309",
    "310": "LABEL_310",
    "311": "LABEL_311",
    "312": "LABEL_312",
    "313": "LABEL_313",
    "314": "LABEL_314",
    "315": "LABEL_315",
    "316": "LABEL_316",
    "317": "LABEL_317",
    "318": "LABEL_318",
    "319": "LABEL_319",
    "320": "LABEL_320",
    "321": "LABEL_321",
    "322": "LABEL_322",
    "323": "LABEL_323",
    "324": "LABEL_324",
    "325": "LABEL_325",
    "326": "LABEL_326",
    "327": "LABEL_327",
    "328": "LABEL_328",
    "329": "LABEL_329",
    "330": "LABEL_330",
    "331": "LABEL_331",
    "332": "LABEL_332",
    "333": "LABEL_333",
    "334": "LABEL_334",
    "335": "LABEL_335",
    "336": "LABEL_336",
    "337": "LABEL_337",
    "338": "LABEL_338",
    "339": "LABEL_339",
    "340": "LABEL_340",
    "341": "LABEL_341",
    "342": "LABEL_342",
    "343": "LABEL_343",
    "344": "LABEL_344",
    "345": "LABEL_345",
    "346": "LABEL_346",
    "347": "LABEL_347",
    "348": "LABEL_348",
    "349": "LABEL_349",
    "350": "LABEL_350",
    "351": "LABEL_351",
    "352": "LABEL_352",
    "353": "LABEL_353",
    "354": "LABEL_354",
    "355": "LABEL_355",
    "356": "LABEL_356",
    "357": "LABEL_357",
    "358": "LABEL_358",
    "359": "LABEL_359",
    "360": "LABEL_360",
    "361": "LABEL_361",
    "362": "LABEL_362",
    "363": "LABEL_363",
    "364": "LABEL_364",
    "365": "LABEL_365",
    "366": "LABEL_366",
    "367": "LABEL_367",
    "368": "LABEL_368",
    "369": "LABEL_369",
    "370": "LABEL_370",
    "371": "LABEL_371",
    "372": "LABEL_372",
    "373": "LABEL_373",
    "374": "LABEL_374",
    "375": "LABEL_375",
    "376": "LABEL_376",
    "377": "LABEL_377",
    "378": "LABEL_378",
    "379": "LABEL_379",
    "380": "LABEL_380",
    "381": "LABEL_381",
    "382": "LABEL_382",
    "383": "LABEL_383",
    "384": "LABEL_384",
    "385": "LABEL_385",
    "386": "LABEL_386",
    "387": "LABEL_387",
    "388": "LABEL_388",
    "389": "LABEL_389",
    "390": "LABEL_390",
    "391": "LABEL_391",
    "392": "LABEL_392",
    "393": "LABEL_393",
    "394": "LABEL_394",
    "395": "LABEL_395",
    "396": "LABEL_396",
    "397": "LABEL_397",
    "398": "LABEL_398",
    "399": "LABEL_399",
    "400": "LABEL_400",
    "401": "LABEL_401",
    "402": "LABEL_402",
    "403": "LABEL_403",
    "404": "LABEL_404",
    "405": "LABEL_405",
    "406": "LABEL_406",
    "407": "LABEL_407",
    "408": "LABEL_408",
    "409": "LABEL_409",
    "410": "LABEL_410",
    "411": "LABEL_411",
    "412": "LABEL_412",
    "413": "LABEL_413",
    "414": "LABEL_414",
    "415": "LABEL_415",
    "416": "LABEL_416",
    "417": "LABEL_417",
    "418": "LABEL_418",
    "419": "LABEL_419",
    "420": "LABEL_420",
    "421": "LABEL_421",
    "422": "LABEL_422",
    "423": "LABEL_423",
    "424": "LABEL_424",
    "425": "LABEL_425",
    "426": "LABEL_426",
    "427": "LABEL_427",
    "428": "LABEL_428",
    "429": "LABEL_429",
    "430": "LABEL_430",
    "431": "LABEL_431",
    "432": "LABEL_432",
    "433": "LABEL_433",
    "434": "LABEL_434",
    "435": "LABEL_435",
    "436": "LABEL_436",
    "437": "LABEL_437",
    "438": "LABEL_438",
    "439": "LABEL_439",
    "440": "LABEL_440",
    "441": "LABEL_441",
    "442": "LABEL_442",
    "443": "LABEL_443",
    "444": "LABEL_444",
    "445": "LABEL_445",
    "446": "LABEL_446",
    "447": "LABEL_447",
    "448": "LABEL_448",
    "449": "LABEL_449",
    "450": "LABEL_450",
    "451": "LABEL_451",
    "452": "LABEL_452",
    "453": "LABEL_453",
    "454": "LABEL_454",
    "455": "LABEL_455",
    "456": "LABEL_456",
    "457": "LABEL_457",
    "458": "LABEL_458",
    "459": "LABEL_459",
    "460": "LABEL_460",
    "461": "LABEL_461",
    "462": "LABEL_462",
    "463": "LABEL_463",
    "464": "LABEL_464",
    "465": "LABEL_465",
    "466": "LABEL_466",
    "467": "LABEL_467",
    "468": "LABEL_468",
    "469": "LABEL_469",
    "470": "LABEL_470",
    "471": "LABEL_471",
    "472": "LABEL_472",
    "473": "LABEL_473",
    "474": "LABEL_474",
    "475": "LABEL_475",
    "476": "LABEL_476",
    "477": "LABEL_477",
    "478": "LABEL_478",
    "479": "LABEL_479",
    "480": "LABEL_480",
    "481": "LABEL_481",
    "482": "LABEL_482",
    "483": "LABEL_483",
    "484": "LABEL_484",
    "485": "LABEL_485",
    "486": "LABEL_486",
    "487": "LABEL_487",
    "488": "LABEL_488",
    "489": "LABEL_489",
    "490": "LABEL_490",
    "491": "LABEL_491",
    "492": "LABEL_492",
    "493": "LABEL_493",
    "494": "LABEL_494",
    "495": "LABEL_495",
    "496": "LABEL_496",
    "497": "LABEL_497",
    "498": "LABEL_498",
    "499": "LABEL_499",
    "500": "LABEL_500",
    "501": "LABEL_501",
    "502": "LABEL_502",
    "503": "LABEL_503",
    "504": "LABEL_504",
    "505": "LABEL_505",
    "506": "LABEL_506",
    "507": "LABEL_507",
    "508": "LABEL_508",
    "509": "LABEL_509",
    "510": "LABEL_510",
    "511": "LABEL_511",
    "512": "LABEL_512",
    "513": "LABEL_513",
    "514": "LABEL_514",
    "515": "LABEL_515",
    "516": "LABEL_516",
    "517": "LABEL_517",
    "518": "LABEL_518",
    "519": "LABEL_519",
    "520": "LABEL_520",
    "521": "LABEL_521",
    "522": "LABEL_522",
    "523": "LABEL_523",
    "524": "LABEL_524",
    "525": "LABEL_525",
    "526": "LABEL_526",
    "527": "LABEL_527",
    "528": "LABEL_528",
    "529": "LABEL_529",
    "530": "LABEL_530",
    "531": "LABEL_531",
    "532": "LABEL_532",
    "533": "LABEL_533",
    "534": "LABEL_534",
    "535": "LABEL_535",
    "536": "LABEL_536",
    "537": "LABEL_537",
    "538": "LABEL_538",
    "539": "LABEL_539",
    "540": "LABEL_540",
    "541": "LABEL_541",
    "542": "LABEL_542",
    "543": "LABEL_543",
    "544": "LABEL_544",
    "545": "LABEL_545",
    "546": "LABEL_546",
    "547": "LABEL_547",
    "548": "LABEL_548",
    "549": "LABEL_549",
    "550": "LABEL_550",
    "551": "LABEL_551",
    "552": "LABEL_552",
    "553": "LABEL_553",
    "554": "LABEL_554",
    "555": "LABEL_555",
    "556": "LABEL_556",
    "557": "LABEL_557",
    "558": "LABEL_558",
    "559": "LABEL_559",
    "560": "LABEL_560",
    "561": "LABEL_561",
    "562": "LABEL_562",
    "563": "LABEL_563",
    "564": "LABEL_564",
    "565": "LABEL_565",
    "566": "LABEL_566",
    "567": "LABEL_567",
    "568": "LABEL_568",
    "569": "LABEL_569",
    "570": "LABEL_570",
    "571": "LABEL_571",
    "572": "LABEL_572",
    "573": "LABEL_573",
    "574": "LABEL_574",
    "575": "LABEL_575",
    "576": "LABEL_576",
    "577": "LABEL_577",
    "578": "LABEL_578",
    "579": "LABEL_579",
    "580": "LABEL_580",
    "581": "LABEL_581",
    "582": "LABEL_582",
    "583": "LABEL_583",
    "584": "LABEL_584",
    "585": "LABEL_585",
    "586": "LABEL_586",
    "587": "LABEL_587",
    "588": "LABEL_588",
    "589": "LABEL_589",
    "590": "LABEL_590",
    "591": "LABEL_591",
    "592": "LABEL_592",
    "593": "LABEL_593",
    "594": "LABEL_594",
    "595": "LABEL_595",
    "596": "LABEL_596",
    "597": "LABEL_597",
    "598": "LABEL_598",
    "599": "LABEL_599",
    "600": "LABEL_600",
    "601": "LABEL_601",
    "602": "LABEL_602",
    "603": "LABEL_603",
    "604": "LABEL_604",
    "605": "LABEL_605",
    "606": "LABEL_606",
    "607": "LABEL_607",
    "608": "LABEL_608",
    "609": "LABEL_609",
    "610": "LABEL_610",
    "611": "LABEL_611",
    "612": "LABEL_612",
    "613": "LABEL_613",
    "614": "LABEL_614",
    "615": "LABEL_615",
    "616": "LABEL_616",
    "617": "LABEL_617",
    "618": "LABEL_618",
    "619": "LABEL_619",
    "620": "LABEL_620",
    "621": "LABEL_621",
    "622": "LABEL_622",
    "623": "LABEL_623",
    "624": "LABEL_624",
    "625": "LABEL_625",
    "626": "LABEL_626",
    "627": "LABEL_627",
    "628": "LABEL_628",
    "629": "LABEL_629",
    "630": "LABEL_630",
    "631": "LABEL_631",
    "632": "LABEL_632",
    "633": "LABEL_633",
    "634": "LABEL_634",
    "635": "LABEL_635",
    "636": "LABEL_636",
    "637": "LABEL_637",
    "638": "LABEL_638",
    "639": "LABEL_639",
    "640": "LABEL_640",
    "641": "LABEL_641",
    "642": "LABEL_642",
    "643": "LABEL_643",
    "644": "LABEL_644",
    "645": "LABEL_645",
    "646": "LABEL_646",
    "647": "LABEL_647",
    "648": "LABEL_648",
    "649": "LABEL_649",
    "650": "LABEL_650",
    "651": "LABEL_651",
    "652": "LABEL_652",
    "653": "LABEL_653",
    "654": "LABEL_654",
    "655": "LABEL_655",
    "656": "LABEL_656",
    "657": "LABEL_657",
    "658": "LABEL_658",
    "659": "LABEL_659",
    "660": "LABEL_660",
    "661": "LABEL_661",
    "662": "LABEL_662",
    "663": "LABEL_663",
    "664": "LABEL_664",
    "665": "LABEL_665",
    "666": "LABEL_666",
    "667": "LABEL_667",
    "668": "LABEL_668",
    "669": "LABEL_669",
    "670": "LABEL_670",
    "671": "LABEL_671",
    "672": "LABEL_672",
    "673": "LABEL_673",
    "674": "LABEL_674",
    "675": "LABEL_675",
    "676": "LABEL_676",
    "677": "LABEL_677",
    "678": "LABEL_678",
    "679": "LABEL_679",
    "680": "LABEL_680",
    "681": "LABEL_681",
    "682": "LABEL_682",
    "683": "LABEL_683",
    "684": "LABEL_684",
    "685": "LABEL_685",
    "686": "LABEL_686",
    "687": "LABEL_687",
    "688": "LABEL_688",
    "689": "LABEL_689",
    "690": "LABEL_690",
    "691": "LABEL_691",
    "692": "LABEL_692",
    "693": "LABEL_693",
    "694": "LABEL_694",
    "695": "LABEL_695",
    "696": "LABEL_696",
    "697": "LABEL_697",
    "698": "LABEL_698",
    "699": "LABEL_699",
    "700": "LABEL_700",
    "701": "LABEL_701",
    "702": "LABEL_702",
    "703": "LABEL_703",
    "704": "LABEL_704",
    "705": "LABEL_705",
    "706": "LABEL_706",
    "707": "LABEL_707",
    "708": "LABEL_708",
    "709": "LABEL_709",
    "710": "LABEL_710",
    "711": "LABEL_711",
    "712": "LABEL_712",
    "713": "LABEL_713",
    "714": "LABEL_714",
    "715": "LABEL_715",
    "716": "LABEL_716",
    "717": "LABEL_717",
    "718": "LABEL_718",
    "719": "LABEL_719",
    "720": "LABEL_720",
    "721": "LABEL_721",
    "722": "LABEL_722",
    "723": "LABEL_723",
    "724": "LABEL_724",
    "725": "LABEL_725",
    "726": "LABEL_726",
    "727": "LABEL_727",
    "728": "LABEL_728",
    "729": "LABEL_729",
    "730": "LABEL_730",
    "731": "LABEL_731",
    "732": "LABEL_732",
    "733": "LABEL_733",
    "734": "LABEL_734",
    "735": "LABEL_735",
    "736": "LABEL_736",
    "737": "LABEL_737",
    "738": "LABEL_738",
    "739": "LABEL_739",
    "740": "LABEL_740",
    "741": "LABEL_741",
    "742": "LABEL_742",
    "743": "LABEL_743",
    "744": "LABEL_744",
    "745": "LABEL_745",
    "746": "LABEL_746",
    "747": "LABEL_747",
    "748": "LABEL_748",
    "749": "LABEL_749",
    "750": "LABEL_750",
    "751": "LABEL_751",
    "752": "LABEL_752",
    "753": "LABEL_753",
    "754": "LABEL_754",
    "755": "LABEL_755",
    "756": "LABEL_756",
    "757": "LABEL_757",
    "758": "LABEL_758",
    "759": "LABEL_759",
    "760": "LABEL_760",
    "761": "LABEL_761",
    "762": "LABEL_762",
    "763": "LABEL_763",
    "764": "LABEL_764",
    "765": "LABEL_765",
    "766": "LABEL_766",
    "767": "LABEL_767",
    "768": "LABEL_768",
    "769": "LABEL_769",
    "770": "LABEL_770",
    "771": "LABEL_771",
    "772": "LABEL_772",
    "773": "LABEL_773",
    "774": "LABEL_774",
    "775": "LABEL_775",
    "776": "LABEL_776",
    "777": "LABEL_777",
    "778": "LABEL_778",
    "779": "LABEL_779",
    "780": "LABEL_780",
    "781": "LABEL_781",
    "782": "LABEL_782",
    "783": "LABEL_783",
    "784": "LABEL_784",
    "785": "LABEL_785",
    "786": "LABEL_786",
    "787": "LABEL_787",
    "788": "LABEL_788",
    "789": "LABEL_789",
    "790": "LABEL_790",
    "791": "LABEL_791",
    "792": "LABEL_792",
    "793": "LABEL_793",
    "794": "LABEL_794",
    "795": "LABEL_795",
    "796": "LABEL_796",
    "797": "LABEL_797",
    "798": "LABEL_798",
    "799": "LABEL_799",
    "800": "LABEL_800",
    "801": "LABEL_801",
    "802": "LABEL_802",
    "803": "LABEL_803",
    "804": "LABEL_804",
    "805": "LABEL_805",
    "806": "LABEL_806",
    "807": "LABEL_807",
    "808": "LABEL_808",
    "809": "LABEL_809",
    "810": "LABEL_810",
    "811": "LABEL_811",
    "812": "LABEL_812",
    "813": "LABEL_813",
    "814": "LABEL_814",
    "815": "LABEL_815",
    "816": "LABEL_816",
    "817": "LABEL_817",
    "818": "LABEL_818",
    "819": "LABEL_819",
    "820": "LABEL_820",
    "821": "LABEL_821",
    "822": "LABEL_822",
    "823": "LABEL_823",
    "824": "LABEL_824",
    "825": "LABEL_825",
    "826": "LABEL_826",
    "827": "LABEL_827",
    "828": "LABEL_828",
    "829": "LABEL_829",
    "830": "LABEL_830",
    "831": "LABEL_831",
    "832": "LABEL_832",
    "833": "LABEL_833",
    "834": "LABEL_834",
    "835": "LABEL_835",
    "836": "LABEL_836",
    "837": "LABEL_837",
    "838": "LABEL_838",
    "839": "LABEL_839",
    "840": "LABEL_840",
    "841": "LABEL_841",
    "842": "LABEL_842",
    "843": "LABEL_843",
    "844": "LABEL_844",
    "845": "LABEL_845",
    "846": "LABEL_846",
    "847": "LABEL_847",
    "848": "LABEL_848",
    "849": "LABEL_849",
    "850": "LABEL_850",
    "851": "LABEL_851",
    "852": "LABEL_852",
    "853": "LABEL_853",
    "854": "LABEL_854",
    "855": "LABEL_855",
    "856": "LABEL_856",
    "857": "LABEL_857",
    "858": "LABEL_858",
    "859": "LABEL_859",
    "860": "LABEL_860",
    "861": "LABEL_861",
    "862": "LABEL_862",
    "863": "LABEL_863",
    "864": "LABEL_864",
    "865": "LABEL_865",
    "866": "LABEL_866",
    "867": "LABEL_867",
    "868": "LABEL_868",
    "869": "LABEL_869",
    "870": "LABEL_870",
    "871": "LABEL_871",
    "872": "LABEL_872",
    "873": "LABEL_873",
    "874": "LABEL_874",
    "875": "LABEL_875",
    "876": "LABEL_876",
    "877": "LABEL_877",
    "878": "LABEL_878",
    "879": "LABEL_879",
    "880": "LABEL_880",
    "881": "LABEL_881",
    "882": "LABEL_882",
    "883": "LABEL_883",
    "884": "LABEL_884",
    "885": "LABEL_885",
    "886": "LABEL_886",
    "887": "LABEL_887",
    "888": "LABEL_888",
    "889": "LABEL_889",
    "890": "LABEL_890",
    "891": "LABEL_891",
    "892": "LABEL_892",
    "893": "LABEL_893",
    "894": "LABEL_894",
    "895": "LABEL_895",
    "896": "LABEL_896",
    "897": "LABEL_897",
    "898": "LABEL_898",
    "899": "LABEL_899",
    "900": "LABEL_900",
    "901": "LABEL_901",
    "902": "LABEL_902",
    "903": "LABEL_903",
    "904": "LABEL_904",
    "905": "LABEL_905",
    "906": "LABEL_906",
    "907": "LABEL_907",
    "908": "LABEL_908",
    "909": "LABEL_909",
    "910": "LABEL_910",
    "911": "LABEL_911",
    "912": "LABEL_912",
    "913": "LABEL_913",
    "914": "LABEL_914",
    "915": "LABEL_915",
    "916": "LABEL_916",
    "917": "LABEL_917",
    "918": "LABEL_918",
    "919": "LABEL_919",
    "920": "LABEL_920",
    "921": "LABEL_921",
    "922": "LABEL_922",
    "923": "LABEL_923",
    "924": "LABEL_924",
    "925": "LABEL_925",
    "926": "LABEL_926",
    "927": "LABEL_927",
    "928": "LABEL_928",
    "929": "LABEL_929",
    "930": "LABEL_930",
    "931": "LABEL_931",
    "932": "LABEL_932",
    "933": "LABEL_933",
    "934": "LABEL_934",
    "935": "LABEL_935",
    "936": "LABEL_936",
    "937": "LABEL_937",
    "938": "LABEL_938",
    "939": "LABEL_939",
    "940": "LABEL_940",
    "941": "LABEL_941",
    "942": "LABEL_942",
    "943": "LABEL_943",
    "944": "LABEL_944",
    "945": "LABEL_945",
    "946": "LABEL_946",
    "947": "LABEL_947",
    "948": "LABEL_948",
    "949": "LABEL_949",
    "950": "LABEL_950",
    "951": "LABEL_951",
    "952": "LABEL_952",
    "953": "LABEL_953",
    "954": "LABEL_954",
    "955": "LABEL_955",
    "956": "LABEL_956",
    "957": "LABEL_957",
    "958": "LABEL_958",
    "959": "LABEL_959",
    "960": "LABEL_960",
    "961": "LABEL_961",
    "962": "LABEL_962",
    "963": "LABEL_963",
    "964": "LABEL_964",
    "965": "LABEL_965",
    "966": "LABEL_966",
    "967": "LABEL_967",
    "968": "LABEL_968",
    "969": "LABEL_969",
    "970": "LABEL_970",
    "971": "LABEL_971",
    "972": "LABEL_972",
    "973": "LABEL_973",
    "974": "LABEL_974",
    "975": "LABEL_975",
    "976": "LABEL_976",
    "977": "LABEL_977",
    "978": "LABEL_978",
    "979": "LABEL_979",
    "980": "LABEL_980",
    "981": "LABEL_981",
    "982": "LABEL_982",
    "983": "LABEL_983",
    "984": "LABEL_984",
    "985": "LABEL_985",
    "986": "LABEL_986",
    "987": "LABEL_987",
    "988": "LABEL_988",
    "989": "LABEL_989",
    "990": "LABEL_990",
    "991": "LABEL_991",
    "992": "LABEL_992",
    "993": "LABEL_993",
    "994": "LABEL_994",
    "995": "LABEL_995",
    "996": "LABEL_996",
    "997": "LABEL_997",
    "998": "LABEL_998",
    "999": "LABEL_999",
    "1000": "LABEL_1000",
    "1001": "LABEL_1001",
    "1002": "LABEL_1002",
    "1003": "LABEL_1003",
    "1004": "LABEL_1004",
    "1005": "LABEL_1005",
    "1006": "LABEL_1006",
    "1007": "LABEL_1007",
    "1008": "LABEL_1008",
    "1009": "LABEL_1009",
    "1010": "LABEL_1010",
    "1011": "LABEL_1011",
    "1012": "LABEL_1012",
    "1013": "LABEL_1013",
    "1014": "LABEL_1014",
    "1015": "LABEL_1015",
    "1016": "LABEL_1016",
    "1017": "LABEL_1017",
    "1018": "LABEL_1018",
    "1019": "LABEL_1019",
    "1020": "LABEL_1020",
    "1021": "LABEL_1021",
    "1022": "LABEL_1022",
    "1023": "LABEL_1023",
    "1024": "LABEL_1024",
    "1025": "LABEL_1025",
    "1026": "LABEL_1026",
    "1027": "LABEL_1027",
    "1028": "LABEL_1028",
    "1029": "LABEL_1029",
    "1030": "LABEL_1030",
    "1031": "LABEL_1031",
    "1032": "LABEL_1032",
    "1033": "LABEL_1033",
    "1034": "LABEL_1034",
    "1035": "LABEL_1035",
    "1036": "LABEL_1036",
    "1037": "LABEL_1037",
    "1038": "LABEL_1038",
    "1039": "LABEL_1039",
    "1040": "LABEL_1040",
    "1041": "LABEL_1041",
    "1042": "LABEL_1042",
    "1043": "LABEL_1043",
    "1044": "LABEL_1044",
    "1045": "LABEL_1045",
    "1046": "LABEL_1046",
    "1047": "LABEL_1047",
    "1048": "LABEL_1048",
    "1049": "LABEL_1049",
    "1050": "LABEL_1050",
    "1051": "LABEL_1051",
    "1052": "LABEL_1052",
    "1053": "LABEL_1053",
    "1054": "LABEL_1054",
    "1055": "LABEL_1055",
    "1056": "LABEL_1056",
    "1057": "LABEL_1057",
    "1058": "LABEL_1058",
    "1059": "LABEL_1059",
    "1060": "LABEL_1060",
    "1061": "LABEL_1061",
    "1062": "LABEL_1062",
    "1063": "LABEL_1063",
    "1064": "LABEL_1064",
    "1065": "LABEL_1065",
    "1066": "LABEL_1066",
    "1067": "LABEL_1067",
    "1068": "LABEL_1068",
    "1069": "LABEL_1069",
    "1070": "LABEL_1070",
    "1071": "LABEL_1071",
    "1072": "LABEL_1072",
    "1073": "LABEL_1073",
    "1074": "LABEL_1074",
    "1075": "LABEL_1075",
    "1076": "LABEL_1076",
    "1077": "LABEL_1077",
    "1078": "LABEL_1078",
    "1079": "LABEL_1079",
    "1080": "LABEL_1080",
    "1081": "LABEL_1081",
    "1082": "LABEL_1082",
    "1083": "LABEL_1083",
    "1084": "LABEL_1084",
    "1085": "LABEL_1085",
    "1086": "LABEL_1086",
    "1087": "LABEL_1087",
    "1088": "LABEL_1088",
    "1089": "LABEL_1089",
    "1090": "LABEL_1090",
    "1091": "LABEL_1091",
    "1092": "LABEL_1092",
    "1093": "LABEL_1093",
    "1094": "LABEL_1094",
    "1095": "LABEL_1095",
    "1096": "LABEL_1096",
    "1097": "LABEL_1097",
    "1098": "LABEL_1098",
    "1099": "LABEL_1099",
    "1100": "LABEL_1100",
    "1101": "LABEL_1101",
    "1102": "LABEL_1102",
    "1103": "LABEL_1103",
    "1104": "LABEL_1104",
    "1105": "LABEL_1105",
    "1106": "LABEL_1106",
    "1107": "LABEL_1107",
    "1108": "LABEL_1108",
    "1109": "LABEL_1109",
    "1110": "LABEL_1110",
    "1111": "LABEL_1111",
    "1112": "LABEL_1112",
    "1113": "LABEL_1113",
    "1114": "LABEL_1114",
    "1115": "LABEL_1115",
    "1116": "LABEL_1116",
    "1117": "LABEL_1117",
    "1118": "LABEL_1118",
    "1119": "LABEL_1119",
    "1120": "LABEL_1120",
    "1121": "LABEL_1121",
    "1122": "LABEL_1122",
    "1123": "LABEL_1123",
    "1124": "LABEL_1124",
    "1125": "LABEL_1125",
    "1126": "LABEL_1126",
    "1127": "LABEL_1127",
    "1128": "LABEL_1128",
    "1129": "LABEL_1129",
    "1130": "LABEL_1130",
    "1131": "LABEL_1131",
    "1132": "LABEL_1132",
    "1133": "LABEL_1133",
    "1134": "LABEL_1134",
    "1135": "LABEL_1135",
    "1136": "LABEL_1136",
    "1137": "LABEL_1137",
    "1138": "LABEL_1138",
    "1139": "LABEL_1139",
    "1140": "LABEL_1140",
    "1141": "LABEL_1141",
    "1142": "LABEL_1142",
    "1143": "LABEL_1143",
    "1144": "LABEL_1144",
    "1145": "LABEL_1145",
    "1146": "LABEL_1146",
    "1147": "LABEL_1147",
    "1148": "LABEL_1148",
    "1149": "LABEL_1149",
    "1150": "LABEL_1150",
    "1151": "LABEL_1151",
    "1152": "LABEL_1152",
    "1153": "LABEL_1153",
    "1154": "LABEL_1154",
    "1155": "LABEL_1155",
    "1156": "LABEL_1156",
    "1157": "LABEL_1157",
    "1158": "LABEL_1158",
    "1159": "LABEL_1159",
    "1160": "LABEL_1160",
    "1161": "LABEL_1161",
    "1162": "LABEL_1162",
    "1163": "LABEL_1163",
    "1164": "LABEL_1164",
    "1165": "LABEL_1165",
    "1166": "LABEL_1166",
    "1167": "LABEL_1167",
    "1168": "LABEL_1168",
    "1169": "LABEL_1169",
    "1170": "LABEL_1170",
    "1171": "LABEL_1171",
    "1172": "LABEL_1172",
    "1173": "LABEL_1173",
    "1174": "LABEL_1174",
    "1175": "LABEL_1175",
    "1176": "LABEL_1176",
    "1177": "LABEL_1177",
    "1178": "LABEL_1178",
    "1179": "LABEL_1179",
    "1180": "LABEL_1180",
    "1181": "LABEL_1181",
    "1182": "LABEL_1182",
    "1183": "LABEL_1183",
    "1184": "LABEL_1184",
    "1185": "LABEL_1185",
    "1186": "LABEL_1186",
    "1187": "LABEL_1187",
    "1188": "LABEL_1188",
    "1189": "LABEL_1189",
    "1190": "LABEL_1190",
    "1191": "LABEL_1191",
    "1192": "LABEL_1192",
    "1193": "LABEL_1193",
    "1194": "LABEL_1194",
    "1195": "LABEL_1195",
    "1196": "LABEL_1196",
    "1197": "LABEL_1197",
    "1198": "LABEL_1198",
    "1199": "LABEL_1199",
    "1200": "LABEL_1200",
    "1201": "LABEL_1201",
    "1202": "LABEL_1202",
    "1203": "LABEL_1203",
    "1204": "LABEL_1204",
    "1205": "LABEL_1205",
    "1206": "LABEL_1206",
    "1207": "LABEL_1207",
    "1208": "LABEL_1208",
    "1209": "LABEL_1209",
    "1210": "LABEL_1210",
    "1211": "LABEL_1211",
    "1212": "LABEL_1212",
    "1213": "LABEL_1213",
    "1214": "LABEL_1214",
    "1215": "LABEL_1215",
    "1216": "LABEL_1216",
    "1217": "LABEL_1217",
    "1218": "LABEL_1218",
    "1219": "LABEL_1219",
    "1220": "LABEL_1220",
    "1221": "LABEL_1221",
    "1222": "LABEL_1222",
    "1223": "LABEL_1223",
    "1224": "LABEL_1224",
    "1225": "LABEL_1225",
    "1226": "LABEL_1226",
    "1227": "LABEL_1227",
    "1228": "LABEL_1228",
    "1229": "LABEL_1229",
    "1230": "LABEL_1230",
    "1231": "LABEL_1231",
    "1232": "LABEL_1232",
    "1233": "LABEL_1233",
    "1234": "LABEL_1234",
    "1235": "LABEL_1235",
    "1236": "LABEL_1236",
    "1237": "LABEL_1237",
    "1238": "LABEL_1238",
    "1239": "LABEL_1239",
    "1240": "LABEL_1240",
    "1241": "LABEL_1241",
    "1242": "LABEL_1242",
    "1243": "LABEL_1243",
    "1244": "LABEL_1244",
    "1245": "LABEL_1245",
    "1246": "LABEL_1246",
    "1247": "LABEL_1247",
    "1248": "LABEL_1248",
    "1249": "LABEL_1249",
    "1250": "LABEL_1250",
    "1251": "LABEL_1251",
    "1252": "LABEL_1252",
    "1253": "LABEL_1253",
    "1254": "LABEL_1254",
    "1255": "LABEL_1255",
    "1256": "LABEL_1256",
    "1257": "LABEL_1257",
    "1258": "LABEL_1258",
    "1259": "LABEL_1259",
    "1260": "LABEL_1260",
    "1261": "LABEL_1261",
    "1262": "LABEL_1262",
    "1263": "LABEL_1263",
    "1264": "LABEL_1264",
    "1265": "LABEL_1265",
    "1266": "LABEL_1266",
    "1267": "LABEL_1267",
    "1268": "LABEL_1268",
    "1269": "LABEL_1269",
    "1270": "LABEL_1270",
    "1271": "LABEL_1271",
    "1272": "LABEL_1272",
    "1273": "LABEL_1273",
    "1274": "LABEL_1274",
    "1275": "LABEL_1275",
    "1276": "LABEL_1276",
    "1277": "LABEL_1277",
    "1278": "LABEL_1278",
    "1279": "LABEL_1279",
    "1280": "LABEL_1280",
    "1281": "LABEL_1281",
    "1282": "LABEL_1282",
    "1283": "LABEL_1283",
    "1284": "LABEL_1284",
    "1285": "LABEL_1285",
    "1286": "LABEL_1286",
    "1287": "LABEL_1287",
    "1288": "LABEL_1288",
    "1289": "LABEL_1289",
    "1290": "LABEL_1290",
    "1291": "LABEL_1291",
    "1292": "LABEL_1292",
    "1293": "LABEL_1293",
    "1294": "LABEL_1294",
    "1295": "LABEL_1295",
    "1296": "LABEL_1296",
    "1297": "LABEL_1297",
    "1298": "LABEL_1298",
    "1299": "LABEL_1299",
    "1300": "LABEL_1300",
    "1301": "LABEL_1301",
    "1302": "LABEL_1302",
    "1303": "LABEL_1303",
    "1304": "LABEL_1304",
    "1305": "LABEL_1305",
    "1306": "LABEL_1306",
    "1307": "LABEL_1307",
    "1308": "LABEL_1308",
    "1309": "LABEL_1309",
    "1310": "LABEL_1310",
    "1311": "LABEL_1311",
    "1312": "LABEL_1312",
    "1313": "LABEL_1313",
    "1314": "LABEL_1314",
    "1315": "LABEL_1315",
    "1316": "LABEL_1316",
    "1317": "LABEL_1317",
    "1318": "LABEL_1318",
    "1319": "LABEL_1319",
    "1320": "LABEL_1320",
    "1321": "LABEL_1321",
    "1322": "LABEL_1322",
    "1323": "LABEL_1323",
    "1324": "LABEL_1324",
    "1325": "LABEL_1325",
    "1326": "LABEL_1326",
    "1327": "LABEL_1327",
    "1328": "LABEL_1328",
    "1329": "LABEL_1329",
    "1330": "LABEL_1330",
    "1331": "LABEL_1331",
    "1332": "LABEL_1332",
    "1333": "LABEL_1333",
    "1334": "LABEL_1334",
    "1335": "LABEL_1335",
    "1336": "LABEL_1336",
    "1337": "LABEL_1337",
    "1338": "LABEL_1338",
    "1339": "LABEL_1339",
    "1340": "LABEL_1340",
    "1341": "LABEL_1341",
    "1342": "LABEL_1342",
    "1343": "LABEL_1343",
    "1344": "LABEL_1344",
    "1345": "LABEL_1345",
    "1346": "LABEL_1346",
    "1347": "LABEL_1347"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_100": 100,
    "LABEL_1000": 1000,
    "LABEL_1001": 1001,
    "LABEL_1002": 1002,
    "LABEL_1003": 1003,
    "LABEL_1004": 1004,
    "LABEL_1005": 1005,
    "LABEL_1006": 1006,
    "LABEL_1007": 1007,
    "LABEL_1008": 1008,
    "LABEL_1009": 1009,
    "LABEL_101": 101,
    "LABEL_1010": 1010,
    "LABEL_1011": 1011,
    "LABEL_1012": 1012,
    "LABEL_1013": 1013,
    "LABEL_1014": 1014,
    "LABEL_1015": 1015,
    "LABEL_1016": 1016,
    "LABEL_1017": 1017,
    "LABEL_1018": 1018,
    "LABEL_1019": 1019,
    "LABEL_102": 102,
    "LABEL_1020": 1020,
    "LABEL_1021": 1021,
    "LABEL_1022": 1022,
    "LABEL_1023": 1023,
    "LABEL_1024": 1024,
    "LABEL_1025": 1025,
    "LABEL_1026": 1026,
    "LABEL_1027": 1027,
    "LABEL_1028": 1028,
    "LABEL_1029": 1029,
    "LABEL_103": 103,
    "LABEL_1030": 1030,
    "LABEL_1031": 1031,
    "LABEL_1032": 1032,
    "LABEL_1033": 1033,
    "LABEL_1034": 1034,
    "LABEL_1035": 1035,
    "LABEL_1036": 1036,
    "LABEL_1037": 1037,
    "LABEL_1038": 1038,
    "LABEL_1039": 1039,
    "LABEL_104": 104,
    "LABEL_1040": 1040,
    "LABEL_1041": 1041,
    "LABEL_1042": 1042,
    "LABEL_1043": 1043,
    "LABEL_1044": 1044,
    "LABEL_1045": 1045,
    "LABEL_1046": 1046,
    "LABEL_1047": 1047,
    "LABEL_1048": 1048,
    "LABEL_1049": 1049,
    "LABEL_105": 105,
    "LABEL_1050": 1050,
    "LABEL_1051": 1051,
    "LABEL_1052": 1052,
    "LABEL_1053": 1053,
    "LABEL_1054": 1054,
    "LABEL_1055": 1055,
    "LABEL_1056": 1056,
    "LABEL_1057": 1057,
    "LABEL_1058": 1058,
    "LABEL_1059": 1059,
    "LABEL_106": 106,
    "LABEL_1060": 1060,
    "LABEL_1061": 1061,
    "LABEL_1062": 1062,
    "LABEL_1063": 1063,
    "LABEL_1064": 1064,
    "LABEL_1065": 1065,
    "LABEL_1066": 1066,
    "LABEL_1067": 1067,
    "LABEL_1068": 1068,
    "LABEL_1069": 1069,
    "LABEL_107": 107,
    "LABEL_1070": 1070,
    "LABEL_1071": 1071,
    "LABEL_1072": 1072,
    "LABEL_1073": 1073,
    "LABEL_1074": 1074,
    "LABEL_1075": 1075,
    "LABEL_1076": 1076,
    "LABEL_1077": 1077,
    "LABEL_1078": 1078,
    "LABEL_1079": 1079,
    "LABEL_108": 108,
    "LABEL_1080": 1080,
    "LABEL_1081": 1081,
    "LABEL_1082": 1082,
    "LABEL_1083": 1083,
    "LABEL_1084": 1084,
    "LABEL_1085": 1085,
    "LABEL_1086": 1086,
    "LABEL_1087": 1087,
    "LABEL_1088": 1088,
    "LABEL_1089": 1089,
    "LABEL_109": 109,
    "LABEL_1090": 1090,
    "LABEL_1091": 1091,
    "LABEL_1092": 1092,
    "LABEL_1093": 1093,
    "LABEL_1094": 1094,
    "LABEL_1095": 1095,
    "LABEL_1096": 1096,
    "LABEL_1097": 1097,
    "LABEL_1098": 1098,
    "LABEL_1099": 1099,
    "LABEL_11": 11,
    "LABEL_110": 110,
    "LABEL_1100": 1100,
    "LABEL_1101": 1101,
    "LABEL_1102": 1102,
    "LABEL_1103": 1103,
    "LABEL_1104": 1104,
    "LABEL_1105": 1105,
    "LABEL_1106": 1106,
    "LABEL_1107": 1107,
    "LABEL_1108": 1108,
    "LABEL_1109": 1109,
    "LABEL_111": 111,
    "LABEL_1110": 1110,
    "LABEL_1111": 1111,
    "LABEL_1112": 1112,
    "LABEL_1113": 1113,
    "LABEL_1114": 1114,
    "LABEL_1115": 1115,
    "LABEL_1116": 1116,
    "LABEL_1117": 1117,
    "LABEL_1118": 1118,
    "LABEL_1119": 1119,
    "LABEL_112": 112,
    "LABEL_1120": 1120,
    "LABEL_1121": 1121,
    "LABEL_1122": 1122,
    "LABEL_1123": 1123,
    "LABEL_1124": 1124,
    "LABEL_1125": 1125,
    "LABEL_1126": 1126,
    "LABEL_1127": 1127,
    "LABEL_1128": 1128,
    "LABEL_1129": 1129,
    "LABEL_113": 113,
    "LABEL_1130": 1130,
    "LABEL_1131": 1131,
    "LABEL_1132": 1132,
    "LABEL_1133": 1133,
    "LABEL_1134": 1134,
    "LABEL_1135": 1135,
    "LABEL_1136": 1136,
    "LABEL_1137": 1137,
    "LABEL_1138": 1138,
    "LABEL_1139": 1139,
    "LABEL_114": 114,
    "LABEL_1140": 1140,
    "LABEL_1141": 1141,
    "LABEL_1142": 1142,
    "LABEL_1143": 1143,
    "LABEL_1144": 1144,
    "LABEL_1145": 1145,
    "LABEL_1146": 1146,
    "LABEL_1147": 1147,
    "LABEL_1148": 1148,
    "LABEL_1149": 1149,
    "LABEL_115": 115,
    "LABEL_1150": 1150,
    "LABEL_1151": 1151,
    "LABEL_1152": 1152,
    "LABEL_1153": 1153,
    "LABEL_1154": 1154,
    "LABEL_1155": 1155,
    "LABEL_1156": 1156,
    "LABEL_1157": 1157,
    "LABEL_1158": 1158,
    "LABEL_1159": 1159,
    "LABEL_116": 116,
    "LABEL_1160": 1160,
    "LABEL_1161": 1161,
    "LABEL_1162": 1162,
    "LABEL_1163": 1163,
    "LABEL_1164": 1164,
    "LABEL_1165": 1165,
    "LABEL_1166": 1166,
    "LABEL_1167": 1167,
    "LABEL_1168": 1168,
    "LABEL_1169": 1169,
    "LABEL_117": 117,
    "LABEL_1170": 1170,
    "LABEL_1171": 1171,
    "LABEL_1172": 1172,
    "LABEL_1173": 1173,
    "LABEL_1174": 1174,
    "LABEL_1175": 1175,
    "LABEL_1176": 1176,
    "LABEL_1177": 1177,
    "LABEL_1178": 1178,
    "LABEL_1179": 1179,
    "LABEL_118": 118,
    "LABEL_1180": 1180,
    "LABEL_1181": 1181,
    "LABEL_1182": 1182,
    "LABEL_1183": 1183,
    "LABEL_1184": 1184,
    "LABEL_1185": 1185,
    "LABEL_1186": 1186,
    "LABEL_1187": 1187,
    "LABEL_1188": 1188,
    "LABEL_1189": 1189,
    "LABEL_119": 119,
    "LABEL_1190": 1190,
    "LABEL_1191": 1191,
    "LABEL_1192": 1192,
    "LABEL_1193": 1193,
    "LABEL_1194": 1194,
    "LABEL_1195": 1195,
    "LABEL_1196": 1196,
    "LABEL_1197": 1197,
    "LABEL_1198": 1198,
    "LABEL_1199": 1199,
    "LABEL_12": 12,
    "LABEL_120": 120,
    "LABEL_1200": 1200,
    "LABEL_1201": 1201,
    "LABEL_1202": 1202,
    "LABEL_1203": 1203,
    "LABEL_1204": 1204,
    "LABEL_1205": 1205,
    "LABEL_1206": 1206,
    "LABEL_1207": 1207,
    "LABEL_1208": 1208,
    "LABEL_1209": 1209,
    "LABEL_121": 121,
    "LABEL_1210": 1210,
    "LABEL_1211": 1211,
    "LABEL_1212": 1212,
    "LABEL_1213": 1213,
    "LABEL_1214": 1214,
    "LABEL_1215": 1215,
    "LABEL_1216": 1216,
    "LABEL_1217": 1217,
    "LABEL_1218": 1218,
    "LABEL_1219": 1219,
    "LABEL_122": 122,
    "LABEL_1220": 1220,
    "LABEL_1221": 1221,
    "LABEL_1222": 1222,
    "LABEL_1223": 1223,
    "LABEL_1224": 1224,
    "LABEL_1225": 1225,
    "LABEL_1226": 1226,
    "LABEL_1227": 1227,
    "LABEL_1228": 1228,
    "LABEL_1229": 1229,
    "LABEL_123": 123,
    "LABEL_1230": 1230,
    "LABEL_1231": 1231,
    "LABEL_1232": 1232,
    "LABEL_1233": 1233,
    "LABEL_1234": 1234,
    "LABEL_1235": 1235,
    "LABEL_1236": 1236,
    "LABEL_1237": 1237,
    "LABEL_1238": 1238,
    "LABEL_1239": 1239,
    "LABEL_124": 124,
    "LABEL_1240": 1240,
    "LABEL_1241": 1241,
    "LABEL_1242": 1242,
    "LABEL_1243": 1243,
    "LABEL_1244": 1244,
    "LABEL_1245": 1245,
    "LABEL_1246": 1246,
    "LABEL_1247": 1247,
    "LABEL_1248": 1248,
    "LABEL_1249": 1249,
    "LABEL_125": 125,
    "LABEL_1250": 1250,
    "LABEL_1251": 1251,
    "LABEL_1252": 1252,
    "LABEL_1253": 1253,
    "LABEL_1254": 1254,
    "LABEL_1255": 1255,
    "LABEL_1256": 1256,
    "LABEL_1257": 1257,
    "LABEL_1258": 1258,
    "LABEL_1259": 1259,
    "LABEL_126": 126,
    "LABEL_1260": 1260,
    "LABEL_1261": 1261,
    "LABEL_1262": 1262,
    "LABEL_1263": 1263,
    "LABEL_1264": 1264,
    "LABEL_1265": 1265,
    "LABEL_1266": 1266,
    "LABEL_1267": 1267,
    "LABEL_1268": 1268,
    "LABEL_1269": 1269,
    "LABEL_127": 127,
    "LABEL_1270": 1270,
    "LABEL_1271": 1271,
    "LABEL_1272": 1272,
    "LABEL_1273": 1273,
    "LABEL_1274": 1274,
    "LABEL_1275": 1275,
    "LABEL_1276": 1276,
    "LABEL_1277": 1277,
    "LABEL_1278": 1278,
    "LABEL_1279": 1279,
    "LABEL_128": 128,
    "LABEL_1280": 1280,
    "LABEL_1281": 1281,
    "LABEL_1282": 1282,
    "LABEL_1283": 1283,
    "LABEL_1284": 1284,
    "LABEL_1285": 1285,
    "LABEL_1286": 1286,
    "LABEL_1287": 1287,
    "LABEL_1288": 1288,
    "LABEL_1289": 1289,
    "LABEL_129": 129,
    "LABEL_1290": 1290,
    "LABEL_1291": 1291,
    "LABEL_1292": 1292,
    "LABEL_1293": 1293,
    "LABEL_1294": 1294,
    "LABEL_1295": 1295,
    "LABEL_1296": 1296,
    "LABEL_1297": 1297,
    "LABEL_1298": 1298,
    "LABEL_1299": 1299,
    "LABEL_13": 13,
    "LABEL_130": 130,
    "LABEL_1300": 1300,
    "LABEL_1301": 1301,
    "LABEL_1302": 1302,
    "LABEL_1303": 1303,
    "LABEL_1304": 1304,
    "LABEL_1305": 1305,
    "LABEL_1306": 1306,
    "LABEL_1307": 1307,
    "LABEL_1308": 1308,
    "LABEL_1309": 1309,
    "LABEL_131": 131,
    "LABEL_1310": 1310,
    "LABEL_1311": 1311,
    "LABEL_1312": 1312,
    "LABEL_1313": 1313,
    "LABEL_1314": 1314,
    "LABEL_1315": 1315,
    "LABEL_1316": 1316,
    "LABEL_1317": 1317,
    "LABEL_1318": 1318,
    "LABEL_1319": 1319,
    "LABEL_132": 132,
    "LABEL_1320": 1320,
    "LABEL_1321": 1321,
    "LABEL_1322": 1322,
    "LABEL_1323": 1323,
    "LABEL_1324": 1324,
    "LABEL_1325": 1325,
    "LABEL_1326": 1326,
    "LABEL_1327": 1327,
    "LABEL_1328": 1328,
    "LABEL_1329": 1329,
    "LABEL_133": 133,
    "LABEL_1330": 1330,
    "LABEL_1331": 1331,
    "LABEL_1332": 1332,
    "LABEL_1333": 1333,
    "LABEL_1334": 1334,
    "LABEL_1335": 1335,
    "LABEL_1336": 1336,
    "LABEL_1337": 1337,
    "LABEL_1338": 1338,
    "LABEL_1339": 1339,
    "LABEL_134": 134,
    "LABEL_1340": 1340,
    "LABEL_1341": 1341,
    "LABEL_1342": 1342,
    "LABEL_1343": 1343,
    "LABEL_1344": 1344,
    "LABEL_1345": 1345,
    "LABEL_1346": 1346,
    "LABEL_1347": 1347,
    "LABEL_135": 135,
    "LABEL_136": 136,
    "LABEL_137": 137,
    "LABEL_138": 138,
    "LABEL_139": 139,
    "LABEL_14": 14,
    "LABEL_140": 140,
    "LABEL_141": 141,
    "LABEL_142": 142,
    "LABEL_143": 143,
    "LABEL_144": 144,
    "LABEL_145": 145,
    "LABEL_146": 146,
    "LABEL_147": 147,
    "LABEL_148": 148,
    "LABEL_149": 149,
    "LABEL_15": 15,
    "LABEL_150": 150,
    "LABEL_151": 151,
    "LABEL_152": 152,
    "LABEL_153": 153,
    "LABEL_154": 154,
    "LABEL_155": 155,
    "LABEL_156": 156,
    "LABEL_157": 157,
    "LABEL_158": 158,
    "LABEL_159": 159,
    "LABEL_16": 16,
    "LABEL_160": 160,
    "LABEL_161": 161,
    "LABEL_162": 162,
    "LABEL_163": 163,
    "LABEL_164": 164,
    "LABEL_165": 165,
    "LABEL_166": 166,
    "LABEL_167": 167,
    "LABEL_168": 168,
    "LABEL_169": 169,
    "LABEL_17": 17,
    "LABEL_170": 170,
    "LABEL_171": 171,
    "LABEL_172": 172,
    "LABEL_173": 173,
    "LABEL_174": 174,
    "LABEL_175": 175,
    "LABEL_176": 176,
    "LABEL_177": 177,
    "LABEL_178": 178,
    "LABEL_179": 179,
    "LABEL_18": 18,
    "LABEL_180": 180,
    "LABEL_181": 181,
    "LABEL_182": 182,
    "LABEL_183": 183,
    "LABEL_184": 184,
    "LABEL_185": 185,
    "LABEL_186": 186,
    "LABEL_187": 187,
    "LABEL_188": 188,
    "LABEL_189": 189,
    "LABEL_19": 19,
    "LABEL_190": 190,
    "LABEL_191": 191,
    "LABEL_192": 192,
    "LABEL_193": 193,
    "LABEL_194": 194,
    "LABEL_195": 195,
    "LABEL_196": 196,
    "LABEL_197": 197,
    "LABEL_198": 198,
    "LABEL_199": 199,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_200": 200,
    "LABEL_201": 201,
    "LABEL_202": 202,
    "LABEL_203": 203,
    "LABEL_204": 204,
    "LABEL_205": 205,
    "LABEL_206": 206,
    "LABEL_207": 207,
    "LABEL_208": 208,
    "LABEL_209": 209,
    "LABEL_21": 21,
    "LABEL_210": 210,
    "LABEL_211": 211,
    "LABEL_212": 212,
    "LABEL_213": 213,
    "LABEL_214": 214,
    "LABEL_215": 215,
    "LABEL_216": 216,
    "LABEL_217": 217,
    "LABEL_218": 218,
    "LABEL_219": 219,
    "LABEL_22": 22,
    "LABEL_220": 220,
    "LABEL_221": 221,
    "LABEL_222": 222,
    "LABEL_223": 223,
    "LABEL_224": 224,
    "LABEL_225": 225,
    "LABEL_226": 226,
    "LABEL_227": 227,
    "LABEL_228": 228,
    "LABEL_229": 229,
    "LABEL_23": 23,
    "LABEL_230": 230,
    "LABEL_231": 231,
    "LABEL_232": 232,
    "LABEL_233": 233,
    "LABEL_234": 234,
    "LABEL_235": 235,
    "LABEL_236": 236,
    "LABEL_237": 237,
    "LABEL_238": 238,
    "LABEL_239": 239,
    "LABEL_24": 24,
    "LABEL_240": 240,
    "LABEL_241": 241,
    "LABEL_242": 242,
    "LABEL_243": 243,
    "LABEL_244": 244,
    "LABEL_245": 245,
    "LABEL_246": 246,
    "LABEL_247": 247,
    "LABEL_248": 248,
    "LABEL_249": 249,
    "LABEL_25": 25,
    "LABEL_250": 250,
    "LABEL_251": 251,
    "LABEL_252": 252,
    "LABEL_253": 253,
    "LABEL_254": 254,
    "LABEL_255": 255,
    "LABEL_256": 256,
    "LABEL_257": 257,
    "LABEL_258": 258,
    "LABEL_259": 259,
    "LABEL_26": 26,
    "LABEL_260": 260,
    "LABEL_261": 261,
    "LABEL_262": 262,
    "LABEL_263": 263,
    "LABEL_264": 264,
    "LABEL_265": 265,
    "LABEL_266": 266,
    "LABEL_267": 267,
    "LABEL_268": 268,
    "LABEL_269": 269,
    "LABEL_27": 27,
    "LABEL_270": 270,
    "LABEL_271": 271,
    "LABEL_272": 272,
    "LABEL_273": 273,
    "LABEL_274": 274,
    "LABEL_275": 275,
    "LABEL_276": 276,
    "LABEL_277": 277,
    "LABEL_278": 278,
    "LABEL_279": 279,
    "LABEL_28": 28,
    "LABEL_280": 280,
    "LABEL_281": 281,
    "LABEL_282": 282,
    "LABEL_283": 283,
    "LABEL_284": 284,
    "LABEL_285": 285,
    "LABEL_286": 286,
    "LABEL_287": 287,
    "LABEL_288": 288,
    "LABEL_289": 289,
    "LABEL_29": 29,
    "LABEL_290": 290,
    "LABEL_291": 291,
    "LABEL_292": 292,
    "LABEL_293": 293,
    "LABEL_294": 294,
    "LABEL_295": 295,
    "LABEL_296": 296,
    "LABEL_297": 297,
    "LABEL_298": 298,
    "LABEL_299": 299,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_300": 300,
    "LABEL_301": 301,
    "LABEL_302": 302,
    "LABEL_303": 303,
    "LABEL_304": 304,
    "LABEL_305": 305,
    "LABEL_306": 306,
    "LABEL_307": 307,
    "LABEL_308": 308,
    "LABEL_309": 309,
    "LABEL_31": 31,
    "LABEL_310": 310,
    "LABEL_311": 311,
    "LABEL_312": 312,
    "LABEL_313": 313,
    "LABEL_314": 314,
    "LABEL_315": 315,
    "LABEL_316": 316,
    "LABEL_317": 317,
    "LABEL_318": 318,
    "LABEL_319": 319,
    "LABEL_32": 32,
    "LABEL_320": 320,
    "LABEL_321": 321,
    "LABEL_322": 322,
    "LABEL_323": 323,
    "LABEL_324": 324,
    "LABEL_325": 325,
    "LABEL_326": 326,
    "LABEL_327": 327,
    "LABEL_328": 328,
    "LABEL_329": 329,
    "LABEL_33": 33,
    "LABEL_330": 330,
    "LABEL_331": 331,
    "LABEL_332": 332,
    "LABEL_333": 333,
    "LABEL_334": 334,
    "LABEL_335": 335,
    "LABEL_336": 336,
    "LABEL_337": 337,
    "LABEL_338": 338,
    "LABEL_339": 339,
    "LABEL_34": 34,
    "LABEL_340": 340,
    "LABEL_341": 341,
    "LABEL_342": 342,
    "LABEL_343": 343,
    "LABEL_344": 344,
    "LABEL_345": 345,
    "LABEL_346": 346,
    "LABEL_347": 347,
    "LABEL_348": 348,
    "LABEL_349": 349,
    "LABEL_35": 35,
    "LABEL_350": 350,
    "LABEL_351": 351,
    "LABEL_352": 352,
    "LABEL_353": 353,
    "LABEL_354": 354,
    "LABEL_355": 355,
    "LABEL_356": 356,
    "LABEL_357": 357,
    "LABEL_358": 358,
    "LABEL_359": 359,
    "LABEL_36": 36,
    "LABEL_360": 360,
    "LABEL_361": 361,
    "LABEL_362": 362,
    "LABEL_363": 363,
    "LABEL_364": 364,
    "LABEL_365": 365,
    "LABEL_366": 366,
    "LABEL_367": 367,
    "LABEL_368": 368,
    "LABEL_369": 369,
    "LABEL_37": 37,
    "LABEL_370": 370,
    "LABEL_371": 371,
    "LABEL_372": 372,
    "LABEL_373": 373,
    "LABEL_374": 374,
    "LABEL_375": 375,
    "LABEL_376": 376,
    "LABEL_377": 377,
    "LABEL_378": 378,
    "LABEL_379": 379,
    "LABEL_38": 38,
    "LABEL_380": 380,
    "LABEL_381": 381,
    "LABEL_382": 382,
    "LABEL_383": 383,
    "LABEL_384": 384,
    "LABEL_385": 385,
    "LABEL_386": 386,
    "LABEL_387": 387,
    "LABEL_388": 388,
    "LABEL_389": 389,
    "LABEL_39": 39,
    "LABEL_390": 390,
    "LABEL_391": 391,
    "LABEL_392": 392,
    "LABEL_393": 393,
    "LABEL_394": 394,
    "LABEL_395": 395,
    "LABEL_396": 396,
    "LABEL_397": 397,
    "LABEL_398": 398,
    "LABEL_399": 399,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_400": 400,
    "LABEL_401": 401,
    "LABEL_402": 402,
    "LABEL_403": 403,
    "LABEL_404": 404,
    "LABEL_405": 405,
    "LABEL_406": 406,
    "LABEL_407": 407,
    "LABEL_408": 408,
    "LABEL_409": 409,
    "LABEL_41": 41,
    "LABEL_410": 410,
    "LABEL_411": 411,
    "LABEL_412": 412,
    "LABEL_413": 413,
    "LABEL_414": 414,
    "LABEL_415": 415,
    "LABEL_416": 416,
    "LABEL_417": 417,
    "LABEL_418": 418,
    "LABEL_419": 419,
    "LABEL_42": 42,
    "LABEL_420": 420,
    "LABEL_421": 421,
    "LABEL_422": 422,
    "LABEL_423": 423,
    "LABEL_424": 424,
    "LABEL_425": 425,
    "LABEL_426": 426,
    "LABEL_427": 427,
    "LABEL_428": 428,
    "LABEL_429": 429,
    "LABEL_43": 43,
    "LABEL_430": 430,
    "LABEL_431": 431,
    "LABEL_432": 432,
    "LABEL_433": 433,
    "LABEL_434": 434,
    "LABEL_435": 435,
    "LABEL_436": 436,
    "LABEL_437": 437,
    "LABEL_438": 438,
    "LABEL_439": 439,
    "LABEL_44": 44,
    "LABEL_440": 440,
    "LABEL_441": 441,
    "LABEL_442": 442,
    "LABEL_443": 443,
    "LABEL_444": 444,
    "LABEL_445": 445,
    "LABEL_446": 446,
    "LABEL_447": 447,
    "LABEL_448": 448,
    "LABEL_449": 449,
    "LABEL_45": 45,
    "LABEL_450": 450,
    "LABEL_451": 451,
    "LABEL_452": 452,
    "LABEL_453": 453,
    "LABEL_454": 454,
    "LABEL_455": 455,
    "LABEL_456": 456,
    "LABEL_457": 457,
    "LABEL_458": 458,
    "LABEL_459": 459,
    "LABEL_46": 46,
    "LABEL_460": 460,
    "LABEL_461": 461,
    "LABEL_462": 462,
    "LABEL_463": 463,
    "LABEL_464": 464,
    "LABEL_465": 465,
    "LABEL_466": 466,
    "LABEL_467": 467,
    "LABEL_468": 468,
    "LABEL_469": 469,
    "LABEL_47": 47,
    "LABEL_470": 470,
    "LABEL_471": 471,
    "LABEL_472": 472,
    "LABEL_473": 473,
    "LABEL_474": 474,
    "LABEL_475": 475,
    "LABEL_476": 476,
    "LABEL_477": 477,
    "LABEL_478": 478,
    "LABEL_479": 479,
    "LABEL_48": 48,
    "LABEL_480": 480,
    "LABEL_481": 481,
    "LABEL_482": 482,
    "LABEL_483": 483,
    "LABEL_484": 484,
    "LABEL_485": 485,
    "LABEL_486": 486,
    "LABEL_487": 487,
    "LABEL_488": 488,
    "LABEL_489": 489,
    "LABEL_49": 49,
    "LABEL_490": 490,
    "LABEL_491": 491,
    "LABEL_492": 492,
    "LABEL_493": 493,
    "LABEL_494": 494,
    "LABEL_495": 495,
    "LABEL_496": 496,
    "LABEL_497": 497,
    "LABEL_498": 498,
    "LABEL_499": 499,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_500": 500,
    "LABEL_501": 501,
    "LABEL_502": 502,
    "LABEL_503": 503,
    "LABEL_504": 504,
    "LABEL_505": 505,
    "LABEL_506": 506,
    "LABEL_507": 507,
    "LABEL_508": 508,
    "LABEL_509": 509,
    "LABEL_51": 51,
    "LABEL_510": 510,
    "LABEL_511": 511,
    "LABEL_512": 512,
    "LABEL_513": 513,
    "LABEL_514": 514,
    "LABEL_515": 515,
    "LABEL_516": 516,
    "LABEL_517": 517,
    "LABEL_518": 518,
    "LABEL_519": 519,
    "LABEL_52": 52,
    "LABEL_520": 520,
    "LABEL_521": 521,
    "LABEL_522": 522,
    "LABEL_523": 523,
    "LABEL_524": 524,
    "LABEL_525": 525,
    "LABEL_526": 526,
    "LABEL_527": 527,
    "LABEL_528": 528,
    "LABEL_529": 529,
    "LABEL_53": 53,
    "LABEL_530": 530,
    "LABEL_531": 531,
    "LABEL_532": 532,
    "LABEL_533": 533,
    "LABEL_534": 534,
    "LABEL_535": 535,
    "LABEL_536": 536,
    "LABEL_537": 537,
    "LABEL_538": 538,
    "LABEL_539": 539,
    "LABEL_54": 54,
    "LABEL_540": 540,
    "LABEL_541": 541,
    "LABEL_542": 542,
    "LABEL_543": 543,
    "LABEL_544": 544,
    "LABEL_545": 545,
    "LABEL_546": 546,
    "LABEL_547": 547,
    "LABEL_548": 548,
    "LABEL_549": 549,
    "LABEL_55": 55,
    "LABEL_550": 550,
    "LABEL_551": 551,
    "LABEL_552": 552,
    "LABEL_553": 553,
    "LABEL_554": 554,
    "LABEL_555": 555,
    "LABEL_556": 556,
    "LABEL_557": 557,
    "LABEL_558": 558,
    "LABEL_559": 559,
    "LABEL_56": 56,
    "LABEL_560": 560,
    "LABEL_561": 561,
    "LABEL_562": 562,
    "LABEL_563": 563,
    "LABEL_564": 564,
    "LABEL_565": 565,
    "LABEL_566": 566,
    "LABEL_567": 567,
    "LABEL_568": 568,
    "LABEL_569": 569,
    "LABEL_57": 57,
    "LABEL_570": 570,
    "LABEL_571": 571,
    "LABEL_572": 572,
    "LABEL_573": 573,
    "LABEL_574": 574,
    "LABEL_575": 575,
    "LABEL_576": 576,
    "LABEL_577": 577,
    "LABEL_578": 578,
    "LABEL_579": 579,
    "LABEL_58": 58,
    "LABEL_580": 580,
    "LABEL_581": 581,
    "LABEL_582": 582,
    "LABEL_583": 583,
    "LABEL_584": 584,
    "LABEL_585": 585,
    "LABEL_586": 586,
    "LABEL_587": 587,
    "LABEL_588": 588,
    "LABEL_589": 589,
    "LABEL_59": 59,
    "LABEL_590": 590,
    "LABEL_591": 591,
    "LABEL_592": 592,
    "LABEL_593": 593,
    "LABEL_594": 594,
    "LABEL_595": 595,
    "LABEL_596": 596,
    "LABEL_597": 597,
    "LABEL_598": 598,
    "LABEL_599": 599,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_600": 600,
    "LABEL_601": 601,
    "LABEL_602": 602,
    "LABEL_603": 603,
    "LABEL_604": 604,
    "LABEL_605": 605,
    "LABEL_606": 606,
    "LABEL_607": 607,
    "LABEL_608": 608,
    "LABEL_609": 609,
    "LABEL_61": 61,
    "LABEL_610": 610,
    "LABEL_611": 611,
    "LABEL_612": 612,
    "LABEL_613": 613,
    "LABEL_614": 614,
    "LABEL_615": 615,
    "LABEL_616": 616,
    "LABEL_617": 617,
    "LABEL_618": 618,
    "LABEL_619": 619,
    "LABEL_62": 62,
    "LABEL_620": 620,
    "LABEL_621": 621,
    "LABEL_622": 622,
    "LABEL_623": 623,
    "LABEL_624": 624,
    "LABEL_625": 625,
    "LABEL_626": 626,
    "LABEL_627": 627,
    "LABEL_628": 628,
    "LABEL_629": 629,
    "LABEL_63": 63,
    "LABEL_630": 630,
    "LABEL_631": 631,
    "LABEL_632": 632,
    "LABEL_633": 633,
    "LABEL_634": 634,
    "LABEL_635": 635,
    "LABEL_636": 636,
    "LABEL_637": 637,
    "LABEL_638": 638,
    "LABEL_639": 639,
    "LABEL_64": 64,
    "LABEL_640": 640,
    "LABEL_641": 641,
    "LABEL_642": 642,
    "LABEL_643": 643,
    "LABEL_644": 644,
    "LABEL_645": 645,
    "LABEL_646": 646,
    "LABEL_647": 647,
    "LABEL_648": 648,
    "LABEL_649": 649,
    "LABEL_65": 65,
    "LABEL_650": 650,
    "LABEL_651": 651,
    "LABEL_652": 652,
    "LABEL_653": 653,
    "LABEL_654": 654,
    "LABEL_655": 655,
    "LABEL_656": 656,
    "LABEL_657": 657,
    "LABEL_658": 658,
    "LABEL_659": 659,
    "LABEL_66": 66,
    "LABEL_660": 660,
    "LABEL_661": 661,
    "LABEL_662": 662,
    "LABEL_663": 663,
    "LABEL_664": 664,
    "LABEL_665": 665,
    "LABEL_666": 666,
    "LABEL_667": 667,
    "LABEL_668": 668,
    "LABEL_669": 669,
    "LABEL_67": 67,
    "LABEL_670": 670,
    "LABEL_671": 671,
    "LABEL_672": 672,
    "LABEL_673": 673,
    "LABEL_674": 674,
    "LABEL_675": 675,
    "LABEL_676": 676,
    "LABEL_677": 677,
    "LABEL_678": 678,
    "LABEL_679": 679,
    "LABEL_68": 68,
    "LABEL_680": 680,
    "LABEL_681": 681,
    "LABEL_682": 682,
    "LABEL_683": 683,
    "LABEL_684": 684,
    "LABEL_685": 685,
    "LABEL_686": 686,
    "LABEL_687": 687,
    "LABEL_688": 688,
    "LABEL_689": 689,
    "LABEL_69": 69,
    "LABEL_690": 690,
    "LABEL_691": 691,
    "LABEL_692": 692,
    "LABEL_693": 693,
    "LABEL_694": 694,
    "LABEL_695": 695,
    "LABEL_696": 696,
    "LABEL_697": 697,
    "LABEL_698": 698,
    "LABEL_699": 699,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_700": 700,
    "LABEL_701": 701,
    "LABEL_702": 702,
    "LABEL_703": 703,
    "LABEL_704": 704,
    "LABEL_705": 705,
    "LABEL_706": 706,
    "LABEL_707": 707,
    "LABEL_708": 708,
    "LABEL_709": 709,
    "LABEL_71": 71,
    "LABEL_710": 710,
    "LABEL_711": 711,
    "LABEL_712": 712,
    "LABEL_713": 713,
    "LABEL_714": 714,
    "LABEL_715": 715,
    "LABEL_716": 716,
    "LABEL_717": 717,
    "LABEL_718": 718,
    "LABEL_719": 719,
    "LABEL_72": 72,
    "LABEL_720": 720,
    "LABEL_721": 721,
    "LABEL_722": 722,
    "LABEL_723": 723,
    "LABEL_724": 724,
    "LABEL_725": 725,
    "LABEL_726": 726,
    "LABEL_727": 727,
    "LABEL_728": 728,
    "LABEL_729": 729,
    "LABEL_73": 73,
    "LABEL_730": 730,
    "LABEL_731": 731,
    "LABEL_732": 732,
    "LABEL_733": 733,
    "LABEL_734": 734,
    "LABEL_735": 735,
    "LABEL_736": 736,
    "LABEL_737": 737,
    "LABEL_738": 738,
    "LABEL_739": 739,
    "LABEL_74": 74,
    "LABEL_740": 740,
    "LABEL_741": 741,
    "LABEL_742": 742,
    "LABEL_743": 743,
    "LABEL_744": 744,
    "LABEL_745": 745,
    "LABEL_746": 746,
    "LABEL_747": 747,
    "LABEL_748": 748,
    "LABEL_749": 749,
    "LABEL_75": 75,
    "LABEL_750": 750,
    "LABEL_751": 751,
    "LABEL_752": 752,
    "LABEL_753": 753,
    "LABEL_754": 754,
    "LABEL_755": 755,
    "LABEL_756": 756,
    "LABEL_757": 757,
    "LABEL_758": 758,
    "LABEL_759": 759,
    "LABEL_76": 76,
    "LABEL_760": 760,
    "LABEL_761": 761,
    "LABEL_762": 762,
    "LABEL_763": 763,
    "LABEL_764": 764,
    "LABEL_765": 765,
    "LABEL_766": 766,
    "LABEL_767": 767,
    "LABEL_768": 768,
    "LABEL_769": 769,
    "LABEL_77": 77,
    "LABEL_770": 770,
    "LABEL_771": 771,
    "LABEL_772": 772,
    "LABEL_773": 773,
    "LABEL_774": 774,
    "LABEL_775": 775,
    "LABEL_776": 776,
    "LABEL_777": 777,
    "LABEL_778": 778,
    "LABEL_779": 779,
    "LABEL_78": 78,
    "LABEL_780": 780,
    "LABEL_781": 781,
    "LABEL_782": 782,
    "LABEL_783": 783,
    "LABEL_784": 784,
    "LABEL_785": 785,
    "LABEL_786": 786,
    "LABEL_787": 787,
    "LABEL_788": 788,
    "LABEL_789": 789,
    "LABEL_79": 79,
    "LABEL_790": 790,
    "LABEL_791": 791,
    "LABEL_792": 792,
    "LABEL_793": 793,
    "LABEL_794": 794,
    "LABEL_795": 795,
    "LABEL_796": 796,
    "LABEL_797": 797,
    "LABEL_798": 798,
    "LABEL_799": 799,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_800": 800,
    "LABEL_801": 801,
    "LABEL_802": 802,
    "LABEL_803": 803,
    "LABEL_804": 804,
    "LABEL_805": 805,
    "LABEL_806": 806,
    "LABEL_807": 807,
    "LABEL_808": 808,
    "LABEL_809": 809,
    "LABEL_81": 81,
    "LABEL_810": 810,
    "LABEL_811": 811,
    "LABEL_812": 812,
    "LABEL_813": 813,
    "LABEL_814": 814,
    "LABEL_815": 815,
    "LABEL_816": 816,
    "LABEL_817": 817,
    "LABEL_818": 818,
    "LABEL_819": 819,
    "LABEL_82": 82,
    "LABEL_820": 820,
    "LABEL_821": 821,
    "LABEL_822": 822,
    "LABEL_823": 823,
    "LABEL_824": 824,
    "LABEL_825": 825,
    "LABEL_826": 826,
    "LABEL_827": 827,
    "LABEL_828": 828,
    "LABEL_829": 829,
    "LABEL_83": 83,
    "LABEL_830": 830,
    "LABEL_831": 831,
    "LABEL_832": 832,
    "LABEL_833": 833,
    "LABEL_834": 834,
    "LABEL_835": 835,
    "LABEL_836": 836,
    "LABEL_837": 837,
    "LABEL_838": 838,
    "LABEL_839": 839,
    "LABEL_84": 84,
    "LABEL_840": 840,
    "LABEL_841": 841,
    "LABEL_842": 842,
    "LABEL_843": 843,
    "LABEL_844": 844,
    "LABEL_845": 845,
    "LABEL_846": 846,
    "LABEL_847": 847,
    "LABEL_848": 848,
    "LABEL_849": 849,
    "LABEL_85": 85,
    "LABEL_850": 850,
    "LABEL_851": 851,
    "LABEL_852": 852,
    "LABEL_853": 853,
    "LABEL_854": 854,
    "LABEL_855": 855,
    "LABEL_856": 856,
    "LABEL_857": 857,
    "LABEL_858": 858,
    "LABEL_859": 859,
    "LABEL_86": 86,
    "LABEL_860": 860,
    "LABEL_861": 861,
    "LABEL_862": 862,
    "LABEL_863": 863,
    "LABEL_864": 864,
    "LABEL_865": 865,
    "LABEL_866": 866,
    "LABEL_867": 867,
    "LABEL_868": 868,
    "LABEL_869": 869,
    "LABEL_87": 87,
    "LABEL_870": 870,
    "LABEL_871": 871,
    "LABEL_872": 872,
    "LABEL_873": 873,
    "LABEL_874": 874,
    "LABEL_875": 875,
    "LABEL_876": 876,
    "LABEL_877": 877,
    "LABEL_878": 878,
    "LABEL_879": 879,
    "LABEL_88": 88,
    "LABEL_880": 880,
    "LABEL_881": 881,
    "LABEL_882": 882,
    "LABEL_883": 883,
    "LABEL_884": 884,
    "LABEL_885": 885,
    "LABEL_886": 886,
    "LABEL_887": 887,
    "LABEL_888": 888,
    "LABEL_889": 889,
    "LABEL_89": 89,
    "LABEL_890": 890,
    "LABEL_891": 891,
    "LABEL_892": 892,
    "LABEL_893": 893,
    "LABEL_894": 894,
    "LABEL_895": 895,
    "LABEL_896": 896,
    "LABEL_897": 897,
    "LABEL_898": 898,
    "LABEL_899": 899,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_900": 900,
    "LABEL_901": 901,
    "LABEL_902": 902,
    "LABEL_903": 903,
    "LABEL_904": 904,
    "LABEL_905": 905,
    "LABEL_906": 906,
    "LABEL_907": 907,
    "LABEL_908": 908,
    "LABEL_909": 909,
    "LABEL_91": 91,
    "LABEL_910": 910,
    "LABEL_911": 911,
    "LABEL_912": 912,
    "LABEL_913": 913,
    "LABEL_914": 914,
    "LABEL_915": 915,
    "LABEL_916": 916,
    "LABEL_917": 917,
    "LABEL_918": 918,
    "LABEL_919": 919,
    "LABEL_92": 92,
    "LABEL_920": 920,
    "LABEL_921": 921,
    "LABEL_922": 922,
    "LABEL_923": 923,
    "LABEL_924": 924,
    "LABEL_925": 925,
    "LABEL_926": 926,
    "LABEL_927": 927,
    "LABEL_928": 928,
    "LABEL_929": 929,
    "LABEL_93": 93,
    "LABEL_930": 930,
    "LABEL_931": 931,
    "LABEL_932": 932,
    "LABEL_933": 933,
    "LABEL_934": 934,
    "LABEL_935": 935,
    "LABEL_936": 936,
    "LABEL_937": 937,
    "LABEL_938": 938,
    "LABEL_939": 939,
    "LABEL_94": 94,
    "LABEL_940": 940,
    "LABEL_941": 941,
    "LABEL_942": 942,
    "LABEL_943": 943,
    "LABEL_944": 944,
    "LABEL_945": 945,
    "LABEL_946": 946,
    "LABEL_947": 947,
    "LABEL_948": 948,
    "LABEL_949": 949,
    "LABEL_95": 95,
    "LABEL_950": 950,
    "LABEL_951": 951,
    "LABEL_952": 952,
    "LABEL_953": 953,
    "LABEL_954": 954,
    "LABEL_955": 955,
    "LABEL_956": 956,
    "LABEL_957": 957,
    "LABEL_958": 958,
    "LABEL_959": 959,
    "LABEL_96": 96,
    "LABEL_960": 960,
    "LABEL_961": 961,
    "LABEL_962": 962,
    "LABEL_963": 963,
    "LABEL_964": 964,
    "LABEL_965": 965,
    "LABEL_966": 966,
    "LABEL_967": 967,
    "LABEL_968": 968,
    "LABEL_969": 969,
    "LABEL_97": 97,
    "LABEL_970": 970,
    "LABEL_971": 971,
    "LABEL_972": 972,
    "LABEL_973": 973,
    "LABEL_974": 974,
    "LABEL_975": 975,
    "LABEL_976": 976,
    "LABEL_977": 977,
    "LABEL_978": 978,
    "LABEL_979": 979,
    "LABEL_98": 98,
    "LABEL_980": 980,
    "LABEL_981": 981,
    "LABEL_982": 982,
    "LABEL_983": 983,
    "LABEL_984": 984,
    "LABEL_985": 985,
    "LABEL_986": 986,
    "LABEL_987": 987,
    "LABEL_988": 988,
    "LABEL_989": 989,
    "LABEL_99": 99,
    "LABEL_990": 990,
    "LABEL_991": 991,
    "LABEL_992": 992,
    "LABEL_993": 993,
    "LABEL_994": 994,
    "LABEL_995": 995,
    "LABEL_996": 996,
    "LABEL_997": 997,
    "LABEL_998": 998,
    "LABEL_999": 999
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "onnx_export": false,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.35.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

01/22/2024 00:17:47 - INFO - __main__ - setting problem type to single_label_classification
tokenizer_config.json:   0%|          | 0.00/447 [00:00<?, ?B/s]tokenizer_config.json: 100%|██████████| 447/447 [00:00<00:00, 897kB/s]
[INFO|configuration_utils.py:717] 2024-01-22 00:17:47,870 >> loading configuration file config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/config.json
[INFO|configuration_utils.py:777] 2024-01-22 00:17:47,871 >> Model config LongformerConfig {
  "_name_or_path": "thunlp/Lawformer",
  "architectures": [
    "LongformerForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "onnx_export": false,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.35.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 269k/269k [00:00<00:00, 997kB/s]tokenizer.json: 100%|██████████| 269k/269k [00:00<00:00, 995kB/s]
special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 981kB/s]
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:17:48,992 >> loading file vocab.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:17:48,992 >> loading file merges.txt from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:17:48,992 >> loading file tokenizer.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:17:48,992 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:17:48,992 >> loading file special_tokens_map.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:17:48,992 >> loading file tokenizer_config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/tokenizer_config.json
[INFO|configuration_utils.py:717] 2024-01-22 00:17:48,993 >> loading configuration file config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/config.json
[INFO|configuration_utils.py:777] 2024-01-22 00:17:48,993 >> Model config LongformerConfig {
  "_name_or_path": "thunlp/Lawformer",
  "architectures": [
    "LongformerForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "onnx_export": false,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.35.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

pytorch_model.bin:   0%|          | 0.00/505M [00:00<?, ?B/s]pytorch_model.bin:   2%|▏         | 10.5M/505M [00:00<00:43, 11.3MB/s]pytorch_model.bin:   4%|▍         | 21.0M/505M [00:01<00:28, 17.2MB/s]pytorch_model.bin:   6%|▌         | 31.5M/505M [00:01<00:20, 23.0MB/s]pytorch_model.bin:   8%|▊         | 41.9M/505M [00:01<00:16, 27.4MB/s]pytorch_model.bin:  10%|█         | 52.4M/505M [00:02<00:15, 30.0MB/s]pytorch_model.bin:  12%|█▏        | 62.9M/505M [00:02<00:17, 25.7MB/s]pytorch_model.bin:  15%|█▍        | 73.4M/505M [00:03<00:17, 24.7MB/s]pytorch_model.bin:  17%|█▋        | 83.9M/505M [00:03<00:17, 24.1MB/s]pytorch_model.bin:  19%|█▊        | 94.4M/505M [00:04<00:17, 23.6MB/s]pytorch_model.bin:  21%|██        | 105M/505M [00:04<00:16, 23.6MB/s] pytorch_model.bin:  23%|██▎       | 115M/505M [00:04<00:16, 23.5MB/s]pytorch_model.bin:  25%|██▍       | 126M/505M [00:05<00:16, 23.4MB/s]pytorch_model.bin:  27%|██▋       | 136M/505M [00:05<00:15, 24.5MB/s]pytorch_model.bin:  29%|██▉       | 147M/505M [00:06<00:14, 24.1MB/s]pytorch_model.bin:  31%|███       | 157M/505M [00:06<00:14, 24.0MB/s]pytorch_model.bin:  33%|███▎      | 168M/505M [00:07<00:13, 25.0MB/s]pytorch_model.bin:  35%|███▌      | 178M/505M [00:07<00:13, 24.7MB/s]pytorch_model.bin:  37%|███▋      | 189M/505M [00:07<00:13, 24.3MB/s]pytorch_model.bin:  39%|███▉      | 199M/505M [00:08<00:12, 25.1MB/s]pytorch_model.bin:  41%|████▏     | 210M/505M [00:08<00:11, 24.8MB/s]pytorch_model.bin:  44%|████▎     | 220M/505M [00:09<00:11, 25.7MB/s]pytorch_model.bin:  46%|████▌     | 231M/505M [00:09<00:10, 25.0MB/s]pytorch_model.bin:  48%|████▊     | 241M/505M [00:09<00:10, 25.7MB/s]pytorch_model.bin:  50%|████▉     | 252M/505M [00:10<00:10, 25.2MB/s]pytorch_model.bin:  52%|█████▏    | 262M/505M [00:10<00:09, 25.8MB/s]pytorch_model.bin:  54%|█████▍    | 273M/505M [00:11<00:09, 25.3MB/s]pytorch_model.bin:  56%|█████▌    | 283M/505M [00:11<00:08, 26.0MB/s]pytorch_model.bin:  58%|█████▊    | 294M/505M [00:12<00:08, 25.3MB/s]pytorch_model.bin:  60%|██████    | 304M/505M [00:12<00:07, 26.0MB/s]pytorch_model.bin:  62%|██████▏   | 315M/505M [00:12<00:07, 25.1MB/s]pytorch_model.bin:  64%|██████▍   | 325M/505M [00:13<00:06, 26.0MB/s]pytorch_model.bin:  66%|██████▋   | 336M/505M [00:13<00:06, 24.5MB/s]pytorch_model.bin:  68%|██████▊   | 346M/505M [00:14<00:06, 26.3MB/s]pytorch_model.bin:  71%|███████   | 357M/505M [00:14<00:05, 25.5MB/s]pytorch_model.bin:  73%|███████▎  | 367M/505M [00:14<00:05, 26.1MB/s]pytorch_model.bin:  75%|███████▍  | 377M/505M [00:15<00:05, 25.4MB/s]pytorch_model.bin:  77%|███████▋  | 388M/505M [00:15<00:04, 26.0MB/s]pytorch_model.bin:  79%|███████▉  | 398M/505M [00:16<00:04, 26.1MB/s]pytorch_model.bin:  81%|████████  | 409M/505M [00:16<00:03, 26.0MB/s]pytorch_model.bin:  83%|████████▎ | 419M/505M [00:16<00:03, 24.8MB/s]pytorch_model.bin:  85%|████████▌ | 430M/505M [00:17<00:03, 24.2MB/s]pytorch_model.bin:  87%|████████▋ | 440M/505M [00:17<00:02, 22.5MB/s]pytorch_model.bin:  89%|████████▉ | 451M/505M [00:18<00:02, 22.5MB/s]pytorch_model.bin:  91%|█████████▏| 461M/505M [00:18<00:02, 21.7MB/s]pytorch_model.bin:  93%|█████████▎| 472M/505M [00:19<00:01, 22.0MB/s]pytorch_model.bin:  95%|█████████▌| 482M/505M [00:19<00:01, 22.3MB/s]pytorch_model.bin:  98%|█████████▊| 493M/505M [00:20<00:00, 22.6MB/s]pytorch_model.bin: 100%|█████████▉| 503M/505M [00:20<00:00, 21.7MB/s]pytorch_model.bin: 100%|██████████| 505M/505M [00:20<00:00, 24.2MB/s]
[INFO|modeling_utils.py:3121] 2024-01-22 00:18:11,112 >> loading weights file pytorch_model.bin from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/pytorch_model.bin
[INFO|modeling_utils.py:3940] 2024-01-22 00:18:12,938 >> Some weights of the model checkpoint at thunlp/Lawformer were not used when initializing LongformerForSequenceClassification: ['longformer.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3952] 2024-01-22 00:18:12,938 >> Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at thunlp/Lawformer and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/22/2024 00:18:12 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.
Running tokenizer on dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-11f2481be46e549b.arrow
01/22/2024 00:18:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-11f2481be46e549b.arrow
Running tokenizer on dataset:  25%|██▌       | 1000/4000 [00:03<00:09, 331.63 examples/s]Running tokenizer on dataset:  50%|█████     | 2000/4000 [00:03<00:03, 579.49 examples/s]Running tokenizer on dataset:  75%|███████▌  | 3000/4000 [00:04<00:01, 798.05 examples/s]Running tokenizer on dataset: 100%|██████████| 4000/4000 [00:05<00:00, 1004.84 examples/s]Running tokenizer on dataset: 100%|██████████| 4000/4000 [00:05<00:00, 772.34 examples/s] 
Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e733d9677fe7bea2.arrow
01/22/2024 00:18:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e733d9677fe7bea2.arrow
Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 1600.41 examples/s]Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 1559.60 examples/s]
Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-088738feb980c3a7.arrow
01/22/2024 00:18:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-ff3709bd4e83e3e3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-088738feb980c3a7.arrow
Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 1657.40 examples/s]Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 1614.31 examples/s]
01/22/2024 00:18:18 - INFO - __main__ - Sample 2619 of the training set: {'date': '2020-03-30', ' verdict': '湖南省衡阳市蒸湘区人民法院刑 事 判 决 书（2020）湘0408刑初64号公诉机关衡阳市蒸湘区人民检察院。被告人彭杰，男，2000年8月25日出生，汉族，高中文化。2019年10月7日因涉嫌犯盗窃罪被衡阳市公安局高新分局刑事拘留，2019年11月6日经衡阳市蒸湘区人民检察院批准由衡阳市公安局高新分局执行逮捕。现羁押于衡阳市看守所。辩护人刘祝明,湖南德巍律师事务所律师。衡阳市蒸湘区人民检察院以衡蒸检公诉刑诉[2020]22号起诉书指控被告人彭杰犯盗窃罪、寻衅滋事罪，于2020年2月26日向本院提起公诉。本院依法组成合议庭，公开开庭审理了本案。衡阳市蒸湘区人民检察院指派检察员常靖出庭支持公诉，被告人彭杰及其辩护人刘祝明到庭参加诉讼。现已审理终结。经审理查明：2019年7月22日20时许，被告人彭杰受吴某1委托，在娄底市娄星区华达阿波罗网吧内拦住准备离开的被害人柳某、吴某2，双方发生口角冲突，在柳某、吴某2骑摩托车离开后，彭杰觉得自己被柳某威胁，心生不满，于是便叫来吴某1、王某、贺某等人，并通过ＱＱ联系被害人柳某、吴某2，要求柳某向自己当面道歉。在当晚22时许，双方仍然约在阿波罗网吧见面，被告人彭杰与贺某、王某、吴某1对被害人柳某、吴某2进行殴打，经鉴定被害人柳某的伤势为轻伤二级。2019年9月17日凌晨2时许，被告人彭杰伙同柳威（2003年3月8日出生，另案处理）、肖金明（身份不详，在逃）在娄底市双峰县宾园街399号11栋楼下，将被害人曾某的福特福瑞斯小车右后窗玻璃砸碎，从车内盗走9盒和气生财香烟（经价格认定为450元）和400余元现金。在永丰镇阳光大厦楼下，将被害人贺某的丰田卡罗拉小车右后窗玻璃砸碎，从车内盗走8盒黄色芙蓉王香烟（经价格认定为200元）。在走马街镇人人乐超市前，将被害人谢某的途观越野车左后方玻璃砸碎，在车内未盗取到财物。在走马街镇伍家粥铺的空坪内，将被害人李某1的路虎越野车左后方玻璃砸碎，在车内未盗取到财物。2019年9月25日凌晨，被告人彭杰伙同柳威、姚达峰、外号“包爷”、外号“小矮”等人商议一起实施盗窃，随后几人分成两批在株洲醴陵市内盗窃财物。在株洲醴陵市征稽所门口，将被害人李某2的雪铁龙右后窗玻璃砸碎，从车内盗走2盒硬盒蓝芙蓉王香烟（经价格认定为68元）、1盒硬盒和天下香烟（经价格认定为100元）、1盒硬盒和气生财香烟（经价格认定为50元）。在仙岳山办事处前进街附近，将被害人叶某的奔驰越野车右后窗玻璃砸碎，从车内盗走3条硬盒和天下香烟（经价格认定为2700元）、1条软盒和天下香烟（经价格认定为1000元）和3000元现金。在仙岳山办事处碧山广场三岔路口附近，将被害人殷某的奥德赛小车左后窗玻璃砸碎，盗走4瓶飞天茅台酒（经价格认定为3996元）和500元现金。在仙岳山办事处左权南路附近，将被害人陈某的大众途观ＳＵＶ右后窗玻璃砸碎，从车内盗走4盒和气生财香烟（经价格认定为200元）、2瓶53度飞天茅台酒（经价格认定为4800元）和现金300元。在金杏美域美宜佳门口，将被害人易某的起亚汽车右窗玻璃砸碎，从车内盗走200元现金。将被害人张某1的大众汽车左后窗玻璃砸碎，从车内盗走2条软盒和天下香烟（经价格认定为2000元）。在河西大厦小区门口，将被害人李云飞丰田霸道右窗玻璃砸碎，从车内盗走2条细支硬盒和天下（经价格认定为2000元）、1条硬盒和天下（经价格认定为900元）、1条白色软盒和天下（经价格认定为1000元）、2盒软盒和天下（经价格认定为200元）、3盒白色软盒和天下（经价格认定为300元）、2瓶43度飞天茅台酒（经价格认定为1998元）。在醴陵市瑞和红木门口，将被害人张某2的普拉多小车右后窗玻璃砸碎，从车内盗走3瓶五粮液白酒（经价格认定为1614元）。在阳三石办事处鑫源广场附近，将被害人张某3的名爵越野车右后窗玻璃砸碎，盗走1条黄色芙蓉王香烟。2019年9月29日凌晨，被告人彭杰伙同柳威、曾波（另案处理）、徐光裕（另案处理)在衡阳市蒸湘区盗窃车内财物，被衡阳市公安局高新开发区分局巡逻民警发现，柳威被现场抓获，被告人彭杰趁机逃脱。2019年10月6日民警在娄底市将被告人彭杰抓获归案。综上，被告人彭杰盗窃财物价值总计为27976元。另查明,公安机关于2019年10月6日扣押了被告人彭杰作案使用的刀具、安全锤等。上述事实，被告人彭杰在开庭审理过程中亦无异议，且有书证、证人证言、被害人的陈述、鉴定意见、现场勘验笔录、辨认笔录、被告人彭杰的供述与辩解等证据证实，足以认定。本院认为，被告人彭杰以非法占有为目的，伙同未成年人采取破坏性手段多次秘密窃取他人财物，数额较大，其行为已构成盗窃罪。被告人彭杰随意殴打他人，致一人轻伤二级，情节恶劣，其行为已构成寻衅滋事罪。衡阳市蒸湘区人民检察院指控被告人彭杰犯盗窃罪、寻衅滋事罪，事实清楚，定性准确，证据确实、充分，其指控的意见成立，本院予以支持。本案的盗窃及寻衅滋事作案均系共同犯罪，被告人彭杰在共同犯罪中起主要作用，系主犯，应当按照其所参与的全部犯罪处罚。被告人彭杰归案后能如实供述自己的犯罪事实，有坦白情节，且自愿认罪认罚,依法可以从轻处罚。公诉机关关于对被告人彭杰以盗窃罪判处一年六个月以上三年以下有期徒刑，并处罚金，以寻衅滋事罪判处一年六个月以下有期徒刑量刑建议，符合法律规定，本院予以支持。为打击刑事犯罪，保护公民的人身权利、财产权利不受侵犯，根据被告人彭杰的犯罪事实、性质、情节以及对社会的危害程度，依照《中华人民共和国刑法》第二百六十四条、第二百九十三条、第二十五条第一款、第二十六条第一、四款、第六十七条第三款、第六十九条第一款、第四十五条、第四十七条、第五十二条、第五十三条第一款、第六十四条，《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》第一条第一、二款、第二条第（三）项、第三条第一款之规定，判决如下：一、被告人彭杰犯盗窃罪，判处有期徒刑二年一个月，并处罚金人民币五千元；犯寻衅滋事罪，判处有期徒刑一年二个月；决定执行有期徒刑三年，并处罚金人民币五千元（刑期从判决执行之日起计算；判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2019年10月6日起至2022年10月5日止；罚金限判决生效后三十日内缴纳）；二、公安机关扣押被告人彭杰作案使用的刀具、安全锤等，予以没收。如不服本判决，可在接到判决书的第二日起十日内，通过本院或者直接向衡阳市中级人民法院提出上诉。书面上诉的，应当提交上诉状正本一份，副本五份。审\u3000判\u3000长\u3000\u3000曾佑荣审\u3000判\u3000员\u3000\u3000唐文婧人民陪审员\u3000\u3000刘瑞金二〇二〇年三月三十日法官助理陈彬彬书记员夏欣附：相关法律条文《中华人民共和国刑法》第二百六十四条盗窃公私财物，数额较大的，或者多次盗窃、入户盗窃、携带凶器盗窃、扒窃的，处三年以下有期徒刑、拘役或者管制，并处或者单处罚金;数额巨大或者有其他严重情节的，处三年以上十年以下有期徒刑，并处罚金;数额特别巨大或者有其他特别严重情节的，处十年以上有期徒刑或者无期徒刑，并处罚金或者没收财产。第二百九十三条有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。第二十五条第一款共同犯罪是指二人以上共同故意犯罪。第二十六条第一款组织、领导犯罪集团进行犯罪活动的或者在共同犯罪中起主要作用的，是主犯。第四款对于第三款规定以外的主犯，应当按照其所参与的或者组织、指挥的全部犯罪处罚。第六十七条第三款犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。第六十九条第一款判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。第四十五条有期徒刑的期限，除本法第五十条、第六十九条规定外，为六个月以上十五年以下。第四十七条有期徒刑的刑期，从判决执行之日起计算;判决执行以前先行羁押的，羁押一日折抵刑期一日。第五十二条判处罚金，应当根据犯罪情节决定罚金数额。第五十三条第一款罚金在判决指定的期限内一次或者分期缴纳。期满不缴纳的，强制缴纳。对于不能全部缴纳罚金的，人民法院在任何时候发现被执行人有可以执行的财产，应当随时追缴。第六十四条犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔；对被害人的合法财产，应当及时返还；违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》第一条第一款、第二款盗窃公私财物价值一千元至三千元以上、三万元至十万元以上、三十万元至五十万元以上的，应当分别认定为刑法第二百六十四条规定的"数额较大"、"数额巨大"、"数额特别巨大"。各省、自治区、直辖市高级人民法院、人民检察院可以根据本地区经济发展状况，并考虑社会治安状况，在前款规定的数额幅度内，确定本地区执行的具体数额标准，报最高人民法院、最高人民检察院批准。第二条盗窃公私财物，具有下列情形之一的，"数额较大"的标准可以按照前条规定标准的百分之五十确定:…(三)组织、控制未成年人盗窃的;…第三条第一款二年内盗窃三次以上的，应当认定为"多次盗窃"。', 'defendant': '[CLS]彭杰[SEP]2019年7月22日20时许，被告人彭杰受吴某1委托，在娄底市娄星区华达阿波罗网吧内拦住准备离开的被害人柳某、吴某2，双方发生口角冲突，在柳某、吴某2骑摩托车离开后，彭杰觉得自己被柳某威胁，心生不满，于是便叫来吴某1、王某、贺某等人，并通过ＱＱ联系被害人柳某、吴某2，要求柳某向自己当面道歉。在当晚22时许，双方仍然约在阿波罗网吧见面，被告人彭杰与贺某、王某、吴某1对被害人柳某、吴某2进行殴打，经鉴定被害人柳某的伤势为轻伤二级。2019年9月17日凌晨2时许，被告人彭杰伙同柳威（2003年3月8日出生，另案处理）、肖金明（身份不详，在逃）在娄底市双峰县宾园街399号11栋楼下，将被害人曾某的福特福瑞斯小车右后窗玻璃砸碎，从车内盗走9盒和气生财香烟（经价格认定为450元）和400余元现金。在永丰镇阳光大厦楼下，将被害人贺某的丰田卡罗拉小车右后窗玻璃砸碎，从车内盗走8盒黄色芙蓉王香烟（经价格认定为200元）。在走马街镇人人乐超市前，将被害人谢某的途观越野车左后方玻璃砸碎，在车内未盗取到财物。在走马街镇伍家粥铺的空坪内，将被害人李某1的路虎越野车左后方玻璃砸碎，在车内未盗取到财物。2019年9月25日凌晨，被告人彭杰伙同柳威、姚达峰、外号“包爷”、外号“小矮”等人商议一起实施盗窃，随后几人分成两批在株洲醴陵市内盗窃财物。在株洲醴陵市征稽所门口，将被害人李某2的雪铁龙右后窗玻璃砸碎，从车内盗走2盒硬盒蓝芙蓉王香烟（经价格认定为68元）、1盒硬盒和天下香烟（经价格认定为100元）、1盒硬盒和气生财香烟（经价格认定为50元）。在仙岳山办事处前进街附近，将被害人叶某的奔驰越野车右后窗玻璃砸碎，从车内盗走3条硬盒和天下香烟（经价格认定为2700元）、1条软盒和天下香烟（经价格认定为1000元）和3000元现金。在仙岳山办事处碧山广场三岔路口附近，将被害人殷某的奥德赛小车左后窗玻璃砸碎，盗走4瓶飞天茅台酒（经价格认定为3996元）和500元现金。在仙岳山办事处左权南路附近，将被害人陈某的大众途观ＳＵＶ右后窗玻璃砸碎，从车内盗走4盒和气生财香烟（经价格认定为200元）、2瓶53度飞天茅台酒（经价格认定为4800元）和现金300元。在金杏美域美宜佳门口，将被害人易某的起亚汽车右窗玻璃砸碎，从车内盗走200元现金。将被害人张某1的大众汽车左后窗玻璃砸碎，从车内盗走2条软盒和天下香烟（经价格认定为2000元）。在河西大厦小区门口，将被害人李云飞丰田霸道右窗玻璃砸碎，从车内盗走2条细支硬盒和天下（经价格认定为2000元）、1条硬盒和天下（经价格认定为900元）、1条白色软盒和天下（经价格认定为1000元）、2盒软盒和天下（经价格认定为200元）、3盒白色软盒和天下（经价格认定为300元）、2瓶43度飞天茅台酒（经价格认定为1998元）。在醴陵市瑞和红木门口，将被害人张某2的普拉多小车右后窗玻璃砸碎，从车内盗走3瓶五粮液白酒（经价格认定为1614元）。在阳三石办事处鑫源广场附近，将被害人张某3的名爵越野车右后窗玻璃砸碎，盗走1条黄色芙蓉王香烟。2019年9月29日凌晨，被告人彭杰伙同柳威、曾波（另案处理）、徐光裕（另案处理)在衡阳市蒸湘区盗窃车内财物，被衡阳市公安局高新开发区分局巡逻民警发现，柳威被现场抓获，被告人彭杰趁机逃脱。2019年10月6日民警在娄底市将被告人彭杰抓获归案。综上，被告人彭杰盗窃财物价值总计为27976元。另查明,公安机关于2019年10月6日扣押了被告人彭杰作案使用的刀具、安全锤等。上述事实，被告人彭杰在开庭审理过程中亦无异议，且有书证、证人证言、被害人的陈述、鉴定意见、现场勘验笔录、辨认笔录、被告人彭杰的供述与辩解等证据证实，足以认定', 'accusation': ["'盗窃',", "'寻衅滋事'"], 'article_content': "[{'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百六十四条', '内容': '【盗窃罪】盗窃公私财物，数额较大的，或者多次盗窃、入户盗窃、携带凶器盗窃、扒窃的，处三年以下有期徒刑、拘役或者管制，并处或者单处罚金；数额巨大或者有其他严重情节的，处三年以上十年以下有期徒刑，并处罚金；数额特别巨大或者有其他特别严重情节的，处十年以上有期徒刑或者无期徒刑，并处罚金或者没收财产。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十三条第一款', '内容': '【寻衅滋事罪】有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。纠集他人多次实施前款行为，严重破坏社会秩序的，处五年以上十年以下有期徒刑，可以并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十三条第一款第一项', '内容': '【寻衅滋事罪】有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。纠集他人多次实施前款行为，严重破坏社会秩序的，处五年以上十年以下有期徒刑，可以并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十三条第一款第二项', '内容': '【寻衅滋事罪】有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。纠集他人多次实施前款行为，严重破坏社会秩序的，处五年以上十年以下有期徒刑，可以并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十三条第一款第三项', '内容': '【寻衅滋事罪】有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。纠集他人多次实施前款行为，严重破坏社会秩序的，处五年以上十年以下有期徒刑，可以并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十三条第四项', '内容': '【寻衅滋事罪】有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。纠集他人多次实施前款行为，严重破坏社会秩序的，处五年以上十年以下有期徒刑，可以并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十三条第二款', '内容': '【寻衅滋事罪】有下列寻衅滋事行为之一，破坏社会秩序的，处五年以下有期徒刑、拘役或者管制：（一）随意殴打他人，情节恶劣的；（二）追逐、拦截、辱骂、恐吓他人，情节恶劣的；（三）强拿硬要或者任意损毁、占用公私财物，情节严重的；（四）在公共场所起哄闹事，造成公共场所秩序严重混乱的。纠集他人多次实施前款行为，严重破坏社会秩序的，处五年以上十年以下有期徒刑，可以并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十五条第一款', '内容': '【共同犯罪的概念】共同犯罪是指二人以上共同故意犯罪。二人以上共同过失犯罪，不以共同犯罪论处；应当负刑事责任的，按照他们所犯的罪分别处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十六条第一款', '内容': '【主犯】组织、领导犯罪集团进行犯罪活动的或者在共同犯罪中起主要作用的，是主犯。三人以上为共同实施犯罪而组成的较为固定的犯罪组织，是犯罪集团。对组织、领导犯罪集团的首要分子，按照集团所犯的全部罪行处罚。对于第三款规定以外的主犯，应当按照其所参与的或者组织、指挥的全部犯罪处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十六条第四款', '内容': '【主犯】组织、领导犯罪集团进行犯罪活动的或者在共同犯罪中起主要作用的，是主犯。三人以上为共同实施犯罪而组成的较为固定的犯罪组织，是犯罪集团。对组织、领导犯罪集团的首要分子，按照集团所犯的全部罪行处罚。对于第三款规定以外的主犯，应当按照其所参与的或者组织、指挥的全部犯罪处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十七条第三款', '内容': '【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。被采取强制措施的犯罪嫌疑人、被告人和正在服刑的罪犯，如实供述司法机关还未掌握的本人其他罪行的，以自首论。犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第一款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第四十五条', '内容': '【有期徒刑的期限】有期徒刑的期限，除本法第五十条、第六十九条规定外，为六个月以上十五年以下。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第四十七条', '内容': '【有期徒刑刑期的计算与折抵】有期徒刑的刑期，从判决执行之日起计算；判决执行以前先行羁押的，羁押一日折抵刑期一日。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十二条', '内容': '【罚金数额的裁量】判处罚金，应当根据犯罪情节决定罚金数额。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十三条第一款', '内容': '【罚金的缴纳】罚金在判决指定的期限内一次或者分期缴纳。期满不缴纳的，强制缴纳。对于不能全部缴纳罚金的，人民法院在任何时候发现被执行人有可以执行的财产，应当随时追缴。由于遭遇不能抗拒的灾祸等原因缴纳确实有困难的，经人民法院裁定，可以延期缴纳、酌情减少或者免除。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十四条', '内容': '【犯罪物品的处理】犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔；对被害人的合法财产，应当及时返还；违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。'}, {'法律名称': '《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》', '法律条文': '第一条第一款', '内容': ''}, {'法律名称': '《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》', '法律条文': '第一条第二款', '内容': ''}, {'法律名称': '《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》', '法律条文': '第二条第三项', '内容': ''}, {'法律名称': '《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》', '法律条文': '第三条第一款', '内容': ''}]", 'province': '湖南省', 'fact': '2019年7月22日20时许，被告人彭杰受吴某1委托，在娄底市娄星区华达阿波罗网吧内拦住准备离开的被害人柳某、吴某2，双方发生口角冲突，在柳某、吴某2骑摩托车离开后，彭杰觉得自己被柳某威胁，心生不满，于是便叫来吴某1、王某、贺某等人，并通过ＱＱ联系被害人柳某、吴某2，要求柳某向自己当面道歉。在当晚22时许，双方仍然约在阿波罗网吧见面，被告人彭杰与贺某、王某、吴某1对被害人柳某、吴某2进行殴打，经鉴定被害人柳某的伤势为轻伤二级。2019年9月17日凌晨2时许，被告人彭杰伙同柳威（2003年3月8日出生，另案处理）、肖金明（身份不详，在逃）在娄底市双峰县宾园街399号11栋楼下，将被害人曾某的福特福瑞斯小车右后窗玻璃砸碎，从车内盗走9盒和气生财香烟（经价格认定为450元）和400余元现金。在永丰镇阳光大厦楼下，将被害人贺某的丰田卡罗拉小车右后窗玻璃砸碎，从车内盗走8盒黄色芙蓉王香烟（经价格认定为200元）。在走马街镇人人乐超市前，将被害人谢某的途观越野车左后方玻璃砸碎，在车内未盗取到财物。在走马街镇伍家粥铺的空坪内，将被害人李某1的路虎越野车左后方玻璃砸碎，在车内未盗取到财物。2019年9月25日凌晨，被告人彭杰伙同柳威、姚达峰、外号“包爷”、外号“小矮”等人商议一起实施盗窃，随后几人分成两批在株洲醴陵市内盗窃财物。在株洲醴陵市征稽所门口，将被害人李某2的雪铁龙右后窗玻璃砸碎，从车内盗走2盒硬盒蓝芙蓉王香烟（经价格认定为68元）、1盒硬盒和天下香烟（经价格认定为100元）、1盒硬盒和气生财香烟（经价格认定为50元）。在仙岳山办事处前进街附近，将被害人叶某的奔驰越野车右后窗玻璃砸碎，从车内盗走3条硬盒和天下香烟（经价格认定为2700元）、1条软盒和天下香烟（经价格认定为1000元）和3000元现金。在仙岳山办事处碧山广场三岔路口附近，将被害人殷某的奥德赛小车左后窗玻璃砸碎，盗走4瓶飞天茅台酒（经价格认定为3996元）和500元现金。在仙岳山办事处左权南路附近，将被害人陈某的大众途观ＳＵＶ右后窗玻璃砸碎，从车内盗走4盒和气生财香烟（经价格认定为200元）、2瓶53度飞天茅台酒（经价格认定为4800元）和现金300元。在金杏美域美宜佳门口，将被害人易某的起亚汽车右窗玻璃砸碎，从车内盗走200元现金。将被害人张某1的大众汽车左后窗玻璃砸碎，从车内盗走2条软盒和天下香烟（经价格认定为2000元）。在河西大厦小区门口，将被害人李云飞丰田霸道右窗玻璃砸碎，从车内盗走2条细支硬盒和天下（经价格认定为2000元）、1条硬盒和天下（经价格认定为900元）、1条白色软盒和天下（经价格认定为1000元）、2盒软盒和天下（经价格认定为200元）、3盒白色软盒和天下（经价格认定为300元）、2瓶43度飞天茅台酒（经价格认定为1998元）。在醴陵市瑞和红木门口，将被害人张某2的普拉多小车右后窗玻璃砸碎，从车内盗走3瓶五粮液白酒（经价格认定为1614元）。在阳三石办事处鑫源广场附近，将被害人张某3的名爵越野车右后窗玻璃砸碎，盗走1条黄色芙蓉王香烟。2019年9月29日凌晨，被告人彭杰伙同柳威、曾波（另案处理）、徐光裕（另案处理)在衡阳市蒸湘区盗窃车内财物，被衡阳市公安局高新开发区分局巡逻民警发现，柳威被现场抓获，被告人彭杰趁机逃脱。2019年10月6日民警在娄底市将被告人彭杰抓获归案。综上，被告人彭杰盗窃财物价值总计为27976元。另查明,公安机关于2019年10月6日扣押了被告人彭杰作案使用的刀具、安全锤等。上述事实，被告人彭杰在开庭审理过程中亦无异议，且有书证、证人证言、被害人的陈述、鉴定意见、现场勘验笔录、辨认笔录、被告人彭杰的供述与辩解等证据证实，足以认定', 'label': 683, 'defendant_judgement': "[{'盗窃': {'有期徒刑': '二年一个月'}}, {'寻衅滋事': {'有期徒刑': '一年二个月'}}, {'有期徒刑': '三年'}]", 'relevant_article': ['293.0,', '264.0'], 'imprisonment': 36.0, 'sentence': '[CLS]彭杰[SEP]2019年7月22日20时许，被告人彭杰受吴某1委托，在娄底市娄星区华达阿波罗网吧内拦住准备离开的被害人柳某、吴某2，双方发生口角冲突，在柳某、吴某2骑摩托车离开后，彭杰觉得自己被柳某威胁，心生不满，于是便叫来吴某1、王某、贺某等人，并通过ＱＱ联系被害人柳某、吴某2，要求柳某向自己当面道歉。在当晚22时许，双方仍然约在阿波罗网吧见面，被告人彭杰与贺某、王某、吴某1对被害人柳某、吴某2进行殴打，经鉴定被害人柳某的伤势为轻伤二级。2019年9月17日凌晨2时许，被告人彭杰伙同柳威（2003年3月8日出生，另案处理）、肖金明（身份不详，在逃）在娄底市双峰县宾园街399号11栋楼下，将被害人曾某的福特福瑞斯小车右后窗玻璃砸碎，从车内盗走9盒和气生财香烟（经价格认定为450元）和400余元现金。在永丰镇阳光大厦楼下，将被害人贺某的丰田卡罗拉小车右后窗玻璃砸碎，从车内盗走8盒黄色芙蓉王香烟（经价格认定为200元）。在走马街镇人人乐超市前，将被害人谢某的途观越野车左后方玻璃砸碎，在车内未盗取到财物。在走马街镇伍家粥铺的空坪内，将被害人李某1的路虎越野车左后方玻璃砸碎，在车内未盗取到财物。2019年9月25日凌晨，被告人彭杰伙同柳威、姚达峰、外号“包爷”、外号“小矮”等人商议一起实施盗窃，随后几人分成两批在株洲醴陵市内盗窃财物。在株洲醴陵市征稽所门口，将被害人李某2的雪铁龙右后窗玻璃砸碎，从车内盗走2盒硬盒蓝芙蓉王香烟（经价格认定为68元）、1盒硬盒和天下香烟（经价格认定为100元）、1盒硬盒和气生财香烟（经价格认定为50元）。在仙岳山办事处前进街附近，将被害人叶某的奔驰越野车右后窗玻璃砸碎，从车内盗走3条硬盒和天下香烟（经价格认定为2700元）、1条软盒和天下香烟（经价格认定为1000元）和3000元现金。在仙岳山办事处碧山广场三岔路口附近，将被害人殷某的奥德赛小车左后窗玻璃砸碎，盗走4瓶飞天茅台酒（经价格认定为3996元）和500元现金。在仙岳山办事处左权南路附近，将被害人陈某的大众途观ＳＵＶ右后窗玻璃砸碎，从车内盗走4盒和气生财香烟（经价格认定为200元）、2瓶53度飞天茅台酒（经价格认定为4800元）和现金300元。在金杏美域美宜佳门口，将被害人易某的起亚汽车右窗玻璃砸碎，从车内盗走200元现金。将被害人张某1的大众汽车左后窗玻璃砸碎，从车内盗走2条软盒和天下香烟（经价格认定为2000元）。在河西大厦小区门口，将被害人李云飞丰田霸道右窗玻璃砸碎，从车内盗走2条细支硬盒和天下（经价格认定为2000元）、1条硬盒和天下（经价格认定为900元）、1条白色软盒和天下（经价格认定为1000元）、2盒软盒和天下（经价格认定为200元）、3盒白色软盒和天下（经价格认定为300元）、2瓶43度飞天茅台酒（经价格认定为1998元）。在醴陵市瑞和红木门口，将被害人张某2的普拉多小车右后窗玻璃砸碎，从车内盗走3瓶五粮液白酒（经价格认定为1614元）。在阳三石办事处鑫源广场附近，将被害人张某3的名爵越野车右后窗玻璃砸碎，盗走1条黄色芙蓉王香烟。2019年9月29日凌晨，被告人彭杰伙同柳威、曾波（另案处理）、徐光裕（另案处理)在衡阳市蒸湘区盗窃车内财物，被衡阳市公安局高新开发区分局巡逻民警发现，柳威被现场抓获，被告人彭杰趁机逃脱。2019年10月6日民警在娄底市将被告人彭杰抓获归案。综上，被告人彭杰盗窃财物价值总计为27976元。另查明,公安机关于2019年10月6日扣押了被告人彭杰作案使用的刀具、安全锤等。上述事实，被告人彭杰在开庭审理过程中亦无异议，且有书证、证人证言、被害人的陈述、鉴定意见、现场勘验笔录、辨认笔录、被告人彭杰的供述与辩解等证据证实，足以认定', 'input_ids': [101, 101, 2510, 3345, 102, 9160, 2399, 128, 3299, 8130, 3189, 8113, 3198, 6387, 8024, 6158, 1440, 782, 2510, 3345, 1358, 1426, 3378, 122, 1999, 2805, 8024, 1762, 2016, 2419, 2356, 2016, 3215, 1277, 1290, 6809, 7350, 3797, 5384, 5381, 1416, 1079, 2882, 857, 1114, 1906, 4895, 2458, 4638, 6158, 2154, 782, 3394, 3378, 510, 1426, 3378, 123, 8024, 1352, 3175, 1355, 4495, 1366, 6235, 1103, 4960, 8024, 1762, 3394, 3378, 510, 1426, 3378, 123, 7744, 3040, 2805, 6756, 4895, 2458, 1400, 8024, 2510, 3345, 6230, 2533, 5632, 2346, 6158, 3394, 3378, 2014, 5516, 8024, 2552, 4495, 679, 4007, 8024, 754, 3221, 912, 1373, 3341, 1426, 3378, 122, 510, 4374, 3378, 510, 6590, 3378, 5023, 782, 8024, 2400, 6858, 6814, 8067, 12826, 5468, 5143, 6158, 2154, 782, 3394, 3378, 510, 1426, 3378, 123, 8024, 6206, 3724, 3394, 3378, 1403, 5632, 2346, 2496, 7481, 6887, 3624, 511, 1762, 2496, 3241, 8130, 3198, 6387, 8024, 1352, 3175, 793, 4197, 5276, 1762, 7350, 3797, 5384, 5381, 1416, 6224, 7481, 8024, 6158, 1440, 782, 2510, 3345, 680, 6590, 3378, 510, 4374, 3378, 510, 1426, 3378, 122, 2190, 6158, 2154, 782, 3394, 3378, 510, 1426, 3378, 123, 6822, 6121, 3666, 2802, 8024, 5307, 7063, 2137, 6158, 2154, 782, 3394, 3378, 4638, 839, 1232, 711, 6768, 839, 753, 5277, 511, 9160, 2399, 130, 3299, 8126, 3189, 1119, 3247, 123, 3198, 6387, 8024, 6158, 1440, 782, 2510, 3345, 832, 1398, 3394, 2014, 8020, 8263, 2399, 124, 3299, 129, 3189, 1139, 4495, 8024, 1369, 3428, 1905, 4415, 8021, 510, 5494, 7032, 3209, 8020, 6716, 819, 679, 6422, 8024, 1762, 6845, 8021, 1762, 2016, 2419, 2356, 1352, 2292, 1344, 2161, 1736, 6125, 9612, 1384, 8111, 3406, 3517, 678, 8024, 2199, 6158, 2154, 782, 3295, 3378, 4638, 4886, 4294, 4886, 4448, 3172, 2207, 6756, 1381, 1400, 4970, 4390, 4461, 4790, 4810, 8024, 794, 6756, 1079, 4668, 6624, 130, 4665, 1469, 3698, 4495, 6568, 7676, 4170, 8020, 5307, 817, 3419, 6371, 2137, 711, 8802, 1039, 8021, 1469, 8230, 865, 1039, 4385, 7032, 511, 1762, 3719, 705, 7252, 7345, 1045, 1920, 1336, 3517, 678, 8024, 2199, 6158, 2154, 782, 6590, 3378, 4638, 705, 4506, 1305, 5384, 2861, 2207, 6756, 1381, 1400, 4970, 4390, 4461, 4790, 4810, 8024, 794, 6756, 1079, 4668, 6624, 129, 4665, 7942, 5682, 5696, 5900, 4374, 7676, 4170, 8020, 5307, 817, 3419, 6371, 2137, 711, 8185, 1039, 8021, 511, 1762, 6624, 7716, 6125, 7252, 782, 782, 727, 6631, 2356, 1184, 8024, 2199, 6158, 2154, 782, 6468, 3378, 4638, 6854, 6225, 6632, 7029, 6756, 2340, 1400, 3175, 4390, 4461, 4790, 4810, 8024, 1762, 6756, 1079, 3313, 4668, 1357, 1168, 6568, 4289, 511, 1762, 6624, 7716, 6125, 7252, 824, 2157, 5114, 7215, 4638, 4958, 1790, 1079, 8024, 2199, 6158, 2154, 782, 3330, 3378, 122, 4638, 6662, 5988, 6632, 7029, 6756, 2340, 1400, 3175, 4390, 4461, 4790, 4810, 8024, 1762, 6756, 1079, 3313, 4668, 1357, 1168, 6568, 4289, 511, 9160, 2399, 130, 3299, 8132, 3189, 1119, 3247, 8024, 6158, 1440, 782, 2510, 3345, 832, 1398, 3394, 2014, 510, 2001, 6809, 2292, 510, 1912, 1384, 100, 1259, 4267, 100, 510, 1912, 1384, 100, 2207, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
01/22/2024 00:18:18 - INFO - __main__ - Sample 456 of the training set: {'date': '2019-12-30', ' verdict': '湖北省咸宁市咸安区人民法院刑 事 判 决 书（2018）鄂1202刑初110号公诉机关咸宁市咸安区人民检察院。被告人陈伦，男，1992年4月7日出生，汉族，湖北省咸宁市人,初中文化，无业，住咸宁市咸安区。因吸食毒品，于2013年6月9日被咸宁市公安局咸安分局决定行政拘留十日；因吸食毒品，于2014年12月16日被咸宁市公安局温泉分局决定强制隔离戒毒二年；因吸食毒品，于2017年10月24日被咸宁市公安局咸安分局决定行政拘留十五日，同年11月1日被咸宁市公安局咸安分局以涉嫌贩卖毒品罪转为刑事拘留,12月4日由咸宁市咸安区人民检察院以涉嫌贩卖毒品罪、容留他人吸毒罪批准逮捕，次日由咸宁市公安局咸安分局执行逮捕。现羁押于咸安区看守所。辩护人郑登堂、肖荣华，均系湖北香泉律师事务所律师。咸宁市咸安区人民检察院以咸安检刑诉〔2018〕66号起诉书指控被告人陈伦犯贩卖毒品罪、容留他人吸毒罪，于2018年2月1日向本院提起公诉。本院依法组成合议庭，适用普通程序，于2018年4月26日公开开庭审理了本案。咸安区人民检察院指派检察员朱晚荣出庭支持公诉，被告人陈伦及其辩护人郑登堂、肖荣华到庭参加诉讼。现已审理终结。公诉机关指控：2017年7月至10月期间，被告人陈伦在温泉聚鑫宾馆8301房间开房住宿。在此期间,吸毒人员陶某、杨某1、杨某2、江某、陈某等人多次在8301房间内找被告人陈伦以微信、支付宝转账方式购买甲基苯丙胺片剂(俗称“麻果”)和甲基苯丙胺(俗称“冰毒”)。并且容留上述吸毒人员在自己开的房间内进行吸食。针对上述指控，公诉机关向法庭提交了证人杨某1、杨某2、陈某、陶某、江某的证言、被告人陈伦的供述及辩解、辨认笔录、微信支付交易记录、到案经过、户籍证明等证据。公诉机关认为，被告人陈伦违反国家毒品管理制度,非法向他人贩卖毒品，并容留他人吸食,其行为触犯了《中华人民共和国刑法》第三百四十七条第四款之规定，情节严重，还触犯了《中华人民共和国刑法》第三百五十四条之规定，应当以贩卖毒品罪、容留他人吸毒罪追究其刑事责任。根据《中华人民共和国刑法》第六十九条之规定，应当数罪并罚。针对公诉机关的指控，被告人陈伦当庭辩称:1.我容留他人吸毒是事实,但我没有贩卖毒品；2.我不认识陈某,也未与他有过接触;3.我没有卖过毒品给江某、杨某2，他们只是在我房间内吸食过一次毒品；4.杨某1每天和我住在一起，我吸食毒品，他就跟着一起吸食，我们之间不存在买卖毒品的问题；5.至于陶某，我只是带他去找别人买过毒品。被告人陈伦的辩护人提出：1.陈伦如实交代了自己容留他人吸毒的罪行，具有坦白情节，依法可以从轻处罚。2.对公诉机关指控陈伦涉嫌贩卖毒品罪的定性无异议，但对公诉机关指控的涉嫌贩卖毒品罪的事实有异议，证人杨某1、杨某2、陈某、江某的证言和电子数据（陈伦手机取证分析报告截图、通话记录、微信、支付宝转账记录）不能充分证明陈伦与杨某1、杨某2、陈某、江某之间有贩卖毒品的行为。经审理查明：2017年7月至10月期间，被告人陈伦在温泉聚鑫宾馆8301房间长期开房住宿。在此期间,吸毒人员陶某、杨某1、杨某2、江某、等人多次到聚鑫宾馆8301房间内找陈伦，以微信、支付宝转账的付款方式，购买甲基苯丙胺片剂(俗称“麻果”)和甲基苯丙胺(俗称“冰毒”)，陈伦将毒品分别卖给上述吸毒人员，并且容留上述吸毒人员在聚鑫宾馆8301房间内进行吸食。吸毒人员陈某通过陶某向陈伦购买上述毒品，陈伦通过陶某将毒品卖给了陈某。上述事实，有经庭审举证、质证，并经本院确认的下列证据证实：1.被告人陈伦供述：2017年6月至10月期间，我断断续续在温泉幸福路聚鑫宾馆8301房间住了几个月时间，杨某1和我住在一起。2017年10月22日晚上8时许，我通过电话联系，找朱晓东购买了1200元钱的麻果和冰毒。我将买来的毒品带回房间和杨某1一起吸食了一部分。次日，我和杨某1又吸食了一部分。第三天我睡觉到下午3点多钟醒来时，剩下的毒品不见了，我怀疑是杨某1拿走了。陶某、杨某2、江某、黄某1等人在我房间里吸食过毒品，毒品是我提供的，但我没收他们的钱。2.证人杨某1的证言：我是2017年6月份通过江某认识陈伦的。在认识陈伦后这几个月的时间里，我多次在陈伦手中购买毒品，具体的次数我记不清楚了。2017年10月24日我在陈伦手中购买了100元钱的毒品，我用手机微信转账给他160元钱，其中60元钱是他找我借去打手机游戏的。我大约在陈伦手上总共购买了1万多元钱的毒品。我还帮陶某、陈某、黄某2他们代买过毒品。2017年10月24日凌晨三四点钟，我去温泉聚鑫宾馆8301房间找陈伦，我让宾馆服务员用房卡开的门，陈伦在房间里睡觉。我玩了会手机也睡着了。上午9时许，杨某2来敲门，我开的门，他进来后坐下玩手机，当时陈伦还在睡觉。11时许，陶某来找陈伦，他敲门时也是我开的门。陶某说他是来帮陈某买东西（指毒品）的，但陈某的钱还没有转到他手机上。于是我们坐着等，等了约半个小时，陶某说陈某的钱还没转过来，并说他自己先买200元钱的（毒品）吸食。于是他用手机转了200元钱给陈伦。陈伦此时还没醒。我看到陈伦装毒品的绿色小铁盒放在他枕头边，就去拿过来，从里面拿出2颗麻果和一点冰毒给了陶某，他就用房间里麻将桌上现成的吸食工具吸食起来，我和杨某2也一起跟着吸食了，三个人一共吸食了1颗半麻果，剩下半颗放回了小铁盒里面。当天，陈伦在睡觉之前，跟我交代有人过来拿东西（指毒品），如对方有钱，就叫我帮忙把东西卖给他们。到了12点钟，陈某的钱还没转过来，陶某说要拿东西（指毒品）走，又拿了2颗麻果加冰毒，但这次只转了100元钱给陈伦，他说这2颗麻果加冰毒是帮陈某拿的，等陈某的钱转到他手机上，他再转100元给陈伦，说完他就走了。过了一会，我一个人到楼下的铁椅上坐着玩手机时，陈某来了，问我陶某来了没有。我没有告诉他陶某已经走了，而是骗他说我上去看一下，让他等我一下。我在上楼时给陶某发短信，问他东西(指毒品)为什么没给陈某，陈某到聚鑫宾馆来了。过了会，陶某打电话要我去大畈他家。于是我和陈某一起去了大畈陶某的家，和陶某一起将2颗麻果加冰毒吸食了。聚鑫宾馆的房间是陈伦开的，我没有出钱，但是我有时用手机微信转几十元钱给陈伦玩游戏。我看见过陶某、杨某2、陈某、江某及一些我不认识的人来找陈伦购买过毒品。3.证人陶某的证言：我吸食的毒品是在陈伦手上购买的。陈伦的外号叫“雷子”，微信名叫“内心的忧伤无人能懂”。2017年10月22日下午1点多钟，在温泉聚鑫宾馆门口我打电话给陈伦，他下楼来把毒品给我，然后我开车到大畈实验小学附近的中海油加油站附近，在车内将毒品吸食。2017年10月初的一天，也是我提前打电话联系好陈伦，然后开车去拿的毒品，拿回后在我家附近吸食的。2017年10月24日中午，我去温泉聚鑫宾馆8301房间是去替陈某找陈伦购买200元钱的毒品麻果和冰毒的。我去时，陈某的钱还没有转到我手机上来。到宾馆房间是杨某1给我开的门，杨某2也在房间里，陈伦在床上睡觉，杨某1就替陈伦从床上枕头边的一个小铁盒内拿出2条用透明吸管装着的麻果和冰毒给我，我和杨某1、杨某2用房间内麻将桌上现成的吸食工具吸食，每人吸食了半颗麻果加冰毒，剩下的半颗麻果放回了小铁盒内。过了约一个半小时，我看时间已是中午1点多钟，就说要替陈某买2条毒品，但他还没有将钱转到我手机上来，而我手机上只有100元钱，先转给陈伦，等陈某转钱给我后，我再转给陈伦。因陈伦还在睡觉，杨某1又作主，从小铁盒内拿了2条毒品给我，我拿到毒品后就离开了。在路上，陈某将200元钱转给我了，我就转了150元给陈伦，剩下50元加油了。4.证人杨某2的证言：2017年10月23日傍晚7点多钟，陈伦给我打电话，说他在温泉聚鑫宾馆8301房间，要我开车去那里，然后把他送到咸安去。我去聚鑫宾馆8301房间后，他又说不去咸安了。当时房间里的麻将桌上有个饮料瓶和吸管，还有1条毒品没吸完，陈伦对我说还有“冰”在那里，要吸自己吸。我就吸了几口。2017年10月24日上午10点多钟，我从家里开车到聚鑫宾馆8301房间，我敲门时是杨某1开的门，陈伦在房间里睡觉，我就在房间里和杨某1玩手机游戏。中午1点多钟，陶某来了，也是杨某1去开的门。陶某来后要买毒品。过了一会，陶某叫我到麻将桌旁吸两口（毒品），我才知道他拿到了麻果，我就吸食了几口，然后继续玩我的手机游戏。之后陶某和杨某1相继离开了房间。下午3点钟，陈伦睡醒了。半个小时后，公安人员来房间将我们带到了公安局。我去找陈伦玩的时候，他经常给毒品我吸，我就用手机转玩游戏的分给他作为吸食毒品的报酬，我多时转给他400万分的游戏分当作200元钱，少时转给他几十万分当作几十元钱。我的微信名叫“夜袭寡妇村”，陈伦的微信名叫“内心的忧伤无人能懂”。5.证人陈某的证言：2017年10月24日13时许，我问陶某有没有东西（指毒品），陶某说没有，但他可以代我到其他人手上购买，于是我通过微信转账给陶某200元钱。到了下午3点钟左右，我还没有接到陶某的电话，我就去了温泉聚鑫宾馆。我刚到聚鑫宾馆楼下就碰到杨某1，我和杨某1一起上楼，上楼以后杨某1去了8301房间，我在三楼大厅等他。我等杨某1时，陈伦从8301房间出来买烟，我就问他陶某转钱给他没，他说转了。然后我就和杨某1一起去陶某家，和陶某一起在他家三楼一间空房里将买来的2颗麻果和少量冰毒吸食完了。6.证人江某的证言：2016年9月份至2017年7月中旬期间，我多次在陈伦手中购买毒品。2016年10月23日18时左右，我打电话给陈伦问他手中有没有东西（指毒品），他说有，让我直接去温泉蓝色港湾宾馆3楼一房间内找他。然后我和杨某1两个人坐出租车到该宾馆找他购买了50颗麻果和5克冰毒，我通过微信转给他2500元钱。2017年6月份的一天，我和陶某一起在咸安区畔山居宾馆四楼找陈伦买了4颗麻果和少量冰毒，给了他400元现金。其他的毒品交易详细经过因为次数太多我记不清楚了，但是我的支付宝有交易记录，大概是一万多元钱的交易。7.辨认笔录，证明吸毒人员杨某2、杨某1、陶某、陈某、江某分别对照片进行混杂辨认后，均指出陈伦是卖毒品的人。8.微信支付记录，证明吸毒人员陶某、杨某1、江某分别多次通过微信转账或支付宝，付款给被告人陈伦。9.行政处罚决定书，证明杨某2、陶某等吸毒人员因吸食毒品被公安机关决定行政拘留；被告人陈伦曾因吸食毒品被公安机关决定行政拘留和强制隔离戒毒。10.户籍证明，证明被告人陈伦的出生日期等基本情况。11.到案经过，证明被告人陈伦系被公安机关传唤到案。针对本案的争议焦点，根据事实、证据和相关法律规定，本院作如下评判：1.关于被告人陈伦提出的其不认识陈某,也未与陈某有过接触的辩解意见。经查，证人陈某、陶某、杨某1的证言可以印证的事实是，陈某向被告人陈伦购买毒品，毒资和毒品的交接都是通过陶某进行的。因此，陈伦辩解其不认识陈某,也未与陈某有过接触，不论是否是事实，并不影响其贩卖毒品给陈某这一事实的成立。2.关于被告人陈伦提出的其没有贩卖毒品及其辩护人提出的证人杨某1、杨某2、陈某、江某的证言和电子数据（陈伦手机取证分析报告截图、通话记录、微信、支付宝转账记录）不能充分证明陈伦与杨某1、杨某2、陈某、江某之间有贩卖毒品的行为的辩解和辩护意见。经查，被告人陈伦贩卖毒品麻果加冰毒给吸毒人员陶某、杨某1、杨某2、江某、陈某等人的事实，有吸毒人员的证言予以证实，还有吸毒人员用微信、支付宝向陈伦转账付款的记录予以佐证，并且上述吸毒人员的证言还能相互印证，故，足以认定。因此，陈伦及其辩护人提出的该辩解和辩护意见，不能成立，本院不予采纳。3.关于被告人陈伦的辩护人提出的陈伦如实交代了自己容留他人吸毒的罪行，具有坦白情节，依法可以从轻处罚的辩护意见。经审理认为，该辩护意见成立，本院予以采纳。本院认为，被告人陈伦违反国家对毒品的管理制度，多次向多人贩卖毒品，其行为已构成贩卖毒品罪，情节严重；被告人陈伦容留他人吸食毒品，其行为已构成容留他人吸毒罪。公诉机关指控的罪名均成立。被告人陈伦到案后如实供述了自己容留他人吸毒的罪行，依法可以对其所犯的容留他人吸毒罪从轻处罚。被告人陈伦一人犯数罪，依法应当数罪并罚。根据被告人陈伦的犯罪事实、情节、性质以及对于社会的危害程度，依照《中华人民共和国刑法》第三百四十七条第四款、第三百五十四条、第五十二条，第六十九条第一款、第三款，《最高人民法院关于审理毒品犯罪案件适用法律若干问题的解释》第四条第（一）项之规定，判决如下：被告人陈伦犯贩卖毒品罪，判处有期徒刑四年，并处罚金人民币二万元；犯容留他人吸毒罪，判处有期徒刑十个月，并处罚金人民币五千元，决定执行有期徒刑四年六个月，并处罚金人民币二万五千元。（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即刑期自2017年11月1日起至2022年4月30日止；罚金限在判决发生法律效力后十日内一次缴纳。）如不服本判决，可在接到判决书的第二日起十日内，通过本院或者直接向咸宁市中级人民法院提出上诉。书面上诉的，应当提交上诉状正本一份，副本二份。审\u3000判\u3000长\u3000\u3000林少坤人民陪审员\u3000\u3000徐艳霞人民陪审员\u3000\u3000吴传宇二〇一九年十二月三十日书\u3000记\u3000员\u3000\u3000郭晓龙附：相关法律及司法解释条文《中华人民共和国刑法》第三百四十七条第四款走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。第三百五十四条容留他人吸食、注射毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金。第五十二条判处罚金，应当根据犯罪情节决定罚金数额。第六十九条第一款、第三款判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。《最高人民法院关于审理毒品犯罪案件适用法律若干问题的解释》第四条走私、贩卖、运输、制造毒品，具有下列情形之一的，应当认定为刑法第三百四十七条第四款规定的“情节严重”：（一）向多人贩卖毒品或者多次走私、贩卖、运输、制造毒品的。', 'defendant': '[CLS]陈伦[SEP]2017年7月至10月期间，被告人陈伦在温泉聚鑫宾馆8301房间长期开房住宿。在此期间,吸毒人员陶某、杨某1、杨某2、江某、等人多次到聚鑫宾馆8301房间内找陈伦，以微信、支付宝转账的付款方式，购买甲基苯丙胺片剂(俗称“麻果”)和甲基苯丙胺(俗称“冰毒”)，陈伦将毒品分别卖给上述吸毒人员，并且容留上述吸毒人员在聚鑫宾馆8301房间内进行吸食。吸毒人员陈某通过陶某向陈伦购买上述毒品，陈伦通过陶某将毒品卖给了陈某。上述事实，有经庭审举证、质证，并经本院确认的下列证据证实：1.被告人陈伦供述：2017年6月至10月期间，我断断续续在温泉幸福路聚鑫宾馆8301房间住了几个月时间，杨某1和我住在一起。2017年10月22日晚上8时许，我通过电话联系，找朱晓东购买了1200元钱的麻果和冰毒。我将买来的毒品带回房间和杨某1一起吸食了一部分。次日，我和杨某1又吸食了一部分。第三天我睡觉到下午3点多钟醒来时，剩下的毒品不见了，我怀疑是杨某1拿走了。陶某、杨某2、江某、黄某1等人在我房间里吸食过毒品，毒品是我提供的，但我没收他们的钱。2.证人杨某1的证言：我是2017年6月份通过江某认识陈伦的。在认识陈伦后这几个月的时间里，我多次在陈伦手中购买毒品，具体的次数我记不清楚了。2017年10月24日我在陈伦手中购买了100元钱的毒品，我用手机微信转账给他160元钱，其中60元钱是他找我借去打手机游戏的。我大约在陈伦手上总共购买了1万多元钱的毒品。我还帮陶某、陈某、黄某2他们代买过毒品。2017年10月24日凌晨三四点钟，我去温泉聚鑫宾馆8301房间找陈伦，我让宾馆服务员用房卡开的门，陈伦在房间里睡觉。我玩了会手机也睡着了。上午9时许，杨某2来敲门，我开的门，他进来后坐下玩手机，当时陈伦还在睡觉。11时许，陶某来找陈伦，他敲门时也是我开的门。陶某说他是来帮陈某买东西（指毒品）的，但陈某的钱还没有转到他手机上。于是我们坐着等，等了约半个小时，陶某说陈某的钱还没转过来，并说他自己先买200元钱的（毒品）吸食。于是他用手机转了200元钱给陈伦。陈伦此时还没醒。我看到陈伦装毒品的绿色小铁盒放在他枕头边，就去拿过来，从里面拿出2颗麻果和一点冰毒给了陶某，他就用房间里麻将桌上现成的吸食工具吸食起来，我和杨某2也一起跟着吸食了，三个人一共吸食了1颗半麻果，剩下半颗放回了小铁盒里面。当天，陈伦在睡觉之前，跟我交代有人过来拿东西（指毒品），如对方有钱，就叫我帮忙把东西卖给他们。到了12点钟，陈某的钱还没转过来，陶某说要拿东西（指毒品）走，又拿了2颗麻果加冰毒，但这次只转了100元钱给陈伦，他说这2颗麻果加冰毒是帮陈某拿的，等陈某的钱转到他手机上，他再转100元给陈伦，说完他就走了。过了一会，我一个人到楼下的铁椅上坐着玩手机时，陈某来了，问我陶某来了没有。我没有告诉他陶某已经走了，而是骗他说我上去看一下，让他等我一下。我在上楼时给陶某发短信，问他东西(指毒品)为什么没给陈某，陈某到聚鑫宾馆来了。过了会，陶某打电话要我去大畈他家。于是我和陈某一起去了大畈陶某的家，和陶某一起将2颗麻果加冰毒吸食了。聚鑫宾馆的房间是陈伦开的，我没有出钱，但是我有时用手机微信转几十元钱给陈伦玩游戏。我看见过陶某、杨某2、陈某、江某及一些我不认识的人来找陈伦购买过毒品。3.证人陶某的证言：我吸食的毒品是在陈伦手上购买的。陈伦的外号叫“雷子”，微信名叫“内心的忧伤无人能懂”。2017年10月22日下午1点多钟，在温泉聚鑫宾馆门口我打电话给陈伦，他下楼来把毒品给我，然后我开车到大畈实验小学附近的中海油加油站附近，在车内将毒品吸食。2017年10月初的一天，也是我提前打电话联系好陈伦，然后开车去拿的毒品，拿回后在我家附近吸食的。2017年10月24日中午，我去温泉聚鑫宾馆8301房间是去替陈某找陈伦购买200元钱的毒品麻果和冰毒的。我去时，陈某的钱还没有转到我手机上来。到宾馆房间是杨某1给我开的门，杨某2也在房间里，陈伦在床上睡觉，杨某1就替陈伦从床上枕头边的一个小铁盒内拿出2条用透明吸管装着的麻果和冰毒给我，我和杨某1、杨某2用房间内麻将桌上现成的吸食工具吸食，每人吸食了半颗麻果加冰毒，剩下的半颗麻果放回了小铁盒内。过了约一个半小时，我看时间已是中午1点多钟，就说要替陈某买2条毒品，但他还没有将钱转到我手机上来，而我手机上只有100元钱，先转给陈伦，等陈某转钱给我后，我再转给陈伦。因陈伦还在睡觉，杨某1又作主，从小铁盒内拿了2条毒品给我，我拿到毒品后就离开了。在路上，陈某将200元钱转给我了，我就转了150元给陈伦，剩下50元加油了。4.证人杨某2的证言：2017年10月23日傍晚7点多钟，陈伦给我打电话，说他在温泉聚鑫宾馆8301房间，要我开车去那里，然后把他送到咸安去。我去聚鑫宾馆8301房间后，他又说不去咸安了。当时房间里的麻将桌上有个饮料瓶和吸管，还有1条毒品没吸完，陈伦对我说还有“冰”在那里，要吸自己吸。我就吸了几口。2017年10月24日上午10点多钟，我从家里开车到聚鑫宾馆8301房间，我敲门时是杨某1开的门，陈伦在房间里睡觉，我就在房间里和杨某1玩手机游戏。中午1点多钟，陶某来了，也是杨某1去开的门。陶某来后要买毒品。过了一会，陶某叫我到麻将桌旁吸两口（毒品），我才知道他拿到了麻果，我就吸食了几口，然后继续玩我的手机游戏。之后陶某和杨某1相继离开了房间。下午3点钟，陈伦睡醒了。半个小时后，公安人员来房间将我们带到了公安局。我去找陈伦玩的时候，他经常给毒品我吸，我就用手机转玩游戏的分给他作为吸食毒品的报酬，我多时转给他400万分的游戏分当作200元钱，少时转给他几十万分当作几十元钱。我的微信名叫“夜袭寡妇村”，陈伦的微信名叫“内心的忧伤无人能懂”。5.证人陈某的证言：2017年10月24日13时许，我问陶某有没有东西（指毒品），陶某说没有，但他可以代我到其他人手上购买，于是我通过微信转账给陶某200元钱。到了下午3点钟左右，我还没有接到陶某的电话，我就去了温泉聚鑫宾馆。我刚到聚鑫宾馆楼下就碰到杨某1，我和杨某1一起上楼，上楼以后杨某1去了8301房间，我在三楼大厅等他。我等杨某1时，陈伦从8301房间出来买烟，我就问他陶某转钱给他没，他说转了。然后我就和杨某1一起去陶某家，和陶某一起在他家三楼一间空房里将买来的2颗麻果和少量冰毒吸食完了。6.证人江某的证言：2016年9月份至2017年7月中旬期间，我多次在陈伦手中购买毒品。2016年10月23日18时左右，我打电话给陈伦问他手中有没有东西（指毒品），他说有，让我直接去温泉蓝色港湾宾馆3楼一房间内找他。然后我和杨某1两个人坐出租车到该宾馆找他购买了50颗麻果和5克冰毒，我通过微信转给他2500元钱。2017年6月份的一天，我和陶某一起在咸安区畔山居宾馆四楼找陈伦买了4颗麻果和少量冰毒，给了他400元现金。其他的毒品交易详细经过因为次数太多我记不清楚了，但是我的支付宝有交易记录，大概是一万多元钱的交易。7.辨认笔录，证明吸毒人员杨某2、杨某1、陶某、陈某、江某分别对照片进行混杂辨认后，均指出陈伦是卖毒品的人。8.微信支付记录，证明吸毒人员陶某、杨某1、江某分别多次通过微信转账或支付宝，付款给被告人陈伦。9.行政处罚决定书，证明杨某2、陶某等吸毒人员因吸食毒品被公安机关决定行政拘留；被告人陈伦曾因吸食毒品被公安机关决定行政拘留和强制隔离戒毒。10.户籍证明，证明被告人陈伦的出生日期等基本情况。11.到案经过，证明被告人陈伦系被公安机关传唤到案。针对本案的争议焦点，根据事实、证据和相关法律规定，本院作如下评判：1.关于被告人陈伦提出的其不认识陈某,也未与陈某有过接触的辩解意见。经查，证人陈某、陶某、杨某1的证言可以印证的事实是，陈某向被告人陈伦购买毒品，毒资和毒品的交接都是通过陶某进行的。因此，陈伦辩解其不认识陈某,也未与陈某有过接触，不论是否是事实，并不影响其贩卖毒品给陈某这一事实的成立。2.关于被告人陈伦提出的其没有贩卖毒品及其辩护人提出的证人杨某1、杨某2、陈某、江某的证言和电子数据（陈伦手机取证分析报告截图、通话记录、微信、支付宝转账记录）不能充分证明陈伦与杨某1、杨某2、陈某、江某之间有贩卖毒品的行为的辩解和辩护意见。经查，被告人陈伦贩卖毒品麻果加冰毒给吸毒人员陶某、杨某1、杨某2、江某、陈某等人的事实，有吸毒人员的证言予以证实，还有吸毒人员用微信、支付宝向陈伦转账付款的记录予以佐证，并且上述吸毒人员的证言还能相互印证，故，足以认定。因此，陈伦及其辩护人提出的该辩解和辩护意见，不能成立，本院不予采纳。3.关于被告人陈伦的辩护人提出的陈伦如实交代了自己容留他人吸毒的罪行，具有坦白情节，依法可以从轻处罚的辩护意见。经审理认为，该辩护意见成立，本院予以采纳', 'accusation': ["'走私、贩卖、运输、制造毒品',", "'容留他人吸毒'"], 'article_content': "[{'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第四款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百五十四条', '内容': '【容留他人吸毒罪】容留他人吸食、注射毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十二条', '内容': '【罚金数额的裁量】判处罚金，应当根据犯罪情节决定罚金数额。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第一款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第三款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《最高人民法院关于审理毒品犯罪案件适用法律若干问题的解释》', '法律条文': '第四条第一项', '内容': ''}]", 'province': '湖北省', 'fact': '2017年7月至10月期间，被告人陈伦在温泉聚鑫宾馆8301房间长期开房住宿。在此期间,吸毒人员陶某、杨某1、杨某2、江某、等人多次到聚鑫宾馆8301房间内找陈伦，以微信、支付宝转账的付款方式，购买甲基苯丙胺片剂(俗称“麻果”)和甲基苯丙胺(俗称“冰毒”)，陈伦将毒品分别卖给上述吸毒人员，并且容留上述吸毒人员在聚鑫宾馆8301房间内进行吸食。吸毒人员陈某通过陶某向陈伦购买上述毒品，陈伦通过陶某将毒品卖给了陈某。上述事实，有经庭审举证、质证，并经本院确认的下列证据证实：1.被告人陈伦供述：2017年6月至10月期间，我断断续续在温泉幸福路聚鑫宾馆8301房间住了几个月时间，杨某1和我住在一起。2017年10月22日晚上8时许，我通过电话联系，找朱晓东购买了1200元钱的麻果和冰毒。我将买来的毒品带回房间和杨某1一起吸食了一部分。次日，我和杨某1又吸食了一部分。第三天我睡觉到下午3点多钟醒来时，剩下的毒品不见了，我怀疑是杨某1拿走了。陶某、杨某2、江某、黄某1等人在我房间里吸食过毒品，毒品是我提供的，但我没收他们的钱。2.证人杨某1的证言：我是2017年6月份通过江某认识陈伦的。在认识陈伦后这几个月的时间里，我多次在陈伦手中购买毒品，具体的次数我记不清楚了。2017年10月24日我在陈伦手中购买了100元钱的毒品，我用手机微信转账给他160元钱，其中60元钱是他找我借去打手机游戏的。我大约在陈伦手上总共购买了1万多元钱的毒品。我还帮陶某、陈某、黄某2他们代买过毒品。2017年10月24日凌晨三四点钟，我去温泉聚鑫宾馆8301房间找陈伦，我让宾馆服务员用房卡开的门，陈伦在房间里睡觉。我玩了会手机也睡着了。上午9时许，杨某2来敲门，我开的门，他进来后坐下玩手机，当时陈伦还在睡觉。11时许，陶某来找陈伦，他敲门时也是我开的门。陶某说他是来帮陈某买东西（指毒品）的，但陈某的钱还没有转到他手机上。于是我们坐着等，等了约半个小时，陶某说陈某的钱还没转过来，并说他自己先买200元钱的（毒品）吸食。于是他用手机转了200元钱给陈伦。陈伦此时还没醒。我看到陈伦装毒品的绿色小铁盒放在他枕头边，就去拿过来，从里面拿出2颗麻果和一点冰毒给了陶某，他就用房间里麻将桌上现成的吸食工具吸食起来，我和杨某2也一起跟着吸食了，三个人一共吸食了1颗半麻果，剩下半颗放回了小铁盒里面。当天，陈伦在睡觉之前，跟我交代有人过来拿东西（指毒品），如对方有钱，就叫我帮忙把东西卖给他们。到了12点钟，陈某的钱还没转过来，陶某说要拿东西（指毒品）走，又拿了2颗麻果加冰毒，但这次只转了100元钱给陈伦，他说这2颗麻果加冰毒是帮陈某拿的，等陈某的钱转到他手机上，他再转100元给陈伦，说完他就走了。过了一会，我一个人到楼下的铁椅上坐着玩手机时，陈某来了，问我陶某来了没有。我没有告诉他陶某已经走了，而是骗他说我上去看一下，让他等我一下。我在上楼时给陶某发短信，问他东西(指毒品)为什么没给陈某，陈某到聚鑫宾馆来了。过了会，陶某打电话要我去大畈他家。于是我和陈某一起去了大畈陶某的家，和陶某一起将2颗麻果加冰毒吸食了。聚鑫宾馆的房间是陈伦开的，我没有出钱，但是我有时用手机微信转几十元钱给陈伦玩游戏。我看见过陶某、杨某2、陈某、江某及一些我不认识的人来找陈伦购买过毒品。3.证人陶某的证言：我吸食的毒品是在陈伦手上购买的。陈伦的外号叫“雷子”，微信名叫“内心的忧伤无人能懂”。2017年10月22日下午1点多钟，在温泉聚鑫宾馆门口我打电话给陈伦，他下楼来把毒品给我，然后我开车到大畈实验小学附近的中海油加油站附近，在车内将毒品吸食。2017年10月初的一天，也是我提前打电话联系好陈伦，然后开车去拿的毒品，拿回后在我家附近吸食的。2017年10月24日中午，我去温泉聚鑫宾馆8301房间是去替陈某找陈伦购买200元钱的毒品麻果和冰毒的。我去时，陈某的钱还没有转到我手机上来。到宾馆房间是杨某1给我开的门，杨某2也在房间里，陈伦在床上睡觉，杨某1就替陈伦从床上枕头边的一个小铁盒内拿出2条用透明吸管装着的麻果和冰毒给我，我和杨某1、杨某2用房间内麻将桌上现成的吸食工具吸食，每人吸食了半颗麻果加冰毒，剩下的半颗麻果放回了小铁盒内。过了约一个半小时，我看时间已是中午1点多钟，就说要替陈某买2条毒品，但他还没有将钱转到我手机上来，而我手机上只有100元钱，先转给陈伦，等陈某转钱给我后，我再转给陈伦。因陈伦还在睡觉，杨某1又作主，从小铁盒内拿了2条毒品给我，我拿到毒品后就离开了。在路上，陈某将200元钱转给我了，我就转了150元给陈伦，剩下50元加油了。4.证人杨某2的证言：2017年10月23日傍晚7点多钟，陈伦给我打电话，说他在温泉聚鑫宾馆8301房间，要我开车去那里，然后把他送到咸安去。我去聚鑫宾馆8301房间后，他又说不去咸安了。当时房间里的麻将桌上有个饮料瓶和吸管，还有1条毒品没吸完，陈伦对我说还有“冰”在那里，要吸自己吸。我就吸了几口。2017年10月24日上午10点多钟，我从家里开车到聚鑫宾馆8301房间，我敲门时是杨某1开的门，陈伦在房间里睡觉，我就在房间里和杨某1玩手机游戏。中午1点多钟，陶某来了，也是杨某1去开的门。陶某来后要买毒品。过了一会，陶某叫我到麻将桌旁吸两口（毒品），我才知道他拿到了麻果，我就吸食了几口，然后继续玩我的手机游戏。之后陶某和杨某1相继离开了房间。下午3点钟，陈伦睡醒了。半个小时后，公安人员来房间将我们带到了公安局。我去找陈伦玩的时候，他经常给毒品我吸，我就用手机转玩游戏的分给他作为吸食毒品的报酬，我多时转给他400万分的游戏分当作200元钱，少时转给他几十万分当作几十元钱。我的微信名叫“夜袭寡妇村”，陈伦的微信名叫“内心的忧伤无人能懂”。5.证人陈某的证言：2017年10月24日13时许，我问陶某有没有东西（指毒品），陶某说没有，但他可以代我到其他人手上购买，于是我通过微信转账给陶某200元钱。到了下午3点钟左右，我还没有接到陶某的电话，我就去了温泉聚鑫宾馆。我刚到聚鑫宾馆楼下就碰到杨某1，我和杨某1一起上楼，上楼以后杨某1去了8301房间，我在三楼大厅等他。我等杨某1时，陈伦从8301房间出来买烟，我就问他陶某转钱给他没，他说转了。然后我就和杨某1一起去陶某家，和陶某一起在他家三楼一间空房里将买来的2颗麻果和少量冰毒吸食完了。6.证人江某的证言：2016年9月份至2017年7月中旬期间，我多次在陈伦手中购买毒品。2016年10月23日18时左右，我打电话给陈伦问他手中有没有东西（指毒品），他说有，让我直接去温泉蓝色港湾宾馆3楼一房间内找他。然后我和杨某1两个人坐出租车到该宾馆找他购买了50颗麻果和5克冰毒，我通过微信转给他2500元钱。2017年6月份的一天，我和陶某一起在咸安区畔山居宾馆四楼找陈伦买了4颗麻果和少量冰毒，给了他400元现金。其他的毒品交易详细经过因为次数太多我记不清楚了，但是我的支付宝有交易记录，大概是一万多元钱的交易。7.辨认笔录，证明吸毒人员杨某2、杨某1、陶某、陈某、江某分别对照片进行混杂辨认后，均指出陈伦是卖毒品的人。8.微信支付记录，证明吸毒人员陶某、杨某1、江某分别多次通过微信转账或支付宝，付款给被告人陈伦。9.行政处罚决定书，证明杨某2、陶某等吸毒人员因吸食毒品被公安机关决定行政拘留；被告人陈伦曾因吸食毒品被公安机关决定行政拘留和强制隔离戒毒。10.户籍证明，证明被告人陈伦的出生日期等基本情况。11.到案经过，证明被告人陈伦系被公安机关传唤到案。针对本案的争议焦点，根据事实、证据和相关法律规定，本院作如下评判：1.关于被告人陈伦提出的其不认识陈某,也未与陈某有过接触的辩解意见。经查，证人陈某、陶某、杨某1的证言可以印证的事实是，陈某向被告人陈伦购买毒品，毒资和毒品的交接都是通过陶某进行的。因此，陈伦辩解其不认识陈某,也未与陈某有过接触，不论是否是事实，并不影响其贩卖毒品给陈某这一事实的成立。2.关于被告人陈伦提出的其没有贩卖毒品及其辩护人提出的证人杨某1、杨某2、陈某、江某的证言和电子数据（陈伦手机取证分析报告截图、通话记录、微信、支付宝转账记录）不能充分证明陈伦与杨某1、杨某2、陈某、江某之间有贩卖毒品的行为的辩解和辩护意见。经查，被告人陈伦贩卖毒品麻果加冰毒给吸毒人员陶某、杨某1、杨某2、江某、陈某等人的事实，有吸毒人员的证言予以证实，还有吸毒人员用微信、支付宝向陈伦转账付款的记录予以佐证，并且上述吸毒人员的证言还能相互印证，故，足以认定。因此，陈伦及其辩护人提出的该辩解和辩护意见，不能成立，本院不予采纳。3.关于被告人陈伦的辩护人提出的陈伦如实交代了自己容留他人吸毒的罪行，具有坦白情节，依法可以从轻处罚的辩护意见。经审理认为，该辩护意见成立，本院予以采纳', 'label': 1080, 'defendant_judgement': "[{'走私、贩卖、运输、制造毒品': {'有期徒刑': '四年'}}, {'容留他人吸毒': {'有期徒刑': '十个月'}}, {'有期徒刑': '四年六个月'}]", 'relevant_article': ['347.0,', '354.0'], 'imprisonment': 54.0, 'sentence': '[CLS]陈伦[SEP]2017年7月至10月期间，被告人陈伦在温泉聚鑫宾馆8301房间长期开房住宿。在此期间,吸毒人员陶某、杨某1、杨某2、江某、等人多次到聚鑫宾馆8301房间内找陈伦，以微信、支付宝转账的付款方式，购买甲基苯丙胺片剂(俗称“麻果”)和甲基苯丙胺(俗称“冰毒”)，陈伦将毒品分别卖给上述吸毒人员，并且容留上述吸毒人员在聚鑫宾馆8301房间内进行吸食。吸毒人员陈某通过陶某向陈伦购买上述毒品，陈伦通过陶某将毒品卖给了陈某。上述事实，有经庭审举证、质证，并经本院确认的下列证据证实：1.被告人陈伦供述：2017年6月至10月期间，我断断续续在温泉幸福路聚鑫宾馆8301房间住了几个月时间，杨某1和我住在一起。2017年10月22日晚上8时许，我通过电话联系，找朱晓东购买了1200元钱的麻果和冰毒。我将买来的毒品带回房间和杨某1一起吸食了一部分。次日，我和杨某1又吸食了一部分。第三天我睡觉到下午3点多钟醒来时，剩下的毒品不见了，我怀疑是杨某1拿走了。陶某、杨某2、江某、黄某1等人在我房间里吸食过毒品，毒品是我提供的，但我没收他们的钱。2.证人杨某1的证言：我是2017年6月份通过江某认识陈伦的。在认识陈伦后这几个月的时间里，我多次在陈伦手中购买毒品，具体的次数我记不清楚了。2017年10月24日我在陈伦手中购买了100元钱的毒品，我用手机微信转账给他160元钱，其中60元钱是他找我借去打手机游戏的。我大约在陈伦手上总共购买了1万多元钱的毒品。我还帮陶某、陈某、黄某2他们代买过毒品。2017年10月24日凌晨三四点钟，我去温泉聚鑫宾馆8301房间找陈伦，我让宾馆服务员用房卡开的门，陈伦在房间里睡觉。我玩了会手机也睡着了。上午9时许，杨某2来敲门，我开的门，他进来后坐下玩手机，当时陈伦还在睡觉。11时许，陶某来找陈伦，他敲门时也是我开的门。陶某说他是来帮陈某买东西（指毒品）的，但陈某的钱还没有转到他手机上。于是我们坐着等，等了约半个小时，陶某说陈某的钱还没转过来，并说他自己先买200元钱的（毒品）吸食。于是他用手机转了200元钱给陈伦。陈伦此时还没醒。我看到陈伦装毒品的绿色小铁盒放在他枕头边，就去拿过来，从里面拿出2颗麻果和一点冰毒给了陶某，他就用房间里麻将桌上现成的吸食工具吸食起来，我和杨某2也一起跟着吸食了，三个人一共吸食了1颗半麻果，剩下半颗放回了小铁盒里面。当天，陈伦在睡觉之前，跟我交代有人过来拿东西（指毒品），如对方有钱，就叫我帮忙把东西卖给他们。到了12点钟，陈某的钱还没转过来，陶某说要拿东西（指毒品）走，又拿了2颗麻果加冰毒，但这次只转了100元钱给陈伦，他说这2颗麻果加冰毒是帮陈某拿的，等陈某的钱转到他手机上，他再转100元给陈伦，说完他就走了。过了一会，我一个人到楼下的铁椅上坐着玩手机时，陈某来了，问我陶某来了没有。我没有告诉他陶某已经走了，而是骗他说我上去看一下，让他等我一下。我在上楼时给陶某发短信，问他东西(指毒品)为什么没给陈某，陈某到聚鑫宾馆来了。过了会，陶某打电话要我去大畈他家。于是我和陈某一起去了大畈陶某的家，和陶某一起将2颗麻果加冰毒吸食了。聚鑫宾馆的房间是陈伦开的，我没有出钱，但是我有时用手机微信转几十元钱给陈伦玩游戏。我看见过陶某、杨某2、陈某、江某及一些我不认识的人来找陈伦购买过毒品。3.证人陶某的证言：我吸食的毒品是在陈伦手上购买的。陈伦的外号叫“雷子”，微信名叫“内心的忧伤无人能懂”。2017年10月22日下午1点多钟，在温泉聚鑫宾馆门口我打电话给陈伦，他下楼来把毒品给我，然后我开车到大畈实验小学附近的中海油加油站附近，在车内将毒品吸食。2017年10月初的一天，也是我提前打电话联系好陈伦，然后开车去拿的毒品，拿回后在我家附近吸食的。2017年10月24日中午，我去温泉聚鑫宾馆8301房间是去替陈某找陈伦购买200元钱的毒品麻果和冰毒的。我去时，陈某的钱还没有转到我手机上来。到宾馆房间是杨某1给我开的门，杨某2也在房间里，陈伦在床上睡觉，杨某1就替陈伦从床上枕头边的一个小铁盒内拿出2条用透明吸管装着的麻果和冰毒给我，我和杨某1、杨某2用房间内麻将桌上现成的吸食工具吸食，每人吸食了半颗麻果加冰毒，剩下的半颗麻果放回了小铁盒内。过了约一个半小时，我看时间已是中午1点多钟，就说要替陈某买2条毒品，但他还没有将钱转到我手机上来，而我手机上只有100元钱，先转给陈伦，等陈某转钱给我后，我再转给陈伦。因陈伦还在睡觉，杨某1又作主，从小铁盒内拿了2条毒品给我，我拿到毒品后就离开了。在路上，陈某将200元钱转给我了，我就转了150元给陈伦，剩下50元加油了。4.证人杨某2的证言：2017年10月23日傍晚7点多钟，陈伦给我打电话，说他在温泉聚鑫宾馆8301房间，要我开车去那里，然后把他送到咸安去。我去聚鑫宾馆8301房间后，他又说不去咸安了。当时房间里的麻将桌上有个饮料瓶和吸管，还有1条毒品没吸完，陈伦对我说还有“冰”在那里，要吸自己吸。我就吸了几口。2017年10月24日上午10点多钟，我从家里开车到聚鑫宾馆8301房间，我敲门时是杨某1开的门，陈伦在房间里睡觉，我就在房间里和杨某1玩手机游戏。中午1点多钟，陶某来了，也是杨某1去开的门。陶某来后要买毒品。过了一会，陶某叫我到麻将桌旁吸两口（毒品），我才知道他拿到了麻果，我就吸食了几口，然后继续玩我的手机游戏。之后陶某和杨某1相继离开了房间。下午3点钟，陈伦睡醒了。半个小时后，公安人员来房间将我们带到了公安局。我去找陈伦玩的时候，他经常给毒品我吸，我就用手机转玩游戏的分给他作为吸食毒品的报酬，我多时转给他400万分的游戏分当作200元钱，少时转给他几十万分当作几十元钱。我的微信名叫“夜袭寡妇村”，陈伦的微信名叫“内心的忧伤无人能懂”。5.证人陈某的证言：2017年10月24日13时许，我问陶某有没有东西（指毒品），陶某说没有，但他可以代我到其他人手上购买，于是我通过微信转账给陶某200元钱。到了下午3点钟左右，我还没有接到陶某的电话，我就去了温泉聚鑫宾馆。我刚到聚鑫宾馆楼下就碰到杨某1，我和杨某1一起上楼，上楼以后杨某1去了8301房间，我在三楼大厅等他。我等杨某1时，陈伦从8301房间出来买烟，我就问他陶某转钱给他没，他说转了。然后我就和杨某1一起去陶某家，和陶某一起在他家三楼一间空房里将买来的2颗麻果和少量冰毒吸食完了。6.证人江某的证言：2016年9月份至2017年7月中旬期间，我多次在陈伦手中购买毒品。2016年10月23日18时左右，我打电话给陈伦问他手中有没有东西（指毒品），他说有，让我直接去温泉蓝色港湾宾馆3楼一房间内找他。然后我和杨某1两个人坐出租车到该宾馆找他购买了50颗麻果和5克冰毒，我通过微信转给他2500元钱。2017年6月份的一天，我和陶某一起在咸安区畔山居宾馆四楼找陈伦买了4颗麻果和少量冰毒，给了他400元现金。其他的毒品交易详细经过因为次数太多我记不清楚了，但是我的支付宝有交易记录，大概是一万多元钱的交易。7.辨认笔录，证明吸毒人员杨某2、杨某1、陶某、陈某、江某分别对照片进行混杂辨认后，均指出陈伦是卖毒品的人。8.微信支付记录，证明吸毒人员陶某、杨某1、江某分别多次通过微信转账或支付宝，付款给被告人陈伦。9.行政处罚决定书，证明杨某2、陶某等吸毒人员因吸食毒品被公安机关决定行政拘留；被告人陈伦曾因吸食毒品被公安机关决定行政拘留和强制隔离戒毒。10.户籍证明，证明被告人陈伦的出生日期等基本情况。11.到案经过，证明被告人陈伦系被公安机关传唤到案。针对本案的争议焦点，根据事实、证据和相关法律规定，本院作如下评判：1.关于被告人陈伦提出的其不认识陈某,也未与陈某有过接触的辩解意见。经查，证人陈某、陶某、杨某1的证言可以印证的事实是，陈某向被告人陈伦购买毒品，毒资和毒品的交接都是通过陶某进行的。因此，陈伦辩解其不认识陈某,也未与陈某有过接触，不论是否是事实，并不影响其贩卖毒品给陈某这一事实的成立。2.关于被告人陈伦提出的其没有贩卖毒品及其辩护人提出的证人杨某1、杨某2、陈某、江某的证言和电子数据（陈伦手机取证分析报告截图、通话记录、微信、支付宝转账记录）不能充分证明陈伦与杨某1、杨某2、陈某、江某之间有贩卖毒品的行为的辩解和辩护意见。经查，被告人陈伦贩卖毒品麻果加冰毒给吸毒人员陶某、杨某1、杨某2、江某、陈某等人的事实，有吸毒人员的证言予以证实，还有吸毒人员用微信、支付宝向陈伦转账付款的记录予以佐证，并且上述吸毒人员的证言还能相互印证，故，足以认定。因此，陈伦及其辩护人提出的该辩解和辩护意见，不能成立，本院不予采纳。3.关于被告人陈伦的辩护人提出的陈伦如实交代了自己容留他人吸毒的罪行，具有坦白情节，依法可以从轻处罚的辩护意见。经审理认为，该辩护意见成立，本院予以采纳', 'input_ids': [101, 101, 7357, 840, 102, 8109, 2399, 128, 3299, 5635, 8108, 3299, 3309, 7313, 8024, 6158, 1440, 782, 7357, 840, 1762, 3946, 3787, 5471, 7144, 2161, 7667, 12428, 8148, 2791, 7313, 7270, 3309, 2458, 2791, 857, 2162, 511, 1762, 3634, 3309, 7313, 117, 1429, 3681, 782, 1447, 7378, 3378, 510, 3342, 3378, 122, 510, 3342, 3378, 123, 510, 3736, 3378, 510, 5023, 782, 1914, 3613, 1168, 5471, 7144, 2161, 7667, 12428, 8148, 2791, 7313, 1079, 2823, 7357, 840, 8024, 809, 2544, 928, 510, 3118, 802, 2140, 6760, 6572, 4638, 802, 3621, 3175, 2466, 8024, 6579, 743, 4508, 1825, 5738, 688, 5542, 4275, 1177, 113, 921, 4917, 100, 7937, 3362, 100, 114, 1469, 4508, 1825, 5738, 688, 5542, 113, 921, 4917, 100, 1102, 3681, 100, 114, 8024, 7357, 840, 2199, 3681, 1501, 1146, 1166, 1297, 5314, 677, 6835, 1429, 3681, 782, 1447, 8024, 2400, 684, 2159, 4522, 677, 6835, 1429, 3681, 782, 1447, 1762, 5471, 7144, 2161, 7667, 12428, 8148, 2791, 7313, 1079, 6822, 6121, 1429, 7608, 511, 1429, 3681, 782, 1447, 7357, 3378, 6858, 6814, 7378, 3378, 1403, 7357, 840, 6579, 743, 677, 6835, 3681, 1501, 8024, 7357, 840, 6858, 6814, 7378, 3378, 2199, 3681, 1501, 1297, 5314, 749, 7357, 3378, 511, 677, 6835, 752, 2141, 8024, 3300, 5307, 2431, 2144, 715, 6395, 510, 6574, 6395, 8024, 2400, 5307, 3315, 7368, 4802, 6371, 4638, 678, 1154, 6395, 2945, 6395, 2141, 8038, 122, 119, 6158, 1440, 782, 7357, 840, 897, 6835, 8038, 8109, 2399, 127, 3299, 5635, 8108, 3299, 3309, 7313, 8024, 2769, 3171, 3171, 5330, 5330, 1762, 3946, 3787, 2401, 4886, 6662, 5471, 7144, 2161, 7667, 12428, 8148, 2791, 7313, 857, 749, 1126, 702, 3299, 3198, 7313, 8024, 3342, 3378, 122, 1469, 2769, 857, 1762, 671, 6629, 511, 8109, 2399, 8108, 3299, 8130, 3189, 3241, 677, 129, 3198, 6387, 8024, 2769, 6858, 6814, 4510, 6413, 5468, 5143, 8024, 2823, 3319, 3236, 691, 6579, 743, 749, 8552, 1039, 7178, 4638, 7937, 3362, 1469, 1102, 3681, 511, 2769, 2199, 743, 3341, 4638, 3681, 1501, 2372, 1726, 2791, 7313, 1469, 3342, 3378, 122, 671, 6629, 1429, 7608, 749, 671, 6956, 1146, 511, 3613, 3189, 8024, 2769, 1469, 3342, 3378, 122, 1348, 1429, 7608, 749, 671, 6956, 1146, 511, 5018, 676, 1921, 2769, 4717, 6230, 1168, 678, 1286, 124, 4157, 1914, 7164, 7008, 3341, 3198, 8024, 1197, 678, 4638, 3681, 1501, 679, 6224, 749, 8024, 2769, 2577, 4542, 3221, 3342, 3378, 122, 2897, 6624, 749, 511, 7378, 3378, 510, 3342, 3378, 123, 510, 3736, 3378, 510, 7942, 3378, 122, 5023, 782, 1762, 2769, 2791, 7313, 7027, 1429, 7608, 6814, 3681, 1501, 8024, 3681, 1501, 3221, 2769, 2990, 897, 4638, 8024, 852, 2769, 3766, 3119, 800, 812, 4638, 7178, 511, 123, 119, 6395, 782, 3342, 3378, 122, 4638, 6395, 6241, 8038, 2769, 3221, 8109, 2399, 127, 3299, 819, 6858, 6814, 3736, 3378, 6371, 6399, 7357, 840, 4638, 511, 1762, 6371, 6399, 7357, 840, 1400, 6821, 1126, 702, 3299, 4638, 3198, 7313, 7027, 8024, 2769, 1914, 3613, 1762, 7357, 840, 2797, 704, 6579, 743, 3681, 1501, 8024, 1072, 860, 4638, 3613, 3144, 2769, 6381, 679, 3926, 3504, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
01/22/2024 00:18:18 - INFO - __main__ - Sample 102 of the training set: {'date': '2020-11-18', ' verdict': '  山东省高唐县人民法院 刑 事 判 决 书 （2020）鲁1526刑初164号 公诉机关山东省高唐县人民检察院。 被告人姜维，又名姜国军，男，1975年9月27日出生于山东省高唐县，汉族，小学肄业，农民，住高唐县。因涉嫌犯强奸罪于2020年1月19日被高唐县公安局决定刑事拘留（在逃），同年5月4日被高唐县公安局监视居住，同年7月24日经高唐县人民检察院批准逮捕，7月27日被高唐县公安局执行逮捕。现羁押于高唐县看守所。 辩护人宁洪峰，山东天地长安律师事务所律师。 山东省高唐县人民检察院以高检一部刑诉[2020]145号起诉书指控被告人姜维犯重婚罪、猥亵儿童罪、强奸罪，于2020年11月3日向本院提起公诉。本院受理后，依法组成合议庭，因涉及个人隐私，适用普通程序不公开开庭审理了本案。山东省高唐县人民检察院指派检察员董以超、代理检察员尹欣欣出庭支持公诉，被告人姜维及其辩护人宁洪峰到庭参加诉讼。现已审理终结。 公诉机关指控：一、重婚罪 2002年11月25日，被告人姜维与刘某结婚。2018年年初，被告人姜维与姜某2开始有不正当男女关系，并多次发生性关系。2018年7月份，姜某2怀孕后不慎流产。2018年11月份，姜某2再次怀孕。2019年4月至9月，被告人姜维与姜某2在禹城市房寺镇滨河社区租房并以夫妻名义共同生活。2019年8月22日，被告人姜维与姜某2生育一子姜某4。2019年9月30日，被告人姜维与刘某离婚。 二、猥亵儿童罪 2018年冬天的一天晚上，被告人姜维趁在姜某2家睡觉之机，在姜某2家中火炕上，钻进被害人姜某1（女，2012年9月13日出生,系姜某2之女）的被窝，用手摸被害人姜某1的阴部，因姜某1大喊，姜某2发现遂将被告人姜维赶走。 三、强奸罪 1、2019年6、7月份的一天，在姜某1家中北屋客厅的床上，被告人姜维以给被害人姜某1买好吃的为由，诱骗被害人姜某1躺在床上，后将被害人姜某1强奸。 2、2019年7月11日上午，被告人姜维带被害人姜某1去高唐县固河镇某村给姜某1报名上小学，报完名后二人返回姜某1家中。后在姜某1家中北屋客厅床上，被告人姜维脱下背心遮住被害人姜某1的面部，将被害人姜某1强奸。 3、2019年8月份，被告人姜维趁被害人姜某1到其家玩耍之机，先后两次将被害人姜某1诱骗至其家南屋火炕上，采取用枕巾遮盖被害人姜某1面部的手段，将被害人姜某1强奸两次。 以上，自2019年6月至2019年8月，被告人姜维强奸被害人姜某1共四次。 为证实上述指控，公诉机关当庭宣读、出示了受案登记表等书证，证人姜某3等人的证言，被害人姜某1的陈述，被告人姜维的供述和辩解，现场勘验等笔录等证据。 公诉机关认为，被告人姜维为满足性欲，奸淫幼女、猥亵儿童，有配偶与他人同居并生育子女，其行为触犯了《中华人民共和国刑法》第二百三十六条第二款、第二百三十七条第三款、第二百五十八条，犯罪事实清楚，证据确实、充分，应当以强奸罪、猥亵儿童罪、重婚罪追究其刑事责任。被告人姜维奸淫幼女多次，应当从重处罚；被告人姜维猥亵儿童，应从重处罚；被告人姜维自愿认罪认罚，同时适用《中华人民共和国刑事诉讼法》第十五条，可以从宽处理；被告人姜维一人犯数罪，同时适用《中华人民共和国刑法》第六十九条，应当数罪并罚。综上，建议对被告人姜维犯强奸罪、猥亵儿童罪、重婚罪判处有期徒刑八年至十年。 被告人姜维对指控事实、罪名及量刑建议没有异议且签字具结，在开庭审理过程中亦无异议。其辩护人提出的辩护意见是，被告人姜维认罪认罚，无犯罪前科，请求对其从轻处罚。 经审理查明：一、强奸罪 2018年年初，被告人姜维与伯父姜某3的养女姜某2发展成不正当男女关系。2019年6、7月份至8月份，被告人姜维先后在姜某2家中北屋客厅的床上及其家南屋的火炕上，以给姜某2之女姜某1（2012年9月13日出生）买好吃的等为由，用其背心或枕巾盖住姜某1面部，奸淫姜某1四次。 二、猥亵儿童罪 2018年冬天的一天晚上，被告人姜维趁在姜某2家中火炕上睡觉之机，钻进被害人姜某1的被窝，用手摸被害人姜某1的阴部，因姜某1大喊，姜某2遂将被告人姜维赶走。 三、重婚罪 2002年11月25日，被告人姜维与刘某结婚。2018年年初，被告人姜维与姜某2发展成不正当男女关系，后姜某2怀孕于2018年7月份不慎流产，2018年11月份姜某2再次怀孕。2019年4月至9月，被告人姜维与姜某2在禹城市某社区租房并以夫妻名义共同生活。2019年8月22日，姜某2在平原县人民医院生育一子姜某4，姜维以丈夫名义办理并在相关手续上签署其名字。2019年9月30日，被告人姜维与刘某离婚。 上述事实有经庭审质证、确认的书证高唐县公安局受案登记表、立案决定书、发破案经过、抓获经过及调取证据通知书、高唐县某小学2019新生信息、高唐县人民医院诊断证明书、禹城市人民医院诊断证明书、姜维与刘某的结婚登记申请书及离婚登记审查处理表、常住人口登记卡索引表、常住人口登记卡及姜某4的出生医学证明副页、平原县人民医院提供的住院病历及手术知情同意书等相关手续一宗、被害人姜某1及被告人姜维的户籍证明，证人姜某2、姜某3、刘某、王某、孙某、杨某的证言，被害人姜某1的陈述，高唐县公安局现场勘验笔录、现场指认笔录及照片，辨认笔录，被告人姜维的供述等证据证实，足以认定。 本院认为，被告人姜维为满足性欲，奸淫幼女；为寻求性刺激，猥亵儿童；有配偶仍与他人以夫妻名义共同生活，其行为已构成强奸罪、猥亵儿童罪、重婚罪，应予依法惩处，对其所犯数罪，应予并罚。公诉机关的指控成立。在强奸犯罪中，被告人姜维当庭自愿认罪，愿意接受处罚，依法从轻处罚；在猥亵儿童、重婚犯罪中，被告人姜维归案后如实供述罪行并当庭自愿认罪，愿意接受处罚，依法从轻处罚。被告人姜维猥亵、多次奸淫不满十二周岁的儿童，依法从重处罚。公诉机关的量刑建议适当。辩护人的辩护意见，本院予以采纳。据此，依照《中华人民共和国刑法》第二百三十六条第一款和第二款、第二百三十七条第一款和第三款、第二百五十八条、第六十七条第三款、第六十九条、第四十五条、第四十七条，《中华人民共和国刑事诉讼法》第十五条之规定，判决如下： 被告人姜维犯强奸罪判处有期徒刑八年；犯猥亵儿童罪，判处有期徒刑二年；犯重婚罪，判处有期徒刑六个月，决定执行有期徒刑九年。 （刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2020年7月27日起至2029年7月26日止。） 如不服本判决，可在接到判决书的第二日起十日内，通过本院或者直接向山东省聊城市中级人民法院提出上诉。书面上诉的，应当提交上诉状正本一份，副本二份。 审\u3000判\u3000长\u3000\u3000张焕新 人民陪审员\u3000\u3000井维明 人民陪审员\u3000\u3000刘玉武 二〇二〇年十一月十八日 书\u3000记\u3000员\u3000\u3000李\u3000蕾', 'defendant': '[CLS]姜维[SEP]一、[MASK] 2018年年初，被告人姜维与伯父姜某3的养女姜某2发展成不正当男女关系。2019年6、7月份至8月份，被告人姜维先后在姜某2家中北屋客厅的床上及其家南屋的火炕上，以给姜某2之女姜某1（2012年9月13日出生）买好吃的等为由，用其背心或枕巾盖住姜某1面部，奸淫姜某1四次。 二、[MASK] 2018年冬天的一天晚上，被告人姜维趁在姜某2家中火炕上睡觉之机，钻进被害人姜某1的被窝，用手摸被害人姜某1的阴部，因姜某1大喊，姜某2遂将被告人姜维赶走。 三、[MASK] 2002年11月25日，被告人姜维与刘某结婚。2018年年初，被告人姜维与姜某2发展成不正当男女关系，后姜某2怀孕于2018年7月份不慎流产，2018年11月份姜某2再次怀孕。2019年4月至9月，被告人姜维与姜某2在禹城市某社区租房并以夫妻名义共同生活。2019年8月22日，姜某2在平原县人民医院生育一子姜某4，姜维以丈夫名义办理并在相关手续上签署其名字。2019年9月30日，被告人姜维与刘某离婚。 上述事实有经庭审质证、确认的书证高唐县公安局受案登记表、立案决定书、发破案经过、抓获经过及调取证据通知书、高唐县某小学2019新生信息、高唐县人民医院诊断证明书、禹城市人民医院诊断证明书、姜维与刘某的结婚登记申请书及离婚登记审查处理表、常住人口登记卡索引表、常住人口登记卡及姜某4的出生医学证明副页、平原县人民医院提供的住院病历及手术知情同意书等相关手续一宗、被害人姜某1及被告人姜维的户籍证明，证人姜某2、姜某3、刘某、王某、孙某、杨某的证言，被害人姜某1的陈述，高唐县公安局现场勘验笔录、现场指认笔录及照片，辨认笔录，被告人姜维的供述等证据证实，足以认定。 ', 'accusation': ["'重婚',", "'猥亵儿童',", "'强奸'"], 'article_content': "[{'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百三十六条第一款', '内容': '【强奸罪】以暴力、胁迫或者其他手段强奸妇女的，处三年以上十年以下有期徒刑。奸淫不满十四周岁的幼女的，以强奸论，从重处罚。强奸妇女、奸淫幼女，有下列情形之一的，处十年以上有期徒刑、无期徒刑或者死刑：（一）强奸妇女、奸淫幼女情节恶劣的；（二）强奸妇女、奸淫幼女多人的；（三）在公共场所当众强奸妇女的；（四）二人以上轮奸的；（五）致使被害人重伤、死亡或者造成其他严重后果的。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百三十六条第二款', '内容': '【强奸罪】以暴力、胁迫或者其他手段强奸妇女的，处三年以上十年以下有期徒刑。奸淫不满十四周岁的幼女的，以强奸论，从重处罚。强奸妇女、奸淫幼女，有下列情形之一的，处十年以上有期徒刑、无期徒刑或者死刑：（一）强奸妇女、奸淫幼女情节恶劣的；（二）强奸妇女、奸淫幼女多人的；（三）在公共场所当众强奸妇女的；（四）二人以上轮奸的；（五）致使被害人重伤、死亡或者造成其他严重后果的。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百三十七条第一款', '内容': '【强制猥亵、侮辱罪】以暴力、胁迫或者其他方法强制猥亵他人或者侮辱妇女的，处五年以下有期徒刑或者拘役。聚众或者在公共场所当众犯前款罪的，或者有其他恶劣情节的，处五年以上有期徒刑。【猥亵儿童罪】猥亵儿童的，依照前两款的规定从重处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百三十七条第三款', '内容': '【强制猥亵、侮辱罪】以暴力、胁迫或者其他方法强制猥亵他人或者侮辱妇女的，处五年以下有期徒刑或者拘役。聚众或者在公共场所当众犯前款罪的，或者有其他恶劣情节的，处五年以上有期徒刑。【猥亵儿童罪】猥亵儿童的，依照前两款的规定从重处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百五十八条', '内容': '【重婚罪】有配偶而重婚的，或者明知他人有配偶而与之结婚的，处二年以下有期徒刑或者拘役。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十七条第三款', '内容': '【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。被采取强制措施的犯罪嫌疑人、被告人和正在服刑的罪犯，如实供述司法机关还未掌握的本人其他罪行的，以自首论。犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第一款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第二款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第三款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第四十五条', '内容': '【有期徒刑的期限】有期徒刑的期限，除本法第五十条、第六十九条规定外，为六个月以上十五年以下。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第四十七条', '内容': '【有期徒刑刑期的计算与折抵】有期徒刑的刑期，从判决执行之日起计算；判决执行以前先行羁押的，羁押一日折抵刑期一日。'}, {'法律名称': '《中华人民共和国刑事诉讼法（2018年）》', '法律条文': '第十五条', '内容': ''}]", 'province': '山东省', 'fact': '一、[MASK] 2018年年初，被告人姜维与伯父姜某3的养女姜某2发展成不正当男女关系。2019年6、7月份至8月份，被告人姜维先后在姜某2家中北屋客厅的床上及其家南屋的火炕上，以给姜某2之女姜某1（2012年9月13日出生）买好吃的等为由，用其背心或枕巾盖住姜某1面部，奸淫姜某1四次。 二、[MASK] 2018年冬天的一天晚上，被告人姜维趁在姜某2家中火炕上睡觉之机，钻进被害人姜某1的被窝，用手摸被害人姜某1的阴部，因姜某1大喊，姜某2遂将被告人姜维赶走。 三、[MASK] 2002年11月25日，被告人姜维与刘某结婚。2018年年初，被告人姜维与姜某2发展成不正当男女关系，后姜某2怀孕于2018年7月份不慎流产，2018年11月份姜某2再次怀孕。2019年4月至9月，被告人姜维与姜某2在禹城市某社区租房并以夫妻名义共同生活。2019年8月22日，姜某2在平原县人民医院生育一子姜某4，姜维以丈夫名义办理并在相关手续上签署其名字。2019年9月30日，被告人姜维与刘某离婚。 上述事实有经庭审质证、确认的书证高唐县公安局受案登记表、立案决定书、发破案经过、抓获经过及调取证据通知书、高唐县某小学2019新生信息、高唐县人民医院诊断证明书、禹城市人民医院诊断证明书、姜维与刘某的结婚登记申请书及离婚登记审查处理表、常住人口登记卡索引表、常住人口登记卡及姜某4的出生医学证明副页、平原县人民医院提供的住院病历及手术知情同意书等相关手续一宗、被害人姜某1及被告人姜维的户籍证明，证人姜某2、姜某3、刘某、王某、孙某、杨某的证言，被害人姜某1的陈述，高唐县公安局现场勘验笔录、现场指认笔录及照片，辨认笔录，被告人姜维的供述等证据证实，足以认定。 ', 'label': 1129, 'defendant_judgement': "[{'重婚': {'有期徒刑': '六个月'}}, {'猥亵儿童': {'有期徒刑': '二年'}}, {'强奸': {'有期徒刑': '八年'}}, {'有期徒刑': '九年'}]", 'relevant_article': ['236.0,', '237.0,', '258.0'], 'imprisonment': 108.0, 'sentence': '[CLS]姜维[SEP]一、[MASK] 2018年年初，被告人姜维与伯父姜某3的养女姜某2发展成不正当男女关系。2019年6、7月份至8月份，被告人姜维先后在姜某2家中北屋客厅的床上及其家南屋的火炕上，以给姜某2之女姜某1（2012年9月13日出生）买好吃的等为由，用其背心或枕巾盖住姜某1面部，奸淫姜某1四次。 二、[MASK] 2018年冬天的一天晚上，被告人姜维趁在姜某2家中火炕上睡觉之机，钻进被害人姜某1的被窝，用手摸被害人姜某1的阴部，因姜某1大喊，姜某2遂将被告人姜维赶走。 三、[MASK] 2002年11月25日，被告人姜维与刘某结婚。2018年年初，被告人姜维与姜某2发展成不正当男女关系，后姜某2怀孕于2018年7月份不慎流产，2018年11月份姜某2再次怀孕。2019年4月至9月，被告人姜维与姜某2在禹城市某社区租房并以夫妻名义共同生活。2019年8月22日，姜某2在平原县人民医院生育一子姜某4，姜维以丈夫名义办理并在相关手续上签署其名字。2019年9月30日，被告人姜维与刘某离婚。 上述事实有经庭审质证、确认的书证高唐县公安局受案登记表、立案决定书、发破案经过、抓获经过及调取证据通知书、高唐县某小学2019新生信息、高唐县人民医院诊断证明书、禹城市人民医院诊断证明书、姜维与刘某的结婚登记申请书及离婚登记审查处理表、常住人口登记卡索引表、常住人口登记卡及姜某4的出生医学证明副页、平原县人民医院提供的住院病历及手术知情同意书等相关手续一宗、被害人姜某1及被告人姜维的户籍证明，证人姜某2、姜某3、刘某、王某、孙某、杨某的证言，被害人姜某1的陈述，高唐县公安局现场勘验笔录、现场指认笔录及照片，辨认笔录，被告人姜维的供述等证据证实，足以认定。 ', 'input_ids': [101, 101, 2002, 5335, 102, 671, 510, 103, 8271, 2399, 2399, 1159, 8024, 6158, 1440, 782, 2002, 5335, 680, 843, 4266, 2002, 3378, 124, 4638, 1075, 1957, 2002, 3378, 123, 1355, 2245, 2768, 679, 3633, 2496, 4511, 1957, 1068, 5143, 511, 9160, 2399, 127, 510, 128, 3299, 819, 5635, 129, 3299, 819, 8024, 6158, 1440, 782, 2002, 5335, 1044, 1400, 1762, 2002, 3378, 123, 2157, 704, 1266, 2238, 2145, 1324, 4638, 2414, 677, 1350, 1071, 2157, 1298, 2238, 4638, 4125, 4145, 677, 8024, 809, 5314, 2002, 3378, 123, 722, 1957, 2002, 3378, 122, 8020, 8151, 2399, 130, 3299, 8124, 3189, 1139, 4495, 8021, 743, 1962, 1391, 4638, 5023, 711, 4507, 8024, 4500, 1071, 5520, 2552, 2772, 3359, 2353, 4667, 857, 2002, 3378, 122, 7481, 6956, 8024, 1960, 3915, 2002, 3378, 122, 1724, 3613, 511, 753, 510, 103, 8271, 2399, 1100, 1921, 4638, 671, 1921, 3241, 677, 8024, 6158, 1440, 782, 2002, 5335, 6630, 1762, 2002, 3378, 123, 2157, 704, 4125, 4145, 677, 4717, 6230, 722, 3322, 8024, 7183, 6822, 6158, 2154, 782, 2002, 3378, 122, 4638, 6158, 4973, 8024, 4500, 2797, 3043, 6158, 2154, 782, 2002, 3378, 122, 4638, 7346, 6956, 8024, 1728, 2002, 3378, 122, 1920, 1591, 8024, 2002, 3378, 123, 6876, 2199, 6158, 1440, 782, 2002, 5335, 6628, 6624, 511, 676, 510, 103, 8301, 2399, 8111, 3299, 8132, 3189, 8024, 6158, 1440, 782, 2002, 5335, 680, 1155, 3378, 5310, 2042, 511, 8271, 2399, 2399, 1159, 8024, 6158, 1440, 782, 2002, 5335, 680, 2002, 3378, 123, 1355, 2245, 2768, 679, 3633, 2496, 4511, 1957, 1068, 5143, 8024, 1400, 2002, 3378, 123, 2577, 2097, 754, 8271, 2399, 128, 3299, 819, 679, 2708, 3837, 772, 8024, 8271, 2399, 8111, 3299, 819, 2002, 3378, 123, 1086, 3613, 2577, 2097, 511, 9160, 2399, 125, 3299, 5635, 130, 3299, 8024, 6158, 1440, 782, 2002, 5335, 680, 2002, 3378, 123, 1762, 4893, 1814, 2356, 3378, 4852, 1277, 4909, 2791, 2400, 809, 1923, 1988, 1399, 721, 1066, 1398, 4495, 3833, 511, 9160, 2399, 129, 3299, 8130, 3189, 8024, 2002, 3378, 123, 1762, 2398, 1333, 1344, 782, 3696, 1278, 7368, 4495, 5509, 671, 2094, 2002, 3378, 125, 8024, 2002, 5335, 809, 675, 1923, 1399, 721, 1215, 4415, 2400, 1762, 4685, 1068, 2797, 5330, 677, 5041, 5392, 1071, 1399, 2099, 511, 9160, 2399, 130, 3299, 8114, 3189, 8024, 6158, 1440, 782, 2002, 5335, 680, 1155, 3378, 4895, 2042, 511, 677, 6835, 752, 2141, 3300, 5307, 2431, 2144, 6574, 6395, 510, 4802, 6371, 4638, 741, 6395, 7770, 1538, 1344, 1062, 2128, 2229, 1358, 3428, 4633, 6381, 6134, 510, 4989, 3428, 1104, 2137, 741, 510, 1355, 4788, 3428, 5307, 6814, 510, 2831, 5815, 5307, 6814, 1350, 6444, 1357, 6395, 2945, 6858, 4761, 741, 510, 7770, 1538, 1344, 3378, 2207, 2110, 9160, 3173, 4495, 928, 2622, 510, 7770, 1538, 1344, 782, 3696, 1278, 7368, 6402, 3171, 6395, 3209, 741, 510, 4893, 1814, 2356, 782, 3696, 1278, 7368, 6402, 3171, 6395, 3209, 741, 510, 2002, 5335, 680, 1155, 3378, 4638, 5310, 2042, 4633, 6381, 4509, 6435, 741, 1350, 4895, 2042, 4633, 6381, 2144, 3389, 1905, 4415, 6134, 510, 2382, 857, 782, 1366, 4633, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
01/22/2024 00:18:18 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:738] 2024-01-22 00:18:24,510 >> The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1723] 2024-01-22 00:18:24,522 >> ***** Running training *****
[INFO|trainer.py:1724] 2024-01-22 00:18:24,522 >>   Num examples = 4,000
[INFO|trainer.py:1725] 2024-01-22 00:18:24,522 >>   Num Epochs = 200
[INFO|trainer.py:1726] 2024-01-22 00:18:24,522 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1729] 2024-01-22 00:18:24,522 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1730] 2024-01-22 00:18:24,522 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1731] 2024-01-22 00:18:24,522 >>   Total optimization steps = 25,000
[INFO|trainer.py:1732] 2024-01-22 00:18:24,523 >>   Number of trainable parameters = 127,319,620
[INFO|integration_utils.py:718] 2024-01-22 00:18:24,524 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: loss4wang. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /local/xiaowang/LJP Task/train/bert_roberta_lawformer/wandb/run-20240122_001827-koezwwet
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lawformer_data2_t2_bs32_lr7e-5
wandb: ⭐️ View project at https://wandb.ai/loss4wang/LJP_baselines_task2
wandb: 🚀 View run at https://wandb.ai/loss4wang/LJP_baselines_task2/runs/koezwwet
  0%|          | 0/25000 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-01-22 00:18:35,341 >> You're using a LongformerTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[INFO|modeling_longformer.py:1920] 2024-01-22 00:18:35,391 >> Initializing global attention on CLS token...
  0%|          | 1/25000 [00:05<40:35:08,  5.84s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:41,193 >> Initializing global attention on CLS token...
  0%|          | 2/25000 [00:07<22:56:57,  3.30s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:42,721 >> Initializing global attention on CLS token...
  0%|          | 3/25000 [00:08<17:20:01,  2.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:44,255 >> Initializing global attention on CLS token...
  0%|          | 4/25000 [00:10<14:40:22,  2.11s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:45,781 >> Initializing global attention on CLS token...
  0%|          | 5/25000 [00:11<13:12:18,  1.90s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:47,308 >> Initializing global attention on CLS token...
  0%|          | 6/25000 [00:13<12:21:06,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:48,849 >> Initializing global attention on CLS token...
  0%|          | 7/25000 [00:15<11:48:09,  1.70s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:50,386 >> Initializing global attention on CLS token...
  0%|          | 8/25000 [00:16<11:25:10,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:51,913 >> Initializing global attention on CLS token...
  0%|          | 9/25000 [00:18<11:10:33,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:53,447 >> Initializing global attention on CLS token...
  0%|          | 10/25000 [00:19<11:14:39,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:55,088 >> Initializing global attention on CLS token...
  0%|          | 11/25000 [00:21<11:03:15,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:56,619 >> Initializing global attention on CLS token...
  0%|          | 12/25000 [00:22<10:55:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:58,151 >> Initializing global attention on CLS token...
  0%|          | 13/25000 [00:24<10:51:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:18:59,692 >> Initializing global attention on CLS token...
  0%|          | 14/25000 [00:25<10:47:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:01,226 >> Initializing global attention on CLS token...
  0%|          | 15/25000 [00:27<10:46:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:02,771 >> Initializing global attention on CLS token...
  0%|          | 16/25000 [00:28<10:44:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:04,305 >> Initializing global attention on CLS token...
  0%|          | 17/25000 [00:30<10:45:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:05,863 >> Initializing global attention on CLS token...
  0%|          | 18/25000 [00:32<10:45:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:07,412 >> Initializing global attention on CLS token...
  0%|          | 19/25000 [00:33<10:43:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:08,957 >> Initializing global attention on CLS token...
  0%|          | 20/25000 [00:35<10:44:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:10,505 >> Initializing global attention on CLS token...
  0%|          | 21/25000 [00:36<10:44:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:12,048 >> Initializing global attention on CLS token...
  0%|          | 22/25000 [00:38<10:43:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:13,592 >> Initializing global attention on CLS token...
  0%|          | 23/25000 [00:39<10:48:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:15,178 >> Initializing global attention on CLS token...
  0%|          | 24/25000 [00:41<10:46:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:16,719 >> Initializing global attention on CLS token...
  0%|          | 25/25000 [00:42<10:44:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:18,260 >> Initializing global attention on CLS token...
  0%|          | 26/25000 [00:44<10:43:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:19,800 >> Initializing global attention on CLS token...
  0%|          | 27/25000 [00:46<10:46:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:21,370 >> Initializing global attention on CLS token...
  0%|          | 28/25000 [00:47<10:45:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:22,914 >> Initializing global attention on CLS token...
  0%|          | 29/25000 [00:49<10:44:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:24,455 >> Initializing global attention on CLS token...
  0%|          | 30/25000 [00:50<10:46:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:26,019 >> Initializing global attention on CLS token...
  0%|          | 31/25000 [00:52<10:44:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:27,562 >> Initializing global attention on CLS token...
  0%|          | 32/25000 [00:53<10:44:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:29,107 >> Initializing global attention on CLS token...
  0%|          | 33/25000 [00:55<10:54:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:30,735 >> Initializing global attention on CLS token...
  0%|          | 34/25000 [00:56<10:51:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:32,284 >> Initializing global attention on CLS token...
  0%|          | 35/25000 [00:58<10:49:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:33,832 >> Initializing global attention on CLS token...
  0%|          | 36/25000 [01:00<10:47:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:35,378 >> Initializing global attention on CLS token...
  0%|          | 37/25000 [01:01<10:46:32,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:36,928 >> Initializing global attention on CLS token...
  0%|          | 38/25000 [01:03<10:46:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:38,478 >> Initializing global attention on CLS token...
  0%|          | 39/25000 [01:04<10:45:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:40,026 >> Initializing global attention on CLS token...
  0%|          | 40/25000 [01:06<10:45:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:41,580 >> Initializing global attention on CLS token...
  0%|          | 41/25000 [01:07<10:45:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:43,127 >> Initializing global attention on CLS token...
  0%|          | 42/25000 [01:09<10:44:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:44,676 >> Initializing global attention on CLS token...
  0%|          | 43/25000 [01:10<10:44:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:46,224 >> Initializing global attention on CLS token...
  0%|          | 44/25000 [01:12<10:45:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:47,786 >> Initializing global attention on CLS token...
  0%|          | 45/25000 [01:13<10:45:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:49,334 >> Initializing global attention on CLS token...
  0%|          | 46/25000 [01:15<10:44:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:50,881 >> Initializing global attention on CLS token...
  0%|          | 47/25000 [01:17<10:45:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:52,439 >> Initializing global attention on CLS token...
  0%|          | 48/25000 [01:18<10:45:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:53,988 >> Initializing global attention on CLS token...
  0%|          | 49/25000 [01:20<10:45:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:55,543 >> Initializing global attention on CLS token...
  0%|          | 50/25000 [01:21<10:47:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:57,111 >> Initializing global attention on CLS token...
  0%|          | 51/25000 [01:23<10:48:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:19:58,673 >> Initializing global attention on CLS token...
  0%|          | 52/25000 [01:24<10:47:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:00,225 >> Initializing global attention on CLS token...
  0%|          | 53/25000 [01:26<10:46:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:01,775 >> Initializing global attention on CLS token...
  0%|          | 54/25000 [01:28<10:48:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:03,349 >> Initializing global attention on CLS token...
  0%|          | 55/25000 [01:29<10:47:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:04,902 >> Initializing global attention on CLS token...
  0%|          | 56/25000 [01:31<10:46:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:06,453 >> Initializing global attention on CLS token...
  0%|          | 57/25000 [01:32<10:52:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:08,051 >> Initializing global attention on CLS token...
  0%|          | 58/25000 [01:34<10:49:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:09,602 >> Initializing global attention on CLS token...
  0%|          | 59/25000 [01:35<10:48:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:11,155 >> Initializing global attention on CLS token...
  0%|          | 60/25000 [01:37<11:01:47,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:12,819 >> Initializing global attention on CLS token...
  0%|          | 61/25000 [01:39<10:56:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:14,373 >> Initializing global attention on CLS token...
  0%|          | 62/25000 [01:40<10:53:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:15,922 >> Initializing global attention on CLS token...
  0%|          | 63/25000 [01:42<10:50:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:17,472 >> Initializing global attention on CLS token...
  0%|          | 64/25000 [01:43<10:56:47,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:19,090 >> Initializing global attention on CLS token...
  0%|          | 65/25000 [01:45<10:53:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:20,640 >> Initializing global attention on CLS token...
  0%|          | 66/25000 [01:46<10:50:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:22,189 >> Initializing global attention on CLS token...
  0%|          | 67/25000 [01:48<10:49:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:23,747 >> Initializing global attention on CLS token...
  0%|          | 68/25000 [01:49<10:47:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:25,296 >> Initializing global attention on CLS token...
  0%|          | 69/25000 [01:51<10:46:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:26,847 >> Initializing global attention on CLS token...
  0%|          | 70/25000 [01:53<10:45:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:28,396 >> Initializing global attention on CLS token...
  0%|          | 71/25000 [01:54<10:47:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:29,964 >> Initializing global attention on CLS token...
  0%|          | 72/25000 [01:56<10:46:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:31,514 >> Initializing global attention on CLS token...
  0%|          | 73/25000 [01:57<10:45:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:33,066 >> Initializing global attention on CLS token...
  0%|          | 74/25000 [01:59<10:46:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:34,627 >> Initializing global attention on CLS token...
  0%|          | 75/25000 [02:00<10:45:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:36,177 >> Initializing global attention on CLS token...
  0%|          | 76/25000 [02:02<10:45:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:37,727 >> Initializing global attention on CLS token...
  0%|          | 77/25000 [02:03<10:49:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:39,314 >> Initializing global attention on CLS token...
  0%|          | 78/25000 [02:05<10:47:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:40,865 >> Initializing global attention on CLS token...
  0%|          | 79/25000 [02:07<10:47:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:42,420 >> Initializing global attention on CLS token...
  0%|          | 80/25000 [02:08<10:46:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:43,971 >> Initializing global attention on CLS token...
  0%|          | 81/25000 [02:10<10:48:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:45,544 >> Initializing global attention on CLS token...
  0%|          | 82/25000 [02:11<10:47:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:47,099 >> Initializing global attention on CLS token...
  0%|          | 83/25000 [02:13<10:46:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:48,649 >> Initializing global attention on CLS token...
  0%|          | 84/25000 [02:14<10:47:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:50,210 >> Initializing global attention on CLS token...
  0%|          | 85/25000 [02:16<10:46:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:51,762 >> Initializing global attention on CLS token...
  0%|          | 86/25000 [02:17<10:45:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:53,313 >> Initializing global attention on CLS token...
  0%|          | 87/25000 [02:19<10:46:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:54,873 >> Initializing global attention on CLS token...
  0%|          | 88/25000 [02:21<10:46:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:56,429 >> Initializing global attention on CLS token...
  0%|          | 89/25000 [02:22<10:47:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:57,993 >> Initializing global attention on CLS token...
  0%|          | 90/25000 [02:24<10:46:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:20:59,551 >> Initializing global attention on CLS token...
  0%|          | 91/25000 [02:25<10:54:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:01,172 >> Initializing global attention on CLS token...
  0%|          | 92/25000 [02:27<10:51:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:02,727 >> Initializing global attention on CLS token...
  0%|          | 93/25000 [02:28<10:49:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:04,278 >> Initializing global attention on CLS token...
  0%|          | 94/25000 [02:30<10:50:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:05,853 >> Initializing global attention on CLS token...
  0%|          | 95/25000 [02:32<10:49:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:07,413 >> Initializing global attention on CLS token...
  0%|          | 96/25000 [02:33<10:48:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:08,968 >> Initializing global attention on CLS token...
  0%|          | 97/25000 [02:35<10:47:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:10,521 >> Initializing global attention on CLS token...
  0%|          | 98/25000 [02:36<10:53:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:12,128 >> Initializing global attention on CLS token...
  0%|          | 99/25000 [02:38<10:50:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:13,684 >> Initializing global attention on CLS token...
  0%|          | 100/25000 [02:39<10:49:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:15,237 >> Initializing global attention on CLS token...
  0%|          | 101/25000 [02:41<10:50:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:16,816 >> Initializing global attention on CLS token...
  0%|          | 102/25000 [02:43<10:48:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:18,367 >> Initializing global attention on CLS token...
  0%|          | 103/25000 [02:44<10:47:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:19,920 >> Initializing global attention on CLS token...
  0%|          | 104/25000 [02:46<10:46:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:21,479 >> Initializing global attention on CLS token...
  0%|          | 105/25000 [02:47<10:47:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:23,043 >> Initializing global attention on CLS token...
  0%|          | 106/25000 [02:49<10:48:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:24,612 >> Initializing global attention on CLS token...
  0%|          | 107/25000 [02:50<10:51:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:26,199 >> Initializing global attention on CLS token...
  0%|          | 108/25000 [02:52<10:50:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:27,763 >> Initializing global attention on CLS token...
  0%|          | 109/25000 [02:53<10:50:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:29,326 >> Initializing global attention on CLS token...
  0%|          | 110/25000 [02:55<10:48:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:30,879 >> Initializing global attention on CLS token...
  0%|          | 111/25000 [02:57<10:47:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:32,433 >> Initializing global attention on CLS token...
  0%|          | 112/25000 [02:58<10:50:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:34,022 >> Initializing global attention on CLS token...
  0%|          | 113/25000 [03:00<10:49:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:35,577 >> Initializing global attention on CLS token...
  0%|          | 114/25000 [03:01<10:47:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:37,130 >> Initializing global attention on CLS token...
  0%|          | 115/25000 [03:03<10:46:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:38,687 >> Initializing global attention on CLS token...
  0%|          | 116/25000 [03:04<10:47:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:40,253 >> Initializing global attention on CLS token...
  0%|          | 117/25000 [03:06<10:46:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:41,804 >> Initializing global attention on CLS token...
  0%|          | 118/25000 [03:08<10:45:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:43,354 >> Initializing global attention on CLS token...
  0%|          | 119/25000 [03:09<10:46:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:44,920 >> Initializing global attention on CLS token...
  0%|          | 120/25000 [03:11<10:46:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:46,477 >> Initializing global attention on CLS token...
  0%|          | 121/25000 [03:12<10:45:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:48,029 >> Initializing global attention on CLS token...
  0%|          | 122/25000 [03:14<10:46:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:49,602 >> Initializing global attention on CLS token...
  0%|          | 123/25000 [03:15<10:48:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:51,171 >> Initializing global attention on CLS token...
  0%|          | 124/25000 [03:17<10:46:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:52,702 >> Initializing global attention on CLS token...
  0%|          | 125/25000 [03:18<10:43:05,  1.55s/it]                                                        0%|          | 125/25000 [03:18<10:43:05,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:21:54,236 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:21:54,239 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:21:54,239 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:21:54,239 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:54,252 >> Initializing global attention on CLS token...
{'loss': 5.9656, 'learning_rate': 6.965e-05, 'epoch': 1.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:54,372 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:54,492 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:54,614 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:54,734 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:54,931 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:07,  7.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,090 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:07,  7.69it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,210 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:07,  7.86it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,329 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:06,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,450 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,570 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,691 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,812 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,932 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,053 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,174 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,295 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:02<00:05,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,415 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,535 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,656 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,776 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,896 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,017 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,137 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,257 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,377 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,498 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,618 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,737 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,858 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:57,977 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,097 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,219 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,338 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,462 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,581 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,701 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,821 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,940 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,061 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,181 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,300 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,420 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,542 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,663 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,785 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,906 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,029 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,150 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:02,  6.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,369 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  7.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,489 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  7.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,609 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  7.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,730 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  7.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,849 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  7.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:00,969 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,091 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,212 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,332 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,452 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,572 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,692 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,809 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,925 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  0%|          | 125/25000 [03:26<10:43:05,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.33it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:22:02,012 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-125
[INFO|configuration_utils.py:461] 2024-01-22 00:22:02,021 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-125/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:22:02,553 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:22:02,555 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:22:02,555 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-125/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:22:03,588 >> Initializing global attention on CLS token...
  1%|          | 126/25000 [03:29<30:03:48,  4.35s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:05,136 >> Initializing global attention on CLS token...
  1%|          | 127/25000 [03:31<24:18:11,  3.52s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:06,710 >> Initializing global attention on CLS token...
  1%|          | 128/25000 [03:32<20:13:41,  2.93s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:08,261 >> Initializing global attention on CLS token...
  1%|          | 129/25000 [03:34<17:22:27,  2.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:09,812 >> Initializing global attention on CLS token...
  1%|          | 130/25000 [03:36<15:23:40,  2.23s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:11,372 >> Initializing global attention on CLS token...
  1%|          | 131/25000 [03:37<13:59:05,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:12,921 >> Initializing global attention on CLS token...
  1%|          | 132/25000 [03:39<13:00:05,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:14,471 >> Initializing global attention on CLS token...
  1%|          | 133/25000 [03:40<12:19:04,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:16,023 >> Initializing global attention on CLS token...
  1%|          | 134/25000 [03:42<11:55:31,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:17,624 >> Initializing global attention on CLS token...
  1%|          | 135/25000 [03:43<11:36:45,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:19,193 >> Initializing global attention on CLS token...
  1%|          | 136/25000 [03:45<11:20:24,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:20,743 >> Initializing global attention on CLS token...
  1%|          | 137/25000 [03:46<11:10:24,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:22,305 >> Initializing global attention on CLS token...
  1%|          | 138/25000 [03:48<11:02:05,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:23,856 >> Initializing global attention on CLS token...
  1%|          | 139/25000 [03:50<10:56:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:25,407 >> Initializing global attention on CLS token...
  1%|          | 140/25000 [03:51<11:00:05,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:27,028 >> Initializing global attention on CLS token...
  1%|          | 141/25000 [03:53<10:55:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:28,580 >> Initializing global attention on CLS token...
  1%|          | 142/25000 [03:54<10:52:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:30,138 >> Initializing global attention on CLS token...
  1%|          | 143/25000 [03:56<10:49:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:31,689 >> Initializing global attention on CLS token...
  1%|          | 144/25000 [03:57<10:48:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:33,245 >> Initializing global attention on CLS token...
  1%|          | 145/25000 [03:59<10:46:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:34,799 >> Initializing global attention on CLS token...
  1%|          | 146/25000 [04:01<10:47:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:36,365 >> Initializing global attention on CLS token...
  1%|          | 147/25000 [04:02<10:48:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:37,940 >> Initializing global attention on CLS token...
  1%|          | 148/25000 [04:04<10:47:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:39,492 >> Initializing global attention on CLS token...
  1%|          | 149/25000 [04:05<10:47:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:41,056 >> Initializing global attention on CLS token...
  1%|          | 150/25000 [04:07<10:46:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:42,610 >> Initializing global attention on CLS token...
  1%|          | 151/25000 [04:08<10:46:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:44,191 >> Initializing global attention on CLS token...
  1%|          | 152/25000 [04:10<10:47:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:45,744 >> Initializing global attention on CLS token...
  1%|          | 153/25000 [04:11<10:48:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:47,320 >> Initializing global attention on CLS token...
  1%|          | 154/25000 [04:13<10:48:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:48,885 >> Initializing global attention on CLS token...
  1%|          | 155/25000 [04:15<10:47:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:50,439 >> Initializing global attention on CLS token...
  1%|          | 156/25000 [04:16<10:53:37,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:52,054 >> Initializing global attention on CLS token...
  1%|          | 157/25000 [04:18<10:50:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:53,607 >> Initializing global attention on CLS token...
  1%|          | 158/25000 [04:19<10:49:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:55,168 >> Initializing global attention on CLS token...
  1%|          | 159/25000 [04:21<10:47:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:56,724 >> Initializing global attention on CLS token...
  1%|          | 160/25000 [04:22<10:52:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:58,323 >> Initializing global attention on CLS token...
  1%|          | 161/25000 [04:24<10:50:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:59,888 >> Initializing global attention on CLS token...
  1%|          | 162/25000 [04:26<10:48:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:01,444 >> Initializing global attention on CLS token...
  1%|          | 163/25000 [04:27<10:46:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:02,997 >> Initializing global attention on CLS token...
  1%|          | 164/25000 [04:29<10:45:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:04,548 >> Initializing global attention on CLS token...
  1%|          | 165/25000 [04:30<10:50:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:06,146 >> Initializing global attention on CLS token...
  1%|          | 166/25000 [04:32<10:47:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:07,697 >> Initializing global attention on CLS token...
  1%|          | 167/25000 [04:33<10:45:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:09,248 >> Initializing global attention on CLS token...
  1%|          | 168/25000 [04:35<10:48:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:10,829 >> Initializing global attention on CLS token...
  1%|          | 169/25000 [04:37<10:47:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:12,385 >> Initializing global attention on CLS token...
  1%|          | 170/25000 [04:38<10:45:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:13,938 >> Initializing global attention on CLS token...
  1%|          | 171/25000 [04:40<10:44:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:15,489 >> Initializing global attention on CLS token...
  1%|          | 172/25000 [04:41<10:46:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:17,068 >> Initializing global attention on CLS token...
  1%|          | 173/25000 [04:43<10:46:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:18,623 >> Initializing global attention on CLS token...
  1%|          | 174/25000 [04:44<10:45:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:20,175 >> Initializing global attention on CLS token...
  1%|          | 175/25000 [04:46<10:49:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:21,770 >> Initializing global attention on CLS token...
  1%|          | 176/25000 [04:47<10:47:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:23,324 >> Initializing global attention on CLS token...
  1%|          | 177/25000 [04:49<10:46:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:24,878 >> Initializing global attention on CLS token...
  1%|          | 178/25000 [04:51<10:49:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:26,468 >> Initializing global attention on CLS token...
  1%|          | 179/25000 [04:52<10:47:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:28,022 >> Initializing global attention on CLS token...
  1%|          | 180/25000 [04:54<10:45:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:29,573 >> Initializing global attention on CLS token...
  1%|          | 181/25000 [04:55<10:44:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:31,125 >> Initializing global attention on CLS token...
  1%|          | 182/25000 [04:57<10:45:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:32,687 >> Initializing global attention on CLS token...
  1%|          | 183/25000 [04:58<10:43:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:34,238 >> Initializing global attention on CLS token...
  1%|          | 184/25000 [05:00<10:43:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:35,789 >> Initializing global attention on CLS token...
  1%|          | 185/25000 [05:01<10:42:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:37,339 >> Initializing global attention on CLS token...
  1%|          | 186/25000 [05:03<10:43:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:38,899 >> Initializing global attention on CLS token...
  1%|          | 187/25000 [05:05<10:42:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:40,450 >> Initializing global attention on CLS token...
  1%|          | 188/25000 [05:06<10:42:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:41,999 >> Initializing global attention on CLS token...
  1%|          | 189/25000 [05:08<10:47:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:43,600 >> Initializing global attention on CLS token...
  1%|          | 190/25000 [05:09<10:45:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:45,149 >> Initializing global attention on CLS token...
  1%|          | 191/25000 [05:11<10:44:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:46,700 >> Initializing global attention on CLS token...
  1%|          | 192/25000 [05:12<10:43:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:48,253 >> Initializing global attention on CLS token...
  1%|          | 193/25000 [05:14<10:54:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:49,897 >> Initializing global attention on CLS token...
  1%|          | 194/25000 [05:16<10:50:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:51,451 >> Initializing global attention on CLS token...
  1%|          | 195/25000 [05:17<10:48:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:53,003 >> Initializing global attention on CLS token...
  1%|          | 196/25000 [05:19<11:01:25,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:54,679 >> Initializing global attention on CLS token...
  1%|          | 197/25000 [05:20<10:55:26,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:56,230 >> Initializing global attention on CLS token...
  1%|          | 198/25000 [05:22<10:51:19,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:57,783 >> Initializing global attention on CLS token...
  1%|          | 199/25000 [05:23<10:48:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:59,333 >> Initializing global attention on CLS token...
  1%|          | 200/25000 [05:25<10:58:24,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:00,984 >> Initializing global attention on CLS token...
  1%|          | 201/25000 [05:27<10:53:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:02,538 >> Initializing global attention on CLS token...
  1%|          | 202/25000 [05:28<10:49:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:04,088 >> Initializing global attention on CLS token...
  1%|          | 203/25000 [05:30<10:47:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:05,639 >> Initializing global attention on CLS token...
  1%|          | 204/25000 [05:31<10:57:47,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:07,292 >> Initializing global attention on CLS token...
  1%|          | 205/25000 [05:33<10:52:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:08,842 >> Initializing global attention on CLS token...
  1%|          | 206/25000 [05:35<10:48:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:10,390 >> Initializing global attention on CLS token...
  1%|          | 207/25000 [05:36<10:55:23,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:12,014 >> Initializing global attention on CLS token...
  1%|          | 208/25000 [05:38<10:51:10,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:13,567 >> Initializing global attention on CLS token...
  1%|          | 209/25000 [05:39<10:48:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:15,119 >> Initializing global attention on CLS token...
  1%|          | 210/25000 [05:41<10:48:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:16,689 >> Initializing global attention on CLS token...
  1%|          | 211/25000 [05:42<10:46:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:18,242 >> Initializing global attention on CLS token...
  1%|          | 212/25000 [05:44<10:44:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:19,792 >> Initializing global attention on CLS token...
  1%|          | 213/25000 [05:45<10:43:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:21,345 >> Initializing global attention on CLS token...
  1%|          | 214/25000 [05:47<10:52:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:22,978 >> Initializing global attention on CLS token...
  1%|          | 215/25000 [05:49<10:49:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:24,530 >> Initializing global attention on CLS token...
  1%|          | 216/25000 [05:50<10:47:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:26,088 >> Initializing global attention on CLS token...
  1%|          | 217/25000 [05:52<10:46:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:27,650 >> Initializing global attention on CLS token...
  1%|          | 218/25000 [05:53<10:45:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:29,203 >> Initializing global attention on CLS token...
  1%|          | 219/25000 [05:55<10:43:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:30,755 >> Initializing global attention on CLS token...
  1%|          | 220/25000 [05:56<10:43:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:32,314 >> Initializing global attention on CLS token...
  1%|          | 221/25000 [05:58<10:49:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:33,923 >> Initializing global attention on CLS token...
  1%|          | 222/25000 [06:00<10:48:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:35,477 >> Initializing global attention on CLS token...
  1%|          | 223/25000 [06:01<10:46:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:37,036 >> Initializing global attention on CLS token...
  1%|          | 224/25000 [06:03<10:49:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:38,624 >> Initializing global attention on CLS token...
  1%|          | 225/25000 [06:04<10:46:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:40,174 >> Initializing global attention on CLS token...
  1%|          | 226/25000 [06:06<10:44:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:41,726 >> Initializing global attention on CLS token...
  1%|          | 227/25000 [06:08<10:53:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:43,360 >> Initializing global attention on CLS token...
  1%|          | 228/25000 [06:09<10:50:43,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:44,918 >> Initializing global attention on CLS token...
  1%|          | 229/25000 [06:11<10:48:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:46,486 >> Initializing global attention on CLS token...
  1%|          | 230/25000 [06:12<10:47:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:48,039 >> Initializing global attention on CLS token...
  1%|          | 231/25000 [06:14<10:50:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:49,629 >> Initializing global attention on CLS token...
  1%|          | 232/25000 [06:15<10:46:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:51,177 >> Initializing global attention on CLS token...
  1%|          | 233/25000 [06:17<10:44:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:52,730 >> Initializing global attention on CLS token...
  1%|          | 234/25000 [06:18<10:48:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:54,324 >> Initializing global attention on CLS token...
  1%|          | 235/25000 [06:20<10:46:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:55,875 >> Initializing global attention on CLS token...
  1%|          | 236/25000 [06:22<10:44:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:57,427 >> Initializing global attention on CLS token...
  1%|          | 237/25000 [06:23<10:43:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,981 >> Initializing global attention on CLS token...
  1%|          | 238/25000 [06:25<10:54:47,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,632 >> Initializing global attention on CLS token...
  1%|          | 239/25000 [06:26<10:50:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,185 >> Initializing global attention on CLS token...
  1%|          | 240/25000 [06:28<10:47:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,734 >> Initializing global attention on CLS token...
  1%|          | 241/25000 [06:29<10:45:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,291 >> Initializing global attention on CLS token...
  1%|          | 242/25000 [06:31<10:45:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:06,853 >> Initializing global attention on CLS token...
  1%|          | 243/25000 [06:33<10:43:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:08,405 >> Initializing global attention on CLS token...
  1%|          | 244/25000 [06:34<10:43:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:09,966 >> Initializing global attention on CLS token...
  1%|          | 245/25000 [06:36<10:53:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:11,601 >> Initializing global attention on CLS token...
  1%|          | 246/25000 [06:37<10:49:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:13,154 >> Initializing global attention on CLS token...
  1%|          | 247/25000 [06:39<10:47:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:14,802 >> Initializing global attention on CLS token...
  1%|          | 248/25000 [06:41<11:03:56,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:16,413 >> Initializing global attention on CLS token...
  1%|          | 249/25000 [06:42<10:56:31,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:17,944 >> Initializing global attention on CLS token...
  1%|          | 250/25000 [06:44<10:49:18,  1.57s/it]                                                        1%|          | 250/25000 [06:44<10:49:18,  1.57s/it][INFO|trainer.py:738] 2024-01-22 00:25:19,480 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:25:19,483 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:25:19,483 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:25:19,483 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:19,496 >> Initializing global attention on CLS token...
{'eval_loss': 5.377981662750244, 'eval_accuracy': 0.196, 'eval_macro_f1': 0.015691734450393416, 'eval_macro_precision': 0.015750603662733207, 'eval_macro_recall': 0.03722914409534128, 'eval_micro_f1': 0.19600000000000004, 'eval_micro_precision': 0.196, 'eval_micro_recall': 0.196, 'eval_combined_score': 0.12181021174406685, 'eval_runtime': 7.7713, 'eval_samples_per_second': 64.339, 'eval_steps_per_second': 8.107, 'epoch': 1.0}
{'loss': 4.8497, 'learning_rate': 6.929999999999999e-05, 'epoch': 2.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:19,616 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.64it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:19,737 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:19,857 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:19,977 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,096 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,217 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,337 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,457 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,580 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,700 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,820 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,941 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,062 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,182 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,304 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,424 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,544 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:07,  6.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,790 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:06,  6.83it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,911 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  7.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,031 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  7.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,152 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  7.73it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,272 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:05,  7.90it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,392 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.03it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,512 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,633 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,754 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,873 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:22,994 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,114 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,234 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,354 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,475 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,596 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,719 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,842 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,965 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,085 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,206 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,326 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,447 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,567 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,687 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,807 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,927 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,051 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,171 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,297 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,418 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,538 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,658 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,779 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:25,899 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,018 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,138 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,258 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,377 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,497 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,617 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,738 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,858 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,974 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  7.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:27,113 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  1%|          | 250/25000 [06:51<10:49:18,  1.57s/it]
100%|██████████| 63/63 [00:07<00:00,  7.98it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:25:27,210 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-250
[INFO|configuration_utils.py:461] 2024-01-22 00:25:27,216 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-250/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:25:27,757 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:25:27,759 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:25:27,759 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-250/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:28,890 >> Initializing global attention on CLS token...
  1%|          | 251/25000 [06:55<30:08:03,  4.38s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:30,436 >> Initializing global attention on CLS token...
  1%|          | 252/25000 [06:56<24:17:45,  3.53s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:31,989 >> Initializing global attention on CLS token...
  1%|          | 253/25000 [06:58<20:19:49,  2.96s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:33,608 >> Initializing global attention on CLS token...
  1%|          | 254/25000 [06:59<17:26:30,  2.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:35,158 >> Initializing global attention on CLS token...
  1%|          | 255/25000 [07:01<15:24:16,  2.24s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:36,708 >> Initializing global attention on CLS token...
  1%|          | 256/25000 [07:02<14:01:38,  2.04s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:38,281 >> Initializing global attention on CLS token...
  1%|          | 257/25000 [07:04<13:01:03,  1.89s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:39,832 >> Initializing global attention on CLS token...
  1%|          | 258/25000 [07:06<12:18:11,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:41,380 >> Initializing global attention on CLS token...
  1%|          | 259/25000 [07:07<11:48:05,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:42,927 >> Initializing global attention on CLS token...
  1%|          | 260/25000 [07:09<11:29:28,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:44,494 >> Initializing global attention on CLS token...
  1%|          | 261/25000 [07:10<11:14:35,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:46,047 >> Initializing global attention on CLS token...
  1%|          | 262/25000 [07:12<11:04:14,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:47,599 >> Initializing global attention on CLS token...
  1%|          | 263/25000 [07:13<11:00:08,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:49,177 >> Initializing global attention on CLS token...
  1%|          | 264/25000 [07:15<10:53:27,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:50,726 >> Initializing global attention on CLS token...
  1%|          | 265/25000 [07:16<10:49:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:52,274 >> Initializing global attention on CLS token...
  1%|          | 266/25000 [07:18<10:45:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:53,823 >> Initializing global attention on CLS token...
  1%|          | 267/25000 [07:20<10:58:04,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:55,489 >> Initializing global attention on CLS token...
  1%|          | 268/25000 [07:21<10:52:12,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:57,038 >> Initializing global attention on CLS token...
  1%|          | 269/25000 [07:23<10:48:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:58,590 >> Initializing global attention on CLS token...
  1%|          | 270/25000 [07:24<10:48:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:00,162 >> Initializing global attention on CLS token...
  1%|          | 271/25000 [07:26<10:45:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:01,713 >> Initializing global attention on CLS token...
  1%|          | 272/25000 [07:27<10:43:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:03,263 >> Initializing global attention on CLS token...
  1%|          | 273/25000 [07:29<10:54:12,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:04,911 >> Initializing global attention on CLS token...
  1%|          | 274/25000 [07:31<10:50:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:06,466 >> Initializing global attention on CLS token...
  1%|          | 275/25000 [07:32<10:46:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:08,014 >> Initializing global attention on CLS token...
  1%|          | 276/25000 [07:34<10:44:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:09,564 >> Initializing global attention on CLS token...
  1%|          | 277/25000 [07:35<10:43:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:11,120 >> Initializing global attention on CLS token...
  1%|          | 278/25000 [07:37<10:42:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:12,670 >> Initializing global attention on CLS token...
  1%|          | 279/25000 [07:38<10:40:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:14,220 >> Initializing global attention on CLS token...
  1%|          | 280/25000 [07:40<10:45:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:15,816 >> Initializing global attention on CLS token...
  1%|          | 281/25000 [07:42<10:43:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:17,369 >> Initializing global attention on CLS token...
  1%|          | 282/25000 [07:43<10:42:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:18,922 >> Initializing global attention on CLS token...
  1%|          | 283/25000 [07:45<10:41:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:20,475 >> Initializing global attention on CLS token...
  1%|          | 284/25000 [07:46<10:44:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:22,057 >> Initializing global attention on CLS token...
  1%|          | 285/25000 [07:48<10:42:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:23,605 >> Initializing global attention on CLS token...
  1%|          | 286/25000 [07:49<10:41:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:25,160 >> Initializing global attention on CLS token...
  1%|          | 287/25000 [07:51<10:40:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:26,710 >> Initializing global attention on CLS token...
  1%|          | 288/25000 [07:53<10:57:09,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:28,398 >> Initializing global attention on CLS token...
  1%|          | 289/25000 [07:54<10:51:30,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:29,948 >> Initializing global attention on CLS token...
  1%|          | 290/25000 [07:56<10:47:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:31,499 >> Initializing global attention on CLS token...
  1%|          | 291/25000 [07:57<10:50:58,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:33,102 >> Initializing global attention on CLS token...
  1%|          | 292/25000 [07:59<10:47:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:34,653 >> Initializing global attention on CLS token...
  1%|          | 293/25000 [08:00<10:44:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:36,202 >> Initializing global attention on CLS token...
  1%|          | 294/25000 [08:02<10:43:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:37,760 >> Initializing global attention on CLS token...
  1%|          | 295/25000 [08:03<10:43:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:39,326 >> Initializing global attention on CLS token...
  1%|          | 296/25000 [08:05<10:42:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:40,878 >> Initializing global attention on CLS token...
  1%|          | 297/25000 [08:07<10:41:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:42,431 >> Initializing global attention on CLS token...
  1%|          | 298/25000 [08:08<10:41:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:43,989 >> Initializing global attention on CLS token...
  1%|          | 299/25000 [08:10<10:47:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:45,595 >> Initializing global attention on CLS token...
  1%|          | 300/25000 [08:11<10:44:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:47,145 >> Initializing global attention on CLS token...
  1%|          | 301/25000 [08:13<10:42:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:48,696 >> Initializing global attention on CLS token...
  1%|          | 302/25000 [08:14<10:43:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:50,261 >> Initializing global attention on CLS token...
  1%|          | 303/25000 [08:16<10:41:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:51,811 >> Initializing global attention on CLS token...
  1%|          | 304/25000 [08:18<10:40:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:53,363 >> Initializing global attention on CLS token...
  1%|          | 305/25000 [08:19<10:39:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:54,914 >> Initializing global attention on CLS token...
  1%|          | 306/25000 [08:21<10:43:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:56,493 >> Initializing global attention on CLS token...
  1%|          | 307/25000 [08:22<10:41:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:58,045 >> Initializing global attention on CLS token...
  1%|          | 308/25000 [08:24<10:40:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:59,600 >> Initializing global attention on CLS token...
  1%|          | 309/25000 [08:25<10:40:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:01,152 >> Initializing global attention on CLS token...
  1%|          | 310/25000 [08:27<10:54:26,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:02,822 >> Initializing global attention on CLS token...
  1%|          | 311/25000 [08:29<10:49:43,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:04,374 >> Initializing global attention on CLS token...
  1%|          | 312/25000 [08:30<10:46:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:05,927 >> Initializing global attention on CLS token...
  1%|▏         | 313/25000 [08:32<10:45:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:07,493 >> Initializing global attention on CLS token...
  1%|▏         | 314/25000 [08:33<11:00:46,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:09,184 >> Initializing global attention on CLS token...
  1%|▏         | 315/25000 [08:35<11:00:24,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:10,787 >> Initializing global attention on CLS token...
  1%|▏         | 316/25000 [08:36<10:54:37,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:12,346 >> Initializing global attention on CLS token...
  1%|▏         | 317/25000 [08:38<10:52:48,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:13,956 >> Initializing global attention on CLS token...
  1%|▏         | 318/25000 [08:40<11:03:54,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:15,600 >> Initializing global attention on CLS token...
  1%|▏         | 319/25000 [08:41<10:56:06,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:17,151 >> Initializing global attention on CLS token...
  1%|▏         | 320/25000 [08:43<10:50:30,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:18,701 >> Initializing global attention on CLS token...
  1%|▏         | 321/25000 [08:44<10:46:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:20,253 >> Initializing global attention on CLS token...
  1%|▏         | 322/25000 [08:46<10:52:57,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:21,875 >> Initializing global attention on CLS token...
  1%|▏         | 323/25000 [08:48<10:48:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:23,431 >> Initializing global attention on CLS token...
  1%|▏         | 324/25000 [08:49<10:45:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:24,983 >> Initializing global attention on CLS token...
  1%|▏         | 325/25000 [08:51<10:50:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:26,591 >> Initializing global attention on CLS token...
  1%|▏         | 326/25000 [08:52<10:46:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:28,141 >> Initializing global attention on CLS token...
  1%|▏         | 327/25000 [08:54<10:43:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:29,692 >> Initializing global attention on CLS token...
  1%|▏         | 328/25000 [08:55<10:42:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:31,246 >> Initializing global attention on CLS token...
  1%|▏         | 329/25000 [08:57<10:44:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:32,824 >> Initializing global attention on CLS token...
  1%|▏         | 330/25000 [08:59<10:42:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:34,378 >> Initializing global attention on CLS token...
  1%|▏         | 331/25000 [09:00<10:41:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:35,931 >> Initializing global attention on CLS token...
  1%|▏         | 332/25000 [09:02<10:40:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:37,484 >> Initializing global attention on CLS token...
  1%|▏         | 333/25000 [09:03<10:46:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:39,087 >> Initializing global attention on CLS token...
  1%|▏         | 334/25000 [09:05<10:43:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:40,640 >> Initializing global attention on CLS token...
  1%|▏         | 335/25000 [09:06<10:41:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:42,192 >> Initializing global attention on CLS token...
  1%|▏         | 336/25000 [09:08<10:57:03,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:43,876 >> Initializing global attention on CLS token...
  1%|▏         | 337/25000 [09:10<10:50:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:45,425 >> Initializing global attention on CLS token...
  1%|▏         | 338/25000 [09:11<10:47:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:46,983 >> Initializing global attention on CLS token...
  1%|▏         | 339/25000 [09:13<10:44:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:48,531 >> Initializing global attention on CLS token...
  1%|▏         | 340/25000 [09:14<10:45:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:50,108 >> Initializing global attention on CLS token...
  1%|▏         | 341/25000 [09:16<10:43:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:51,661 >> Initializing global attention on CLS token...
  1%|▏         | 342/25000 [09:17<10:41:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:53,210 >> Initializing global attention on CLS token...
  1%|▏         | 343/25000 [09:19<10:40:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:54,766 >> Initializing global attention on CLS token...
  1%|▏         | 344/25000 [09:21<10:48:52,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:56,390 >> Initializing global attention on CLS token...
  1%|▏         | 345/25000 [09:22<10:45:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:57,945 >> Initializing global attention on CLS token...
  1%|▏         | 346/25000 [09:24<10:43:08,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:59,495 >> Initializing global attention on CLS token...
  1%|▏         | 347/25000 [09:25<10:41:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:01,058 >> Initializing global attention on CLS token...
  1%|▏         | 348/25000 [09:27<10:46:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:02,647 >> Initializing global attention on CLS token...
  1%|▏         | 349/25000 [09:28<10:43:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:04,196 >> Initializing global attention on CLS token...
  1%|▏         | 350/25000 [09:30<10:41:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:05,748 >> Initializing global attention on CLS token...
  1%|▏         | 351/25000 [09:31<10:43:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:07,326 >> Initializing global attention on CLS token...
  1%|▏         | 352/25000 [09:33<10:41:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:08,878 >> Initializing global attention on CLS token...
  1%|▏         | 353/25000 [09:35<10:40:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:10,429 >> Initializing global attention on CLS token...
  1%|▏         | 354/25000 [09:36<10:45:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:12,024 >> Initializing global attention on CLS token...
  1%|▏         | 355/25000 [09:38<10:42:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:13,575 >> Initializing global attention on CLS token...
  1%|▏         | 356/25000 [09:39<10:40:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:15,124 >> Initializing global attention on CLS token...
  1%|▏         | 357/25000 [09:41<10:39:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:16,674 >> Initializing global attention on CLS token...
  1%|▏         | 358/25000 [09:42<10:41:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:18,251 >> Initializing global attention on CLS token...
  1%|▏         | 359/25000 [09:44<10:40:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:19,802 >> Initializing global attention on CLS token...
  1%|▏         | 360/25000 [09:46<10:39:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:21,351 >> Initializing global attention on CLS token...
  1%|▏         | 361/25000 [09:47<10:43:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:22,942 >> Initializing global attention on CLS token...
  1%|▏         | 362/25000 [09:49<10:41:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,492 >> Initializing global attention on CLS token...
  1%|▏         | 363/25000 [09:50<10:39:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,042 >> Initializing global attention on CLS token...
  1%|▏         | 364/25000 [09:52<10:39:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,594 >> Initializing global attention on CLS token...
  1%|▏         | 365/25000 [09:53<10:42:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,177 >> Initializing global attention on CLS token...
  1%|▏         | 366/25000 [09:55<10:40:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:30,731 >> Initializing global attention on CLS token...
  1%|▏         | 367/25000 [09:56<10:39:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:32,283 >> Initializing global attention on CLS token...
  1%|▏         | 368/25000 [09:58<10:41:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:33,858 >> Initializing global attention on CLS token...
  1%|▏         | 369/25000 [10:00<10:40:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:35,409 >> Initializing global attention on CLS token...
  1%|▏         | 370/25000 [10:01<10:42:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:36,983 >> Initializing global attention on CLS token...
  1%|▏         | 371/25000 [10:03<10:40:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:38,536 >> Initializing global attention on CLS token...
  1%|▏         | 372/25000 [10:04<10:41:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:40,106 >> Initializing global attention on CLS token...
  1%|▏         | 373/25000 [10:06<10:40:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:41,659 >> Initializing global attention on CLS token...
  1%|▏         | 374/25000 [10:07<10:43:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:43,223 >> Initializing global attention on CLS token...
  2%|▏         | 375/25000 [10:09<10:38:57,  1.56s/it]                                                        2%|▏         | 375/25000 [10:09<10:38:57,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:28:44,756 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:28:44,759 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:28:44,759 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:28:44,760 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:44,772 >> Initializing global attention on CLS token...
{'eval_loss': 4.893075942993164, 'eval_accuracy': 0.252, 'eval_macro_f1': 0.033932074176563805, 'eval_macro_precision': 0.030963131809098432, 'eval_macro_recall': 0.06416400921609254, 'eval_micro_f1': 0.252, 'eval_micro_precision': 0.252, 'eval_micro_recall': 0.252, 'eval_combined_score': 0.16243703074310784, 'eval_runtime': 7.7249, 'eval_samples_per_second': 64.726, 'eval_steps_per_second': 8.155, 'epoch': 2.0}
{'loss': 4.257, 'learning_rate': 6.895e-05, 'epoch': 3.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:44,894 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:06,  9.67it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,102 >> Initializing global attention on CLS token...

  5%|▍         | 3/63 [00:00<00:06,  9.03it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,222 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:06,  8.73it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,344 >> Initializing global attention on CLS token...

  8%|▊         | 5/63 [00:00<00:06,  8.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,465 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  8.44it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,587 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,710 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,841 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:06,  8.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,964 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,086 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,208 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,330 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,452 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,572 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,692 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,813 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:02<00:05,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:46,934 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,056 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,178 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,299 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,421 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,543 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,665 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:05,  7.54it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,822 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  7.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,941 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  7.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,061 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  7.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,221 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  7.59it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,344 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,464 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  7.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,585 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,706 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,826 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:03,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:48,948 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,074 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,196 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,318 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,439 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,560 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,686 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,810 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:05<00:02,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,933 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,054 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,175 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,295 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,420 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,543 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,665 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:02,  6.81it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,870 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:06<00:01,  7.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,997 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  7.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,119 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  7.64it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,238 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  7.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,359 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,482 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,602 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,726 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,846 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:07<00:00,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:51,967 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:52,087 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:52,207 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:52,327 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:52,444 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:52,558 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  2%|▏         | 375/25000 [10:17<10:38:57,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.36it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:28:52,645 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-375
[INFO|configuration_utils.py:461] 2024-01-22 00:28:52,653 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-375/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:28:53,369 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-375/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:28:53,370 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-375/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:28:53,371 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-375/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:54,780 >> Initializing global attention on CLS token...
  2%|▏         | 376/25000 [10:21<31:23:11,  4.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:56,437 >> Initializing global attention on CLS token...
  2%|▏         | 377/25000 [10:22<25:08:35,  3.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:57,985 >> Initializing global attention on CLS token...
  2%|▏         | 378/25000 [10:24<20:46:40,  3.04s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:59,534 >> Initializing global attention on CLS token...
  2%|▏         | 379/25000 [10:25<17:43:34,  2.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:01,085 >> Initializing global attention on CLS token...
  2%|▏         | 380/25000 [10:27<15:36:03,  2.28s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:02,641 >> Initializing global attention on CLS token...
  2%|▏         | 381/25000 [10:28<14:05:42,  2.06s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:04,189 >> Initializing global attention on CLS token...
  2%|▏         | 382/25000 [10:30<13:03:04,  1.91s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:05,741 >> Initializing global attention on CLS token...
  2%|▏         | 383/25000 [10:32<12:27:49,  1.82s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:07,363 >> Initializing global attention on CLS token...
  2%|▏         | 384/25000 [10:33<11:53:57,  1.74s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:08,913 >> Initializing global attention on CLS token...
  2%|▏         | 385/25000 [10:35<11:30:50,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:10,464 >> Initializing global attention on CLS token...
  2%|▏         | 386/25000 [10:36<11:14:27,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:12,015 >> Initializing global attention on CLS token...
  2%|▏         | 387/25000 [10:38<11:03:30,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:13,570 >> Initializing global attention on CLS token...
  2%|▏         | 388/25000 [10:39<10:54:50,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:15,118 >> Initializing global attention on CLS token...
  2%|▏         | 389/25000 [10:41<10:49:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:16,667 >> Initializing global attention on CLS token...
  2%|▏         | 390/25000 [10:42<10:46:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:18,227 >> Initializing global attention on CLS token...
  2%|▏         | 391/25000 [10:44<10:42:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:19,777 >> Initializing global attention on CLS token...
  2%|▏         | 392/25000 [10:45<10:40:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:21,325 >> Initializing global attention on CLS token...
  2%|▏         | 393/25000 [10:47<10:38:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:22,874 >> Initializing global attention on CLS token...
  2%|▏         | 394/25000 [10:49<10:38:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:24,427 >> Initializing global attention on CLS token...
  2%|▏         | 395/25000 [10:50<10:37:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:25,977 >> Initializing global attention on CLS token...
  2%|▏         | 396/25000 [10:52<10:36:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:27,525 >> Initializing global attention on CLS token...
  2%|▏         | 397/25000 [10:53<10:36:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:29,073 >> Initializing global attention on CLS token...
  2%|▏         | 398/25000 [10:55<10:47:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:30,717 >> Initializing global attention on CLS token...
  2%|▏         | 399/25000 [10:56<10:43:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:32,267 >> Initializing global attention on CLS token...
  2%|▏         | 400/25000 [10:58<10:41:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:33,816 >> Initializing global attention on CLS token...
  2%|▏         | 401/25000 [11:00<10:42:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:35,390 >> Initializing global attention on CLS token...
  2%|▏         | 402/25000 [11:01<10:40:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:36,953 >> Initializing global attention on CLS token...
  2%|▏         | 403/25000 [11:03<10:40:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:38,504 >> Initializing global attention on CLS token...
  2%|▏         | 404/25000 [11:04<10:42:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:40,083 >> Initializing global attention on CLS token...
  2%|▏         | 405/25000 [11:06<10:44:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:41,664 >> Initializing global attention on CLS token...
  2%|▏         | 406/25000 [11:07<10:41:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:43,215 >> Initializing global attention on CLS token...
  2%|▏         | 407/25000 [11:09<10:39:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:44,768 >> Initializing global attention on CLS token...
  2%|▏         | 408/25000 [11:10<10:41:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:46,349 >> Initializing global attention on CLS token...
  2%|▏         | 409/25000 [11:12<10:41:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:47,905 >> Initializing global attention on CLS token...
  2%|▏         | 410/25000 [11:14<10:39:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:49,453 >> Initializing global attention on CLS token...
  2%|▏         | 411/25000 [11:15<10:38:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:51,004 >> Initializing global attention on CLS token...
  2%|▏         | 412/25000 [11:17<10:40:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:52,585 >> Initializing global attention on CLS token...
  2%|▏         | 413/25000 [11:18<10:39:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:54,138 >> Initializing global attention on CLS token...
  2%|▏         | 414/25000 [11:20<10:38:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:55,690 >> Initializing global attention on CLS token...
  2%|▏         | 415/25000 [11:21<10:50:19,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:57,345 >> Initializing global attention on CLS token...
  2%|▏         | 416/25000 [11:23<10:46:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:58,900 >> Initializing global attention on CLS token...
  2%|▏         | 417/25000 [11:25<10:43:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:00,455 >> Initializing global attention on CLS token...
  2%|▏         | 418/25000 [11:26<10:41:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:02,009 >> Initializing global attention on CLS token...
  2%|▏         | 419/25000 [11:28<10:47:14,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:03,623 >> Initializing global attention on CLS token...
  2%|▏         | 420/25000 [11:29<10:43:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:05,172 >> Initializing global attention on CLS token...
  2%|▏         | 421/25000 [11:31<10:41:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:06,724 >> Initializing global attention on CLS token...
  2%|▏         | 422/25000 [11:32<10:41:30,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:08,292 >> Initializing global attention on CLS token...
  2%|▏         | 423/25000 [11:34<10:39:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:09,845 >> Initializing global attention on CLS token...
  2%|▏         | 424/25000 [11:36<10:38:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:11,396 >> Initializing global attention on CLS token...
  2%|▏         | 425/25000 [11:37<10:37:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:12,948 >> Initializing global attention on CLS token...
  2%|▏         | 426/25000 [11:39<10:39:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:14,517 >> Initializing global attention on CLS token...
  2%|▏         | 427/25000 [11:40<10:38:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:16,069 >> Initializing global attention on CLS token...
  2%|▏         | 428/25000 [11:42<10:38:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:17,662 >> Initializing global attention on CLS token...
  2%|▏         | 429/25000 [11:43<10:45:05,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:19,242 >> Initializing global attention on CLS token...
  2%|▏         | 430/25000 [11:45<10:45:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:20,820 >> Initializing global attention on CLS token...
  2%|▏         | 431/25000 [11:47<10:43:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:22,381 >> Initializing global attention on CLS token...
  2%|▏         | 432/25000 [11:48<10:41:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:23,938 >> Initializing global attention on CLS token...
  2%|▏         | 433/25000 [11:50<10:45:25,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:25,536 >> Initializing global attention on CLS token...
  2%|▏         | 434/25000 [11:51<10:43:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:27,098 >> Initializing global attention on CLS token...
  2%|▏         | 435/25000 [11:53<10:41:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:28,649 >> Initializing global attention on CLS token...
  2%|▏         | 436/25000 [11:54<10:39:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:30,199 >> Initializing global attention on CLS token...
  2%|▏         | 437/25000 [11:56<10:41:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:31,784 >> Initializing global attention on CLS token...
  2%|▏         | 438/25000 [11:57<10:39:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:33,335 >> Initializing global attention on CLS token...
  2%|▏         | 439/25000 [11:59<10:38:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:34,888 >> Initializing global attention on CLS token...
  2%|▏         | 440/25000 [12:01<10:37:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:36,441 >> Initializing global attention on CLS token...
  2%|▏         | 441/25000 [12:02<10:50:00,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:38,099 >> Initializing global attention on CLS token...
  2%|▏         | 442/25000 [12:04<10:45:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:39,650 >> Initializing global attention on CLS token...
  2%|▏         | 443/25000 [12:05<10:42:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:41,204 >> Initializing global attention on CLS token...
  2%|▏         | 444/25000 [12:07<10:45:52,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:42,805 >> Initializing global attention on CLS token...
  2%|▏         | 445/25000 [12:09<10:42:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:44,356 >> Initializing global attention on CLS token...
  2%|▏         | 446/25000 [12:10<10:40:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:45,906 >> Initializing global attention on CLS token...
  2%|▏         | 447/25000 [12:12<10:38:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:47,457 >> Initializing global attention on CLS token...
  2%|▏         | 448/25000 [12:13<10:44:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:49,064 >> Initializing global attention on CLS token...
  2%|▏         | 449/25000 [12:15<10:41:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:50,616 >> Initializing global attention on CLS token...
  2%|▏         | 450/25000 [12:16<10:39:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:52,166 >> Initializing global attention on CLS token...
  2%|▏         | 451/25000 [12:18<10:38:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:53,723 >> Initializing global attention on CLS token...
  2%|▏         | 452/25000 [12:19<10:37:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:55,275 >> Initializing global attention on CLS token...
  2%|▏         | 453/25000 [12:21<10:36:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:56,826 >> Initializing global attention on CLS token...
  2%|▏         | 454/25000 [12:23<10:36:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:58,378 >> Initializing global attention on CLS token...
  2%|▏         | 455/25000 [12:24<10:38:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:59,956 >> Initializing global attention on CLS token...
  2%|▏         | 456/25000 [12:26<10:38:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:01,511 >> Initializing global attention on CLS token...
  2%|▏         | 457/25000 [12:27<10:36:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:03,061 >> Initializing global attention on CLS token...
  2%|▏         | 458/25000 [12:29<10:37:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:04,622 >> Initializing global attention on CLS token...
  2%|▏         | 459/25000 [12:30<10:36:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:06,174 >> Initializing global attention on CLS token...
  2%|▏         | 460/25000 [12:32<10:37:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:07,737 >> Initializing global attention on CLS token...
  2%|▏         | 461/25000 [12:33<10:36:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:09,288 >> Initializing global attention on CLS token...
  2%|▏         | 462/25000 [12:35<10:43:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:10,903 >> Initializing global attention on CLS token...
  2%|▏         | 463/25000 [12:37<10:48:09,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:12,514 >> Initializing global attention on CLS token...
  2%|▏         | 464/25000 [12:38<10:44:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:14,067 >> Initializing global attention on CLS token...
  2%|▏         | 465/25000 [12:40<10:44:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:15,644 >> Initializing global attention on CLS token...
  2%|▏         | 466/25000 [12:41<10:41:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:17,197 >> Initializing global attention on CLS token...
  2%|▏         | 467/25000 [12:43<10:41:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:18,767 >> Initializing global attention on CLS token...
  2%|▏         | 468/25000 [12:44<10:39:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:20,321 >> Initializing global attention on CLS token...
  2%|▏         | 469/25000 [12:46<10:42:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:21,911 >> Initializing global attention on CLS token...
  2%|▏         | 470/25000 [12:48<10:40:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:23,463 >> Initializing global attention on CLS token...
  2%|▏         | 471/25000 [12:49<10:38:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:25,014 >> Initializing global attention on CLS token...
  2%|▏         | 472/25000 [12:51<10:40:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:26,589 >> Initializing global attention on CLS token...
  2%|▏         | 473/25000 [12:52<10:38:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:28,141 >> Initializing global attention on CLS token...
  2%|▏         | 474/25000 [12:54<10:36:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:29,691 >> Initializing global attention on CLS token...
  2%|▏         | 475/25000 [12:55<10:35:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:31,239 >> Initializing global attention on CLS token...
  2%|▏         | 476/25000 [12:57<10:37:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:32,807 >> Initializing global attention on CLS token...
  2%|▏         | 477/25000 [12:59<10:36:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:34,361 >> Initializing global attention on CLS token...
  2%|▏         | 478/25000 [13:00<10:36:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:35,919 >> Initializing global attention on CLS token...
  2%|▏         | 479/25000 [13:02<10:38:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:37,491 >> Initializing global attention on CLS token...
  2%|▏         | 480/25000 [13:03<10:36:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:39,041 >> Initializing global attention on CLS token...
  2%|▏         | 481/25000 [13:05<10:36:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:40,594 >> Initializing global attention on CLS token...
  2%|▏         | 482/25000 [13:06<10:35:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:42,144 >> Initializing global attention on CLS token...
  2%|▏         | 483/25000 [13:08<10:36:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:43,706 >> Initializing global attention on CLS token...
  2%|▏         | 484/25000 [13:09<10:35:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:45,258 >> Initializing global attention on CLS token...
  2%|▏         | 485/25000 [13:11<10:34:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:46,808 >> Initializing global attention on CLS token...
  2%|▏         | 486/25000 [13:13<10:39:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,399 >> Initializing global attention on CLS token...
  2%|▏         | 487/25000 [13:14<10:37:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,948 >> Initializing global attention on CLS token...
  2%|▏         | 488/25000 [13:16<10:36:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,499 >> Initializing global attention on CLS token...
  2%|▏         | 489/25000 [13:17<10:35:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,053 >> Initializing global attention on CLS token...
  2%|▏         | 490/25000 [13:19<10:44:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:54,681 >> Initializing global attention on CLS token...
  2%|▏         | 491/25000 [13:20<10:41:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:56,231 >> Initializing global attention on CLS token...
  2%|▏         | 492/25000 [13:22<10:38:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:57,783 >> Initializing global attention on CLS token...
  2%|▏         | 493/25000 [13:23<10:37:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:59,338 >> Initializing global attention on CLS token...
  2%|▏         | 494/25000 [13:25<10:36:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:00,890 >> Initializing global attention on CLS token...
  2%|▏         | 495/25000 [13:27<10:35:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:02,442 >> Initializing global attention on CLS token...
  2%|▏         | 496/25000 [13:28<10:35:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:03,994 >> Initializing global attention on CLS token...
  2%|▏         | 497/25000 [13:30<10:36:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:05,559 >> Initializing global attention on CLS token...
  2%|▏         | 498/25000 [13:31<10:35:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:07,112 >> Initializing global attention on CLS token...
  2%|▏         | 499/25000 [13:33<10:34:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:08,643 >> Initializing global attention on CLS token...
  2%|▏         | 500/25000 [13:34<10:33:38,  1.55s/it]                                                        2%|▏         | 500/25000 [13:34<10:33:38,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:32:10,187 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:32:10,191 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:32:10,191 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:32:10,191 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,204 >> Initializing global attention on CLS token...
{'eval_loss': 4.647832870483398, 'eval_accuracy': 0.28, 'eval_macro_f1': 0.03849383159155621, 'eval_macro_precision': 0.027647006858336747, 'eval_macro_recall': 0.07924164995843318, 'eval_micro_f1': 0.28, 'eval_micro_precision': 0.28, 'eval_micro_recall': 0.28, 'eval_combined_score': 0.18076892691547516, 'eval_runtime': 7.8837, 'eval_samples_per_second': 63.422, 'eval_steps_per_second': 7.991, 'epoch': 3.0}
{'loss': 3.7896, 'learning_rate': 6.859999999999999e-05, 'epoch': 4.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,324 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.74it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,444 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,563 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.53it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,684 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,805 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:10,935 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,054 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.83it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,174 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.72it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,294 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.62it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,414 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,537 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,657 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,778 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,900 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,020 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,139 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,266 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,386 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,506 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,626 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,747 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,866 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,986 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,107 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,230 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,350 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,470 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,590 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,710 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,830 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:13,949 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,069 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,189 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,309 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,431 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,551 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,671 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,791 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,911 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,031 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  7.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,197 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  7.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,326 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  7.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,446 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  7.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,568 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.00it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,689 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,809 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:15,929 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,050 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,170 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,290 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,410 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,532 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,652 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,772 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,891 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,011 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,131 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,252 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,372 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,492 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,609 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,724 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  2%|▏         | 500/25000 [13:42<10:33:38,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.39it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:32:17,809 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-500
[INFO|configuration_utils.py:461] 2024-01-22 00:32:17,815 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-500/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:32:18,399 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:32:18,400 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:32:18,400 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-500/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:32:19,515 >> Initializing global attention on CLS token...
  2%|▏         | 501/25000 [13:45<29:35:04,  4.35s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:21,079 >> Initializing global attention on CLS token...
  2%|▏         | 502/25000 [13:47<23:52:46,  3.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:22,631 >> Initializing global attention on CLS token...
  2%|▏         | 503/25000 [13:48<19:52:28,  2.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:24,180 >> Initializing global attention on CLS token...
  2%|▏         | 504/25000 [13:50<17:04:40,  2.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:25,734 >> Initializing global attention on CLS token...
  2%|▏         | 505/25000 [13:51<15:08:34,  2.23s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:27,293 >> Initializing global attention on CLS token...
  2%|▏         | 506/25000 [13:53<13:46:09,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:28,845 >> Initializing global attention on CLS token...
  2%|▏         | 507/25000 [13:55<12:48:01,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:30,394 >> Initializing global attention on CLS token...
  2%|▏         | 508/25000 [13:56<12:15:02,  1.80s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:32,009 >> Initializing global attention on CLS token...
  2%|▏         | 509/25000 [13:58<11:44:29,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:33,558 >> Initializing global attention on CLS token...
  2%|▏         | 510/25000 [13:59<11:22:42,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:35,106 >> Initializing global attention on CLS token...
  2%|▏         | 511/25000 [14:01<11:08:04,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:36,660 >> Initializing global attention on CLS token...
  2%|▏         | 512/25000 [14:02<11:00:37,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:38,236 >> Initializing global attention on CLS token...
  2%|▏         | 513/25000 [14:04<10:52:04,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:39,787 >> Initializing global attention on CLS token...
  2%|▏         | 514/25000 [14:05<10:46:24,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:41,337 >> Initializing global attention on CLS token...
  2%|▏         | 515/25000 [14:07<10:42:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:42,893 >> Initializing global attention on CLS token...
  2%|▏         | 516/25000 [14:09<10:39:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:44,443 >> Initializing global attention on CLS token...
  2%|▏         | 517/25000 [14:10<10:37:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:46,002 >> Initializing global attention on CLS token...
  2%|▏         | 518/25000 [14:12<10:37:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:47,553 >> Initializing global attention on CLS token...
  2%|▏         | 519/25000 [14:13<10:45:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:49,180 >> Initializing global attention on CLS token...
  2%|▏         | 520/25000 [14:15<10:41:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:50,731 >> Initializing global attention on CLS token...
  2%|▏         | 521/25000 [14:16<10:38:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:52,280 >> Initializing global attention on CLS token...
  2%|▏         | 522/25000 [14:18<10:37:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:53,840 >> Initializing global attention on CLS token...
  2%|▏         | 523/25000 [14:20<10:36:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:55,391 >> Initializing global attention on CLS token...
  2%|▏         | 524/25000 [14:21<10:35:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:56,944 >> Initializing global attention on CLS token...
  2%|▏         | 525/25000 [14:23<10:34:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:58,497 >> Initializing global attention on CLS token...
  2%|▏         | 526/25000 [14:24<10:44:58,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:00,136 >> Initializing global attention on CLS token...
  2%|▏         | 527/25000 [14:26<10:41:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:01,685 >> Initializing global attention on CLS token...
  2%|▏         | 528/25000 [14:27<10:38:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:03,234 >> Initializing global attention on CLS token...
  2%|▏         | 529/25000 [14:29<10:37:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:04,792 >> Initializing global attention on CLS token...
  2%|▏         | 530/25000 [14:30<10:35:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:06,341 >> Initializing global attention on CLS token...
  2%|▏         | 531/25000 [14:32<10:34:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:07,898 >> Initializing global attention on CLS token...
  2%|▏         | 532/25000 [14:34<10:35:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:09,455 >> Initializing global attention on CLS token...
  2%|▏         | 533/25000 [14:35<10:45:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:11,099 >> Initializing global attention on CLS token...
  2%|▏         | 534/25000 [14:37<10:41:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:12,648 >> Initializing global attention on CLS token...
  2%|▏         | 535/25000 [14:38<10:38:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:14,197 >> Initializing global attention on CLS token...
  2%|▏         | 536/25000 [14:40<10:40:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:15,777 >> Initializing global attention on CLS token...
  2%|▏         | 537/25000 [14:41<10:37:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:17,327 >> Initializing global attention on CLS token...
  2%|▏         | 538/25000 [14:43<10:36:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:18,879 >> Initializing global attention on CLS token...
  2%|▏         | 539/25000 [14:45<10:34:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:20,430 >> Initializing global attention on CLS token...
  2%|▏         | 540/25000 [14:46<10:38:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:22,020 >> Initializing global attention on CLS token...
  2%|▏         | 541/25000 [14:48<10:36:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:23,568 >> Initializing global attention on CLS token...
  2%|▏         | 542/25000 [14:49<10:35:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:25,118 >> Initializing global attention on CLS token...
  2%|▏         | 543/25000 [14:51<10:36:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:26,688 >> Initializing global attention on CLS token...
  2%|▏         | 544/25000 [14:52<10:35:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:28,244 >> Initializing global attention on CLS token...
  2%|▏         | 545/25000 [14:54<10:34:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:29,795 >> Initializing global attention on CLS token...
  2%|▏         | 546/25000 [14:55<10:33:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:31,345 >> Initializing global attention on CLS token...
  2%|▏         | 547/25000 [14:57<10:40:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:32,955 >> Initializing global attention on CLS token...
  2%|▏         | 548/25000 [14:59<10:37:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:34,504 >> Initializing global attention on CLS token...
  2%|▏         | 549/25000 [15:00<10:36:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:36,056 >> Initializing global attention on CLS token...
  2%|▏         | 550/25000 [15:02<10:36:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:37,618 >> Initializing global attention on CLS token...
  2%|▏         | 551/25000 [15:03<10:34:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:39,171 >> Initializing global attention on CLS token...
  2%|▏         | 552/25000 [15:05<10:33:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:40,719 >> Initializing global attention on CLS token...
  2%|▏         | 553/25000 [15:06<10:34:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:42,277 >> Initializing global attention on CLS token...
  2%|▏         | 554/25000 [15:08<10:48:15,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:43,949 >> Initializing global attention on CLS token...
  2%|▏         | 555/25000 [15:10<10:43:12,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:45,499 >> Initializing global attention on CLS token...
  2%|▏         | 556/25000 [15:11<10:40:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:47,052 >> Initializing global attention on CLS token...
  2%|▏         | 557/25000 [15:13<10:38:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:48,612 >> Initializing global attention on CLS token...
  2%|▏         | 558/25000 [15:14<10:36:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:50,160 >> Initializing global attention on CLS token...
  2%|▏         | 559/25000 [15:16<10:34:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:51,710 >> Initializing global attention on CLS token...
  2%|▏         | 560/25000 [15:17<10:34:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:53,264 >> Initializing global attention on CLS token...
  2%|▏         | 561/25000 [15:19<10:44:07,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:54,902 >> Initializing global attention on CLS token...
  2%|▏         | 562/25000 [15:21<10:40:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:56,454 >> Initializing global attention on CLS token...
  2%|▏         | 563/25000 [15:22<10:37:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:58,003 >> Initializing global attention on CLS token...
  2%|▏         | 564/25000 [15:24<10:42:19,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:59,607 >> Initializing global attention on CLS token...
  2%|▏         | 565/25000 [15:25<10:38:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:01,157 >> Initializing global attention on CLS token...
  2%|▏         | 566/25000 [15:27<10:36:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:02,705 >> Initializing global attention on CLS token...
  2%|▏         | 567/25000 [15:28<10:34:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:04,263 >> Initializing global attention on CLS token...
  2%|▏         | 568/25000 [15:30<10:37:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:05,834 >> Initializing global attention on CLS token...
  2%|▏         | 569/25000 [15:32<10:35:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:07,385 >> Initializing global attention on CLS token...
  2%|▏         | 570/25000 [15:33<10:34:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:08,937 >> Initializing global attention on CLS token...
  2%|▏         | 571/25000 [15:35<10:35:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:10,503 >> Initializing global attention on CLS token...
  2%|▏         | 572/25000 [15:36<10:34:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:12,059 >> Initializing global attention on CLS token...
  2%|▏         | 573/25000 [15:38<10:34:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:13,612 >> Initializing global attention on CLS token...
  2%|▏         | 574/25000 [15:39<10:33:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:15,166 >> Initializing global attention on CLS token...
  2%|▏         | 575/25000 [15:41<10:37:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:16,753 >> Initializing global attention on CLS token...
  2%|▏         | 576/25000 [15:42<10:35:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:18,305 >> Initializing global attention on CLS token...
  2%|▏         | 577/25000 [15:44<10:34:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:19,857 >> Initializing global attention on CLS token...
  2%|▏         | 578/25000 [15:46<10:43:15,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:21,487 >> Initializing global attention on CLS token...
  2%|▏         | 579/25000 [15:47<10:39:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:23,041 >> Initializing global attention on CLS token...
  2%|▏         | 580/25000 [15:49<10:37:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:24,593 >> Initializing global attention on CLS token...
  2%|▏         | 581/25000 [15:50<10:38:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:26,177 >> Initializing global attention on CLS token...
  2%|▏         | 582/25000 [15:52<10:50:17,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:27,834 >> Initializing global attention on CLS token...
  2%|▏         | 583/25000 [15:54<10:44:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:29,386 >> Initializing global attention on CLS token...
  2%|▏         | 584/25000 [15:55<10:40:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:30,939 >> Initializing global attention on CLS token...
  2%|▏         | 585/25000 [15:57<10:48:40,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:32,579 >> Initializing global attention on CLS token...
  2%|▏         | 586/25000 [15:58<10:43:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:34,131 >> Initializing global attention on CLS token...
  2%|▏         | 587/25000 [16:00<10:40:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:35,687 >> Initializing global attention on CLS token...
  2%|▏         | 588/25000 [16:01<10:44:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:37,295 >> Initializing global attention on CLS token...
  2%|▏         | 589/25000 [16:03<10:40:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:38,849 >> Initializing global attention on CLS token...
  2%|▏         | 590/25000 [16:05<10:38:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:40,402 >> Initializing global attention on CLS token...
  2%|▏         | 591/25000 [16:06<10:35:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:41,952 >> Initializing global attention on CLS token...
  2%|▏         | 592/25000 [16:08<10:35:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:43,514 >> Initializing global attention on CLS token...
  2%|▏         | 593/25000 [16:09<10:34:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:45,065 >> Initializing global attention on CLS token...
  2%|▏         | 594/25000 [16:11<10:33:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:46,615 >> Initializing global attention on CLS token...
  2%|▏         | 595/25000 [16:12<10:32:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:48,165 >> Initializing global attention on CLS token...
  2%|▏         | 596/25000 [16:14<10:31:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:49,714 >> Initializing global attention on CLS token...
  2%|▏         | 597/25000 [16:15<10:31:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:51,263 >> Initializing global attention on CLS token...
  2%|▏         | 598/25000 [16:17<10:31:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:52,821 >> Initializing global attention on CLS token...
  2%|▏         | 599/25000 [16:19<10:33:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:54,391 >> Initializing global attention on CLS token...
  2%|▏         | 600/25000 [16:20<10:32:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:55,942 >> Initializing global attention on CLS token...
  2%|▏         | 601/25000 [16:22<10:32:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:57,494 >> Initializing global attention on CLS token...
  2%|▏         | 602/25000 [16:23<10:36:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:59,077 >> Initializing global attention on CLS token...
  2%|▏         | 603/25000 [16:25<10:34:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:00,629 >> Initializing global attention on CLS token...
  2%|▏         | 604/25000 [16:26<10:33:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:02,181 >> Initializing global attention on CLS token...
  2%|▏         | 605/25000 [16:28<10:33:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:03,739 >> Initializing global attention on CLS token...
  2%|▏         | 606/25000 [16:29<10:33:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:05,302 >> Initializing global attention on CLS token...
  2%|▏         | 607/25000 [16:31<10:33:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:06,854 >> Initializing global attention on CLS token...
  2%|▏         | 608/25000 [16:33<10:31:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:08,402 >> Initializing global attention on CLS token...
  2%|▏         | 609/25000 [16:34<10:44:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:10,057 >> Initializing global attention on CLS token...
  2%|▏         | 610/25000 [16:36<10:40:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,611 >> Initializing global attention on CLS token...
  2%|▏         | 611/25000 [16:37<10:37:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,162 >> Initializing global attention on CLS token...
  2%|▏         | 612/25000 [16:39<10:35:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,718 >> Initializing global attention on CLS token...
  2%|▏         | 613/25000 [16:40<10:35:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,280 >> Initializing global attention on CLS token...
  2%|▏         | 614/25000 [16:42<10:34:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,834 >> Initializing global attention on CLS token...
  2%|▏         | 615/25000 [16:44<10:33:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:19,384 >> Initializing global attention on CLS token...
  2%|▏         | 616/25000 [16:45<10:34:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:20,952 >> Initializing global attention on CLS token...
  2%|▏         | 617/25000 [16:47<10:32:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:22,503 >> Initializing global attention on CLS token...
  2%|▏         | 618/25000 [16:48<10:31:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:24,053 >> Initializing global attention on CLS token...
  2%|▏         | 619/25000 [16:50<10:31:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:25,605 >> Initializing global attention on CLS token...
  2%|▏         | 620/25000 [16:51<10:31:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:27,162 >> Initializing global attention on CLS token...
  2%|▏         | 621/25000 [16:53<10:31:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:28,713 >> Initializing global attention on CLS token...
  2%|▏         | 622/25000 [16:54<10:31:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:30,265 >> Initializing global attention on CLS token...
  2%|▏         | 623/25000 [16:56<10:43:14,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:31,924 >> Initializing global attention on CLS token...
  2%|▏         | 624/25000 [16:58<10:40:04,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:33,455 >> Initializing global attention on CLS token...
  2%|▎         | 625/25000 [16:59<10:34:51,  1.56s/it]                                                        2%|▎         | 625/25000 [16:59<10:34:51,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:35:34,991 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:35:34,994 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:35:34,994 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:35:34,994 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,007 >> Initializing global attention on CLS token...
{'eval_loss': 4.393170356750488, 'eval_accuracy': 0.298, 'eval_macro_f1': 0.050860335566403105, 'eval_macro_precision': 0.042955210808570815, 'eval_macro_recall': 0.08649198734522967, 'eval_micro_f1': 0.298, 'eval_micro_precision': 0.298, 'eval_micro_recall': 0.298, 'eval_combined_score': 0.19604393338860054, 'eval_runtime': 7.6158, 'eval_samples_per_second': 65.653, 'eval_steps_per_second': 8.272, 'epoch': 4.0}
{'loss': 3.4134, 'learning_rate': 6.824999999999999e-05, 'epoch': 5.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,129 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.58it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,249 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,377 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,502 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,621 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,741 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,861 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,981 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,102 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.61it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,222 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,341 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,461 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,582 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,702 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,822 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,944 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,065 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,186 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:07,  6.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,456 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:06,  6.60it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,576 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  7.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,696 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  7.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,818 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:05,  7.61it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:37,938 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  7.80it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,059 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  7.96it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,179 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,299 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,419 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,538 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,690 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  7.67it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,810 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:04,  7.85it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,930 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  7.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,051 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:03,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,172 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,295 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,421 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,547 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,666 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,786 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:39,907 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,027 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,147 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,266 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,386 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,507 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,627 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,748 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,868 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,988 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,107 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,227 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,347 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,468 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,588 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,708 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,828 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,948 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:42,068 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:42,187 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:42,307 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:42,428 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:42,545 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:42,659 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  2%|▎         | 625/25000 [17:07<10:34:51,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.39it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:35:42,809 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-625
[INFO|configuration_utils.py:461] 2024-01-22 00:35:42,822 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-625/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:35:43,456 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-625/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:35:43,458 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-625/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:35:43,458 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-625/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:44,573 >> Initializing global attention on CLS token...
  3%|▎         | 626/25000 [17:10<30:11:13,  4.46s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:46,224 >> Initializing global attention on CLS token...
  3%|▎         | 627/25000 [17:12<24:16:17,  3.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:47,772 >> Initializing global attention on CLS token...
  3%|▎         | 628/25000 [17:13<20:09:26,  2.98s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:49,330 >> Initializing global attention on CLS token...
  3%|▎         | 629/25000 [17:15<17:16:07,  2.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:50,886 >> Initializing global attention on CLS token...
  3%|▎         | 630/25000 [17:17<15:18:04,  2.26s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:52,467 >> Initializing global attention on CLS token...
  3%|▎         | 631/25000 [17:18<14:03:23,  2.08s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:54,125 >> Initializing global attention on CLS token...
  3%|▎         | 632/25000 [17:20<13:00:10,  1.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:55,674 >> Initializing global attention on CLS token...
  3%|▎         | 633/25000 [17:21<12:20:02,  1.82s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:57,266 >> Initializing global attention on CLS token...
  3%|▎         | 634/25000 [17:23<11:46:53,  1.74s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:58,816 >> Initializing global attention on CLS token...
  3%|▎         | 635/25000 [17:25<11:25:23,  1.69s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:00,381 >> Initializing global attention on CLS token...
  3%|▎         | 636/25000 [17:26<11:08:27,  1.65s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:01,930 >> Initializing global attention on CLS token...
  3%|▎         | 637/25000 [17:28<10:56:50,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:03,481 >> Initializing global attention on CLS token...
  3%|▎         | 638/25000 [17:29<10:49:02,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:05,049 >> Initializing global attention on CLS token...
  3%|▎         | 639/25000 [17:31<10:48:06,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:06,626 >> Initializing global attention on CLS token...
  3%|▎         | 640/25000 [17:32<10:42:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:08,177 >> Initializing global attention on CLS token...
  3%|▎         | 641/25000 [17:34<10:38:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:09,725 >> Initializing global attention on CLS token...
  3%|▎         | 642/25000 [17:35<10:37:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:11,288 >> Initializing global attention on CLS token...
  3%|▎         | 643/25000 [17:37<10:34:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:12,835 >> Initializing global attention on CLS token...
  3%|▎         | 644/25000 [17:39<10:32:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:14,383 >> Initializing global attention on CLS token...
  3%|▎         | 645/25000 [17:40<10:31:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:15,933 >> Initializing global attention on CLS token...
  3%|▎         | 646/25000 [17:42<10:42:51,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:17,582 >> Initializing global attention on CLS token...
  3%|▎         | 647/25000 [17:43<10:38:30,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:19,131 >> Initializing global attention on CLS token...
  3%|▎         | 648/25000 [17:45<10:35:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:20,683 >> Initializing global attention on CLS token...
  3%|▎         | 649/25000 [17:46<10:35:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:22,249 >> Initializing global attention on CLS token...
  3%|▎         | 650/25000 [17:48<10:34:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:23,803 >> Initializing global attention on CLS token...
  3%|▎         | 651/25000 [17:50<10:32:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:25,350 >> Initializing global attention on CLS token...
  3%|▎         | 652/25000 [17:51<10:35:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:26,950 >> Initializing global attention on CLS token...
  3%|▎         | 653/25000 [17:53<10:37:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:28,514 >> Initializing global attention on CLS token...
  3%|▎         | 654/25000 [17:54<10:34:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:30,062 >> Initializing global attention on CLS token...
  3%|▎         | 655/25000 [17:56<10:33:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:31,614 >> Initializing global attention on CLS token...
  3%|▎         | 656/25000 [17:57<10:36:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:33,207 >> Initializing global attention on CLS token...
  3%|▎         | 657/25000 [17:59<10:34:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:34,756 >> Initializing global attention on CLS token...
  3%|▎         | 658/25000 [18:00<10:33:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:36,309 >> Initializing global attention on CLS token...
  3%|▎         | 659/25000 [18:02<10:31:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:37,860 >> Initializing global attention on CLS token...
  3%|▎         | 660/25000 [18:04<10:35:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:39,451 >> Initializing global attention on CLS token...
  3%|▎         | 661/25000 [18:05<10:33:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:41,002 >> Initializing global attention on CLS token...
  3%|▎         | 662/25000 [18:07<10:32:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:42,554 >> Initializing global attention on CLS token...
  3%|▎         | 663/25000 [18:08<10:40:04,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:44,175 >> Initializing global attention on CLS token...
  3%|▎         | 664/25000 [18:10<10:36:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:45,727 >> Initializing global attention on CLS token...
  3%|▎         | 665/25000 [18:11<10:34:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:47,277 >> Initializing global attention on CLS token...
  3%|▎         | 666/25000 [18:13<10:32:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:48,829 >> Initializing global attention on CLS token...
  3%|▎         | 667/25000 [18:15<10:32:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:50,391 >> Initializing global attention on CLS token...
  3%|▎         | 668/25000 [18:16<10:31:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:51,941 >> Initializing global attention on CLS token...
  3%|▎         | 669/25000 [18:18<10:30:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:53,492 >> Initializing global attention on CLS token...
  3%|▎         | 670/25000 [18:19<10:30:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:55,044 >> Initializing global attention on CLS token...
  3%|▎         | 671/25000 [18:21<10:42:38,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:56,700 >> Initializing global attention on CLS token...
  3%|▎         | 672/25000 [18:22<10:38:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:58,250 >> Initializing global attention on CLS token...
  3%|▎         | 673/25000 [18:24<10:35:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:59,799 >> Initializing global attention on CLS token...
  3%|▎         | 674/25000 [18:26<10:40:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:01,410 >> Initializing global attention on CLS token...
  3%|▎         | 675/25000 [18:27<10:37:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:02,963 >> Initializing global attention on CLS token...
  3%|▎         | 676/25000 [18:29<10:35:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:04,521 >> Initializing global attention on CLS token...
  3%|▎         | 677/25000 [18:30<10:33:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:06,071 >> Initializing global attention on CLS token...
  3%|▎         | 678/25000 [18:32<10:47:22,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:07,749 >> Initializing global attention on CLS token...
  3%|▎         | 679/25000 [18:33<10:42:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:09,302 >> Initializing global attention on CLS token...
  3%|▎         | 680/25000 [18:35<10:39:21,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:10,863 >> Initializing global attention on CLS token...
  3%|▎         | 681/25000 [18:37<10:44:18,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:12,482 >> Initializing global attention on CLS token...
  3%|▎         | 682/25000 [18:38<10:40:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:14,040 >> Initializing global attention on CLS token...
  3%|▎         | 683/25000 [18:40<10:36:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:15,591 >> Initializing global attention on CLS token...
  3%|▎         | 684/25000 [18:41<10:49:51,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:17,269 >> Initializing global attention on CLS token...
  3%|▎         | 685/25000 [18:43<10:43:05,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:18,818 >> Initializing global attention on CLS token...
  3%|▎         | 686/25000 [18:45<10:40:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:20,380 >> Initializing global attention on CLS token...
  3%|▎         | 687/25000 [18:46<10:36:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:21,931 >> Initializing global attention on CLS token...
  3%|▎         | 688/25000 [18:48<10:39:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:23,529 >> Initializing global attention on CLS token...
  3%|▎         | 689/25000 [18:49<10:36:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:25,080 >> Initializing global attention on CLS token...
  3%|▎         | 690/25000 [18:51<10:33:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:26,631 >> Initializing global attention on CLS token...
  3%|▎         | 691/25000 [18:52<10:35:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:28,209 >> Initializing global attention on CLS token...
  3%|▎         | 692/25000 [18:54<10:33:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:29,762 >> Initializing global attention on CLS token...
  3%|▎         | 693/25000 [18:55<10:32:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:31,314 >> Initializing global attention on CLS token...
  3%|▎         | 694/25000 [18:57<10:31:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:32,871 >> Initializing global attention on CLS token...
  3%|▎         | 695/25000 [18:59<10:35:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:34,461 >> Initializing global attention on CLS token...
  3%|▎         | 696/25000 [19:00<10:33:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:36,015 >> Initializing global attention on CLS token...
  3%|▎         | 697/25000 [19:02<10:31:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:37,565 >> Initializing global attention on CLS token...
  3%|▎         | 698/25000 [19:03<10:43:47,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:39,222 >> Initializing global attention on CLS token...
  3%|▎         | 699/25000 [19:05<10:38:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:40,772 >> Initializing global attention on CLS token...
  3%|▎         | 700/25000 [19:07<10:41:52,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:42,375 >> Initializing global attention on CLS token...
  3%|▎         | 701/25000 [19:08<10:37:51,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:43,927 >> Initializing global attention on CLS token...
  3%|▎         | 702/25000 [19:10<10:43:31,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:45,549 >> Initializing global attention on CLS token...
  3%|▎         | 703/25000 [19:11<10:38:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:47,098 >> Initializing global attention on CLS token...
  3%|▎         | 704/25000 [19:13<10:41:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:48,696 >> Initializing global attention on CLS token...
  3%|▎         | 705/25000 [19:14<10:38:12,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:50,259 >> Initializing global attention on CLS token...
  3%|▎         | 706/25000 [19:16<10:38:07,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:51,832 >> Initializing global attention on CLS token...
  3%|▎         | 707/25000 [19:18<10:35:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:53,390 >> Initializing global attention on CLS token...
  3%|▎         | 708/25000 [19:19<10:33:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:54,941 >> Initializing global attention on CLS token...
  3%|▎         | 709/25000 [19:21<10:34:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:56,512 >> Initializing global attention on CLS token...
  3%|▎         | 710/25000 [19:22<10:32:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:58,066 >> Initializing global attention on CLS token...
  3%|▎         | 711/25000 [19:24<10:31:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:59,623 >> Initializing global attention on CLS token...
  3%|▎         | 712/25000 [19:25<10:30:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:01,175 >> Initializing global attention on CLS token...
  3%|▎         | 713/25000 [19:27<10:34:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:02,766 >> Initializing global attention on CLS token...
  3%|▎         | 714/25000 [19:28<10:35:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:04,339 >> Initializing global attention on CLS token...
  3%|▎         | 715/25000 [19:30<10:32:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:05,890 >> Initializing global attention on CLS token...
  3%|▎         | 716/25000 [19:32<10:31:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:07,444 >> Initializing global attention on CLS token...
  3%|▎         | 717/25000 [19:33<10:30:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:08,992 >> Initializing global attention on CLS token...
  3%|▎         | 718/25000 [19:35<10:30:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:10,551 >> Initializing global attention on CLS token...
  3%|▎         | 719/25000 [19:36<10:29:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:12,101 >> Initializing global attention on CLS token...
  3%|▎         | 720/25000 [19:38<10:32:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:13,680 >> Initializing global attention on CLS token...
  3%|▎         | 721/25000 [19:39<10:31:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:15,232 >> Initializing global attention on CLS token...
  3%|▎         | 722/25000 [19:41<10:41:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:16,876 >> Initializing global attention on CLS token...
  3%|▎         | 723/25000 [19:43<10:38:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:18,434 >> Initializing global attention on CLS token...
  3%|▎         | 724/25000 [19:44<10:34:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:19,986 >> Initializing global attention on CLS token...
  3%|▎         | 725/25000 [19:46<10:41:46,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:21,612 >> Initializing global attention on CLS token...
  3%|▎         | 726/25000 [19:47<10:37:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:23,164 >> Initializing global attention on CLS token...
  3%|▎         | 727/25000 [19:49<10:40:30,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:24,764 >> Initializing global attention on CLS token...
  3%|▎         | 728/25000 [19:50<10:36:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:26,315 >> Initializing global attention on CLS token...
  3%|▎         | 729/25000 [19:52<10:44:28,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:27,954 >> Initializing global attention on CLS token...
  3%|▎         | 730/25000 [19:54<10:40:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:29,511 >> Initializing global attention on CLS token...
  3%|▎         | 731/25000 [19:55<10:36:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:31,064 >> Initializing global attention on CLS token...
  3%|▎         | 732/25000 [19:57<10:35:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:32,630 >> Initializing global attention on CLS token...
  3%|▎         | 733/25000 [19:58<10:33:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:34,186 >> Initializing global attention on CLS token...
  3%|▎         | 734/25000 [20:00<10:34:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,761 >> Initializing global attention on CLS token...
  3%|▎         | 735/25000 [20:01<10:33:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,319 >> Initializing global attention on CLS token...
  3%|▎         | 736/25000 [20:03<10:36:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,915 >> Initializing global attention on CLS token...
  3%|▎         | 737/25000 [20:05<10:34:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,470 >> Initializing global attention on CLS token...
  3%|▎         | 738/25000 [20:06<10:48:21,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:42,153 >> Initializing global attention on CLS token...
  3%|▎         | 739/25000 [20:08<10:51:11,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:43,781 >> Initializing global attention on CLS token...
  3%|▎         | 740/25000 [20:09<10:44:00,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:45,352 >> Initializing global attention on CLS token...
  3%|▎         | 741/25000 [20:11<10:42:30,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:46,913 >> Initializing global attention on CLS token...
  3%|▎         | 742/25000 [20:13<10:37:43,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:48,462 >> Initializing global attention on CLS token...
  3%|▎         | 743/25000 [20:14<10:34:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:50,012 >> Initializing global attention on CLS token...
  3%|▎         | 744/25000 [20:16<10:31:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:51,569 >> Initializing global attention on CLS token...
  3%|▎         | 745/25000 [20:17<10:35:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:53,151 >> Initializing global attention on CLS token...
  3%|▎         | 746/25000 [20:19<10:32:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:54,700 >> Initializing global attention on CLS token...
  3%|▎         | 747/25000 [20:20<10:30:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:56,253 >> Initializing global attention on CLS token...
  3%|▎         | 748/25000 [20:22<10:36:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:57,860 >> Initializing global attention on CLS token...
  3%|▎         | 749/25000 [20:24<10:33:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:59,390 >> Initializing global attention on CLS token...
  3%|▎         | 750/25000 [20:25<10:29:11,  1.56s/it]                                                        3%|▎         | 750/25000 [20:25<10:29:11,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:39:00,921 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:39:00,925 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:39:00,926 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:39:00,926 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:00,950 >> Initializing global attention on CLS token...
{'eval_loss': 4.296175479888916, 'eval_accuracy': 0.316, 'eval_macro_f1': 0.07230917568492226, 'eval_macro_precision': 0.06379738444754733, 'eval_macro_recall': 0.111689249063831, 'eval_micro_f1': 0.316, 'eval_micro_precision': 0.316, 'eval_micro_recall': 0.316, 'eval_combined_score': 0.21597082988518584, 'eval_runtime': 7.8073, 'eval_samples_per_second': 64.043, 'eval_steps_per_second': 8.069, 'epoch': 5.0}
{'loss': 3.0636, 'learning_rate': 6.79e-05, 'epoch': 6.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,072 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,192 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,315 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,444 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,563 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,684 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,803 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.85it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,924 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.73it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,043 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,163 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,283 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,409 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,529 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,649 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,771 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:02,891 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,012 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,132 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,252 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,371 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,490 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:07,  5.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,783 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:06,  6.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,904 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:05,  6.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,024 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:05,  7.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,146 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  7.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,265 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  7.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,386 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  7.85it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,512 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.89it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,635 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  7.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,757 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,878 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,000 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:03,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,121 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,244 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.03it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,372 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,493 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  7.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,623 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,743 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,863 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:05,984 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:05<00:02,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,104 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,224 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,346 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,467 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,588 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,708 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,841 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,960 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:06<00:01,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,081 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,201 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,323 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,443 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,563 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,689 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,809 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,929 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:08,050 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:08,172 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:08,292 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:08,412 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:08,530 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:08,647 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  3%|▎         | 750/25000 [20:33<10:29:11,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.31it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:39:08,734 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-750
[INFO|configuration_utils.py:461] 2024-01-22 00:39:08,740 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-750/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:39:09,560 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:39:09,562 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:39:09,562 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-750/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:39:10,719 >> Initializing global attention on CLS token...
  3%|▎         | 751/25000 [20:36<30:20:28,  4.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:12,324 >> Initializing global attention on CLS token...
  3%|▎         | 752/25000 [20:38<24:23:03,  3.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:13,881 >> Initializing global attention on CLS token...
  3%|▎         | 753/25000 [20:40<20:14:24,  3.01s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:15,451 >> Initializing global attention on CLS token...
  3%|▎         | 754/25000 [20:41<17:17:38,  2.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:16,998 >> Initializing global attention on CLS token...
  3%|▎         | 755/25000 [20:43<15:13:53,  2.26s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:18,546 >> Initializing global attention on CLS token...
  3%|▎         | 756/25000 [20:44<13:47:23,  2.05s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:20,094 >> Initializing global attention on CLS token...
  3%|▎         | 757/25000 [20:46<12:49:10,  1.90s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:21,662 >> Initializing global attention on CLS token...
  3%|▎         | 758/25000 [20:47<12:06:53,  1.80s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:23,217 >> Initializing global attention on CLS token...
  3%|▎         | 759/25000 [20:49<11:36:44,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:24,767 >> Initializing global attention on CLS token...
  3%|▎         | 760/25000 [20:50<11:15:37,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:26,318 >> Initializing global attention on CLS token...
  3%|▎         | 761/25000 [20:52<11:01:45,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:27,876 >> Initializing global attention on CLS token...
  3%|▎         | 762/25000 [20:54<10:51:02,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:29,426 >> Initializing global attention on CLS token...
  3%|▎         | 763/25000 [20:55<10:43:56,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:30,979 >> Initializing global attention on CLS token...
  3%|▎         | 764/25000 [20:57<10:38:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:32,529 >> Initializing global attention on CLS token...
  3%|▎         | 765/25000 [20:58<10:34:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:34,080 >> Initializing global attention on CLS token...
  3%|▎         | 766/25000 [21:00<10:32:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:35,629 >> Initializing global attention on CLS token...
  3%|▎         | 767/25000 [21:01<10:30:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:37,179 >> Initializing global attention on CLS token...
  3%|▎         | 768/25000 [21:03<10:31:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:38,754 >> Initializing global attention on CLS token...
  3%|▎         | 769/25000 [21:04<10:30:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:40,305 >> Initializing global attention on CLS token...
  3%|▎         | 770/25000 [21:06<10:29:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:41,856 >> Initializing global attention on CLS token...
  3%|▎         | 771/25000 [21:08<10:28:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:43,405 >> Initializing global attention on CLS token...
  3%|▎         | 772/25000 [21:09<10:33:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:45,004 >> Initializing global attention on CLS token...
  3%|▎         | 773/25000 [21:11<10:30:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:46,554 >> Initializing global attention on CLS token...
  3%|▎         | 774/25000 [21:12<10:29:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:48,106 >> Initializing global attention on CLS token...
  3%|▎         | 775/25000 [21:14<10:33:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:49,691 >> Initializing global attention on CLS token...
  3%|▎         | 776/25000 [21:15<10:30:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:51,240 >> Initializing global attention on CLS token...
  3%|▎         | 777/25000 [21:17<10:28:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:52,787 >> Initializing global attention on CLS token...
  3%|▎         | 778/25000 [21:18<10:27:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:54,335 >> Initializing global attention on CLS token...
  3%|▎         | 779/25000 [21:20<10:28:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:55,896 >> Initializing global attention on CLS token...
  3%|▎         | 780/25000 [21:22<10:27:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:57,445 >> Initializing global attention on CLS token...
  3%|▎         | 781/25000 [21:23<10:27:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:59,004 >> Initializing global attention on CLS token...
  3%|▎         | 782/25000 [21:25<10:27:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:00,557 >> Initializing global attention on CLS token...
  3%|▎         | 783/25000 [21:26<10:33:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:02,165 >> Initializing global attention on CLS token...
  3%|▎         | 784/25000 [21:28<10:34:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:03,737 >> Initializing global attention on CLS token...
  3%|▎         | 785/25000 [21:29<10:31:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:05,289 >> Initializing global attention on CLS token...
  3%|▎         | 786/25000 [21:31<10:34:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:06,880 >> Initializing global attention on CLS token...
  3%|▎         | 787/25000 [21:33<10:31:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:08,429 >> Initializing global attention on CLS token...
  3%|▎         | 788/25000 [21:34<10:36:56,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:10,037 >> Initializing global attention on CLS token...
  3%|▎         | 789/25000 [21:36<10:33:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:11,588 >> Initializing global attention on CLS token...
  3%|▎         | 790/25000 [21:37<10:39:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:13,206 >> Initializing global attention on CLS token...
  3%|▎         | 791/25000 [21:39<10:35:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:14,755 >> Initializing global attention on CLS token...
  3%|▎         | 792/25000 [21:40<10:31:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:16,306 >> Initializing global attention on CLS token...
  3%|▎         | 793/25000 [21:42<10:36:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:17,912 >> Initializing global attention on CLS token...
  3%|▎         | 794/25000 [21:44<10:33:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:19,462 >> Initializing global attention on CLS token...
  3%|▎         | 795/25000 [21:45<10:31:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:21,012 >> Initializing global attention on CLS token...
  3%|▎         | 796/25000 [21:47<10:29:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:22,565 >> Initializing global attention on CLS token...
  3%|▎         | 797/25000 [21:48<10:38:58,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:24,203 >> Initializing global attention on CLS token...
  3%|▎         | 798/25000 [21:50<10:34:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:25,753 >> Initializing global attention on CLS token...
  3%|▎         | 799/25000 [21:51<10:31:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:27,303 >> Initializing global attention on CLS token...
  3%|▎         | 800/25000 [21:53<10:30:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:28,858 >> Initializing global attention on CLS token...
  3%|▎         | 801/25000 [21:55<10:28:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:30,408 >> Initializing global attention on CLS token...
  3%|▎         | 802/25000 [21:56<10:28:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:31,962 >> Initializing global attention on CLS token...
  3%|▎         | 803/25000 [21:58<10:27:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:33,515 >> Initializing global attention on CLS token...
  3%|▎         | 804/25000 [21:59<10:38:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:35,165 >> Initializing global attention on CLS token...
  3%|▎         | 805/25000 [22:01<10:35:20,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:36,719 >> Initializing global attention on CLS token...
  3%|▎         | 806/25000 [22:02<10:32:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:38,269 >> Initializing global attention on CLS token...
  3%|▎         | 807/25000 [22:04<10:31:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:39,831 >> Initializing global attention on CLS token...
  3%|▎         | 808/25000 [22:06<10:29:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:41,384 >> Initializing global attention on CLS token...
  3%|▎         | 809/25000 [22:07<10:28:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:42,936 >> Initializing global attention on CLS token...
  3%|▎         | 810/25000 [22:09<10:27:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:44,486 >> Initializing global attention on CLS token...
  3%|▎         | 811/25000 [22:10<10:41:07,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:46,155 >> Initializing global attention on CLS token...
  3%|▎         | 812/25000 [22:12<10:36:36,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:47,709 >> Initializing global attention on CLS token...
  3%|▎         | 813/25000 [22:13<10:33:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:49,260 >> Initializing global attention on CLS token...
  3%|▎         | 814/25000 [22:15<10:40:15,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:50,889 >> Initializing global attention on CLS token...
  3%|▎         | 815/25000 [22:17<10:35:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:52,439 >> Initializing global attention on CLS token...
  3%|▎         | 816/25000 [22:18<10:32:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:53,992 >> Initializing global attention on CLS token...
  3%|▎         | 817/25000 [22:20<10:30:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:55,544 >> Initializing global attention on CLS token...
  3%|▎         | 818/25000 [22:21<10:31:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:57,119 >> Initializing global attention on CLS token...
  3%|▎         | 819/25000 [22:23<10:29:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:58,669 >> Initializing global attention on CLS token...
  3%|▎         | 820/25000 [22:24<10:28:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:00,223 >> Initializing global attention on CLS token...
  3%|▎         | 821/25000 [22:26<10:30:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:01,795 >> Initializing global attention on CLS token...
  3%|▎         | 822/25000 [22:27<10:28:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:03,345 >> Initializing global attention on CLS token...
  3%|▎         | 823/25000 [22:29<10:27:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:04,899 >> Initializing global attention on CLS token...
  3%|▎         | 824/25000 [22:31<10:26:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:06,449 >> Initializing global attention on CLS token...
  3%|▎         | 825/25000 [22:32<10:34:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:08,069 >> Initializing global attention on CLS token...
  3%|▎         | 826/25000 [22:34<10:31:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:09,622 >> Initializing global attention on CLS token...
  3%|▎         | 827/25000 [22:35<10:29:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:11,172 >> Initializing global attention on CLS token...
  3%|▎         | 828/25000 [22:37<10:28:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:12,730 >> Initializing global attention on CLS token...
  3%|▎         | 829/25000 [22:38<10:27:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:14,281 >> Initializing global attention on CLS token...
  3%|▎         | 830/25000 [22:40<10:26:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:15,832 >> Initializing global attention on CLS token...
  3%|▎         | 831/25000 [22:42<10:26:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:17,417 >> Initializing global attention on CLS token...
  3%|▎         | 832/25000 [22:43<10:42:37,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:19,076 >> Initializing global attention on CLS token...
  3%|▎         | 833/25000 [22:45<10:37:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:20,626 >> Initializing global attention on CLS token...
  3%|▎         | 834/25000 [22:46<10:33:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:22,180 >> Initializing global attention on CLS token...
  3%|▎         | 835/25000 [22:48<10:35:16,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:23,766 >> Initializing global attention on CLS token...
  3%|▎         | 836/25000 [22:49<10:32:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:25,322 >> Initializing global attention on CLS token...
  3%|▎         | 837/25000 [22:51<10:30:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:26,873 >> Initializing global attention on CLS token...
  3%|▎         | 838/25000 [22:53<10:28:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:28,542 >> Initializing global attention on CLS token...
  3%|▎         | 839/25000 [22:54<10:41:48,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:30,095 >> Initializing global attention on CLS token...
  3%|▎         | 840/25000 [22:56<10:36:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:31,645 >> Initializing global attention on CLS token...
  3%|▎         | 841/25000 [22:57<10:32:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:33,199 >> Initializing global attention on CLS token...
  3%|▎         | 842/25000 [22:59<10:36:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:34,795 >> Initializing global attention on CLS token...
  3%|▎         | 843/25000 [23:00<10:32:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:36,345 >> Initializing global attention on CLS token...
  3%|▎         | 844/25000 [23:02<10:30:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:37,897 >> Initializing global attention on CLS token...
  3%|▎         | 845/25000 [23:04<10:31:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:39,473 >> Initializing global attention on CLS token...
  3%|▎         | 846/25000 [23:05<10:29:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:41,028 >> Initializing global attention on CLS token...
  3%|▎         | 847/25000 [23:07<10:28:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:42,585 >> Initializing global attention on CLS token...
  3%|▎         | 848/25000 [23:08<10:28:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:44,141 >> Initializing global attention on CLS token...
  3%|▎         | 849/25000 [23:10<10:29:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:45,715 >> Initializing global attention on CLS token...
  3%|▎         | 850/25000 [23:11<10:28:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:47,266 >> Initializing global attention on CLS token...
  3%|▎         | 851/25000 [23:13<10:26:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:48,815 >> Initializing global attention on CLS token...
  3%|▎         | 852/25000 [23:15<10:28:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:50,386 >> Initializing global attention on CLS token...
  3%|▎         | 853/25000 [23:16<10:27:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:51,937 >> Initializing global attention on CLS token...
  3%|▎         | 854/25000 [23:18<10:26:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:53,492 >> Initializing global attention on CLS token...
  3%|▎         | 855/25000 [23:19<10:26:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:55,045 >> Initializing global attention on CLS token...
  3%|▎         | 856/25000 [23:21<10:33:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:56,660 >> Initializing global attention on CLS token...
  3%|▎         | 857/25000 [23:22<10:30:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:58,213 >> Initializing global attention on CLS token...
  3%|▎         | 858/25000 [23:24<10:33:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,799 >> Initializing global attention on CLS token...
  3%|▎         | 859/25000 [23:26<10:32:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,378 >> Initializing global attention on CLS token...
  3%|▎         | 860/25000 [23:27<10:32:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,937 >> Initializing global attention on CLS token...
  3%|▎         | 861/25000 [23:29<10:31:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,504 >> Initializing global attention on CLS token...
  3%|▎         | 862/25000 [23:30<10:29:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:06,056 >> Initializing global attention on CLS token...
  3%|▎         | 863/25000 [23:32<10:28:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:07,615 >> Initializing global attention on CLS token...
  3%|▎         | 864/25000 [23:33<10:27:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:09,165 >> Initializing global attention on CLS token...
  3%|▎         | 865/25000 [23:35<10:26:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:10,727 >> Initializing global attention on CLS token...
  3%|▎         | 866/25000 [23:36<10:26:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:12,281 >> Initializing global attention on CLS token...
  3%|▎         | 867/25000 [23:38<10:34:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:13,902 >> Initializing global attention on CLS token...
  3%|▎         | 868/25000 [23:40<10:32:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:15,462 >> Initializing global attention on CLS token...
  3%|▎         | 869/25000 [23:41<10:29:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:17,014 >> Initializing global attention on CLS token...
  3%|▎         | 870/25000 [23:43<10:30:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:18,585 >> Initializing global attention on CLS token...
  3%|▎         | 871/25000 [23:44<10:28:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:20,136 >> Initializing global attention on CLS token...
  3%|▎         | 872/25000 [23:46<10:29:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:21,709 >> Initializing global attention on CLS token...
  3%|▎         | 873/25000 [23:47<10:27:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:23,258 >> Initializing global attention on CLS token...
  3%|▎         | 874/25000 [23:49<10:27:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:24,801 >> Initializing global attention on CLS token...
  4%|▎         | 875/25000 [23:51<10:24:37,  1.55s/it]                                                        4%|▎         | 875/25000 [23:51<10:24:37,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:42:26,337 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:42:26,340 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:42:26,341 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:42:26,341 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,353 >> Initializing global attention on CLS token...
{'eval_loss': 4.20540714263916, 'eval_accuracy': 0.298, 'eval_macro_f1': 0.06937428095060855, 'eval_macro_precision': 0.058200994053791054, 'eval_macro_recall': 0.11076736444383503, 'eval_micro_f1': 0.298, 'eval_micro_precision': 0.298, 'eval_micro_recall': 0.298, 'eval_combined_score': 0.20433466277831921, 'eval_runtime': 7.8049, 'eval_samples_per_second': 64.062, 'eval_steps_per_second': 8.072, 'epoch': 6.0}
{'loss': 2.7724, 'learning_rate': 6.754999999999999e-05, 'epoch': 7.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,473 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,594 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,713 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,841 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,961 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,081 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,201 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,322 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.74it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,442 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,562 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,683 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,804 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:27,924 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,043 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,164 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,284 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,404 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,525 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,645 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,765 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,885 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,006 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,126 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,246 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:06,  5.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,539 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:05,  6.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,659 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:05,  6.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,779 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  7.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,899 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.53it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,020 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  7.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,139 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:04,  7.92it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,259 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.00it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,382 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:03,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,502 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,621 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,742 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,864 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:30,984 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,104 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,229 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,350 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,470 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,590 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,711 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,831 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,951 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,070 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,189 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,308 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,428 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,550 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,671 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,791 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,910 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,034 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,154 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,274 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,394 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,514 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,634 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,755 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,872 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:33,989 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  4%|▎         | 875/25000 [23:58<10:24:37,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.34it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:42:34,073 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-875
[INFO|configuration_utils.py:461] 2024-01-22 00:42:34,080 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-875/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:42:34,642 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-875/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:42:34,649 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-875/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:42:34,649 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-875/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:35,938 >> Initializing global attention on CLS token...
  4%|▎         | 876/25000 [24:02<29:46:54,  4.44s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:37,545 >> Initializing global attention on CLS token...
  4%|▎         | 877/25000 [24:03<23:57:38,  3.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:39,094 >> Initializing global attention on CLS token...
  4%|▎         | 878/25000 [24:05<19:53:22,  2.97s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:40,648 >> Initializing global attention on CLS token...
  4%|▎         | 879/25000 [24:06<17:18:52,  2.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:42,333 >> Initializing global attention on CLS token...
  4%|▎         | 880/25000 [24:08<15:13:57,  2.27s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:43,882 >> Initializing global attention on CLS token...
  4%|▎         | 881/25000 [24:10<13:47:08,  2.06s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:45,436 >> Initializing global attention on CLS token...
  4%|▎         | 882/25000 [24:11<12:54:43,  1.93s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:47,059 >> Initializing global attention on CLS token...
  4%|▎         | 883/25000 [24:13<12:08:54,  1.81s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:48,607 >> Initializing global attention on CLS token...
  4%|▎         | 884/25000 [24:14<11:36:59,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:50,156 >> Initializing global attention on CLS token...
  4%|▎         | 885/25000 [24:16<11:14:33,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:51,704 >> Initializing global attention on CLS token...
  4%|▎         | 886/25000 [24:17<11:10:16,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:53,347 >> Initializing global attention on CLS token...
  4%|▎         | 887/25000 [24:19<10:55:57,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:54,896 >> Initializing global attention on CLS token...
  4%|▎         | 888/25000 [24:21<10:46:58,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:56,454 >> Initializing global attention on CLS token...
  4%|▎         | 889/25000 [24:22<10:39:43,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:58,004 >> Initializing global attention on CLS token...
  4%|▎         | 890/25000 [24:24<10:52:42,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:59,704 >> Initializing global attention on CLS token...
  4%|▎         | 891/25000 [24:25<10:43:28,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:01,252 >> Initializing global attention on CLS token...
  4%|▎         | 892/25000 [24:27<10:37:17,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:02,815 >> Initializing global attention on CLS token...
  4%|▎         | 893/25000 [24:29<10:42:23,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:04,431 >> Initializing global attention on CLS token...
  4%|▎         | 894/25000 [24:30<10:36:40,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:05,983 >> Initializing global attention on CLS token...
  4%|▎         | 895/25000 [24:32<10:32:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:07,535 >> Initializing global attention on CLS token...
  4%|▎         | 896/25000 [24:33<10:30:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:09,088 >> Initializing global attention on CLS token...
  4%|▎         | 897/25000 [24:35<10:35:01,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:10,698 >> Initializing global attention on CLS token...
  4%|▎         | 898/25000 [24:36<10:31:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:12,246 >> Initializing global attention on CLS token...
  4%|▎         | 899/25000 [24:38<10:28:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:13,797 >> Initializing global attention on CLS token...
  4%|▎         | 900/25000 [24:39<10:26:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:15,348 >> Initializing global attention on CLS token...
  4%|▎         | 901/25000 [24:41<10:31:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:16,943 >> Initializing global attention on CLS token...
  4%|▎         | 902/25000 [24:43<10:28:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:18,493 >> Initializing global attention on CLS token...
  4%|▎         | 903/25000 [24:44<10:26:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:20,044 >> Initializing global attention on CLS token...
  4%|▎         | 904/25000 [24:46<10:27:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:21,613 >> Initializing global attention on CLS token...
  4%|▎         | 905/25000 [24:47<10:36:45,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:23,251 >> Initializing global attention on CLS token...
  4%|▎         | 906/25000 [24:49<10:32:37,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:24,803 >> Initializing global attention on CLS token...
  4%|▎         | 907/25000 [24:51<10:29:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:26,356 >> Initializing global attention on CLS token...
  4%|▎         | 908/25000 [24:52<10:36:04,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:27,976 >> Initializing global attention on CLS token...
  4%|▎         | 909/25000 [24:54<10:32:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:29,529 >> Initializing global attention on CLS token...
  4%|▎         | 910/25000 [24:55<10:29:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:31,079 >> Initializing global attention on CLS token...
  4%|▎         | 911/25000 [24:57<10:27:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:32,629 >> Initializing global attention on CLS token...
  4%|▎         | 912/25000 [24:58<10:37:13,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:34,275 >> Initializing global attention on CLS token...
  4%|▎         | 913/25000 [25:00<10:32:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:35,826 >> Initializing global attention on CLS token...
  4%|▎         | 914/25000 [25:02<10:32:28,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:37,400 >> Initializing global attention on CLS token...
  4%|▎         | 915/25000 [25:03<10:40:45,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:39,042 >> Initializing global attention on CLS token...
  4%|▎         | 916/25000 [25:05<10:35:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:40,594 >> Initializing global attention on CLS token...
  4%|▎         | 917/25000 [25:06<10:35:28,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:42,180 >> Initializing global attention on CLS token...
  4%|▎         | 918/25000 [25:08<10:31:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:43,731 >> Initializing global attention on CLS token...
  4%|▎         | 919/25000 [25:09<10:36:18,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:45,343 >> Initializing global attention on CLS token...
  4%|▎         | 920/25000 [25:11<10:31:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:46,893 >> Initializing global attention on CLS token...
  4%|▎         | 921/25000 [25:13<10:40:20,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:48,537 >> Initializing global attention on CLS token...
  4%|▎         | 922/25000 [25:14<10:45:18,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:50,172 >> Initializing global attention on CLS token...
  4%|▎         | 923/25000 [25:16<10:38:17,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:51,725 >> Initializing global attention on CLS token...
  4%|▎         | 924/25000 [25:17<10:37:50,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:53,311 >> Initializing global attention on CLS token...
  4%|▎         | 925/25000 [25:19<10:33:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:54,862 >> Initializing global attention on CLS token...
  4%|▎         | 926/25000 [25:21<10:32:55,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:56,439 >> Initializing global attention on CLS token...
  4%|▎         | 927/25000 [25:22<10:29:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:57,992 >> Initializing global attention on CLS token...
  4%|▎         | 928/25000 [25:24<10:28:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:59,550 >> Initializing global attention on CLS token...
  4%|▎         | 929/25000 [25:25<10:26:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:01,102 >> Initializing global attention on CLS token...
  4%|▎         | 930/25000 [25:27<10:37:29,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:02,754 >> Initializing global attention on CLS token...
  4%|▎         | 931/25000 [25:28<10:38:01,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:04,348 >> Initializing global attention on CLS token...
  4%|▎         | 932/25000 [25:30<10:33:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:05,912 >> Initializing global attention on CLS token...
  4%|▎         | 933/25000 [25:32<10:39:24,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:07,529 >> Initializing global attention on CLS token...
  4%|▎         | 934/25000 [25:33<10:33:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:09,078 >> Initializing global attention on CLS token...
  4%|▎         | 935/25000 [25:35<10:30:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:10,630 >> Initializing global attention on CLS token...
  4%|▎         | 936/25000 [25:36<10:27:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:12,181 >> Initializing global attention on CLS token...
  4%|▎         | 937/25000 [25:38<10:38:24,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:13,834 >> Initializing global attention on CLS token...
  4%|▍         | 938/25000 [25:40<10:33:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:15,386 >> Initializing global attention on CLS token...
  4%|▍         | 939/25000 [25:41<10:30:08,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:16,938 >> Initializing global attention on CLS token...
  4%|▍         | 940/25000 [25:43<10:32:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:18,531 >> Initializing global attention on CLS token...
  4%|▍         | 941/25000 [25:44<10:29:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:20,079 >> Initializing global attention on CLS token...
  4%|▍         | 942/25000 [25:46<10:28:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:21,641 >> Initializing global attention on CLS token...
  4%|▍         | 943/25000 [25:47<10:26:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:23,193 >> Initializing global attention on CLS token...
  4%|▍         | 944/25000 [25:49<10:31:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:24,794 >> Initializing global attention on CLS token...
  4%|▍         | 945/25000 [25:50<10:28:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:26,344 >> Initializing global attention on CLS token...
  4%|▍         | 946/25000 [25:52<10:26:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:27,894 >> Initializing global attention on CLS token...
  4%|▍         | 947/25000 [25:54<10:32:07,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:29,507 >> Initializing global attention on CLS token...
  4%|▍         | 948/25000 [25:55<10:29:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:31,059 >> Initializing global attention on CLS token...
  4%|▍         | 949/25000 [25:57<10:27:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:32,613 >> Initializing global attention on CLS token...
  4%|▍         | 950/25000 [25:58<10:25:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:34,163 >> Initializing global attention on CLS token...
  4%|▍         | 951/25000 [26:00<10:30:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:35,763 >> Initializing global attention on CLS token...
  4%|▍         | 952/25000 [26:01<10:28:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:37,317 >> Initializing global attention on CLS token...
  4%|▍         | 953/25000 [26:03<10:25:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:38,868 >> Initializing global attention on CLS token...
  4%|▍         | 954/25000 [26:05<10:31:21,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:40,474 >> Initializing global attention on CLS token...
  4%|▍         | 955/25000 [26:06<10:28:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:42,027 >> Initializing global attention on CLS token...
  4%|▍         | 956/25000 [26:08<10:26:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:43,576 >> Initializing global attention on CLS token...
  4%|▍         | 957/25000 [26:09<10:24:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:45,127 >> Initializing global attention on CLS token...
  4%|▍         | 958/25000 [26:11<10:36:02,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:46,780 >> Initializing global attention on CLS token...
  4%|▍         | 959/25000 [26:12<10:31:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:48,329 >> Initializing global attention on CLS token...
  4%|▍         | 960/25000 [26:14<10:28:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:49,880 >> Initializing global attention on CLS token...
  4%|▍         | 961/25000 [26:16<10:32:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:51,485 >> Initializing global attention on CLS token...
  4%|▍         | 962/25000 [26:17<10:29:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:53,035 >> Initializing global attention on CLS token...
  4%|▍         | 963/25000 [26:19<10:27:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:54,589 >> Initializing global attention on CLS token...
  4%|▍         | 964/25000 [26:20<10:32:02,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:56,252 >> Initializing global attention on CLS token...
  4%|▍         | 965/25000 [26:22<10:35:41,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:57,803 >> Initializing global attention on CLS token...
  4%|▍         | 966/25000 [26:24<10:31:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:59,358 >> Initializing global attention on CLS token...
  4%|▍         | 967/25000 [26:25<10:28:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:00,908 >> Initializing global attention on CLS token...
  4%|▍         | 968/25000 [26:27<10:26:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:02,463 >> Initializing global attention on CLS token...
  4%|▍         | 969/25000 [26:28<10:25:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:04,016 >> Initializing global attention on CLS token...
  4%|▍         | 970/25000 [26:30<10:24:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:05,569 >> Initializing global attention on CLS token...
  4%|▍         | 971/25000 [26:31<10:31:25,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:07,184 >> Initializing global attention on CLS token...
  4%|▍         | 972/25000 [26:33<10:27:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:08,734 >> Initializing global attention on CLS token...
  4%|▍         | 973/25000 [26:34<10:25:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:10,286 >> Initializing global attention on CLS token...
  4%|▍         | 974/25000 [26:36<10:24:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:11,839 >> Initializing global attention on CLS token...
  4%|▍         | 975/25000 [26:38<10:29:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:13,436 >> Initializing global attention on CLS token...
  4%|▍         | 976/25000 [26:39<10:26:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:14,986 >> Initializing global attention on CLS token...
  4%|▍         | 977/25000 [26:41<10:24:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:16,537 >> Initializing global attention on CLS token...
  4%|▍         | 978/25000 [26:42<10:23:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:18,086 >> Initializing global attention on CLS token...
  4%|▍         | 979/25000 [26:44<10:39:03,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:19,775 >> Initializing global attention on CLS token...
  4%|▍         | 980/25000 [26:45<10:33:38,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:21,325 >> Initializing global attention on CLS token...
  4%|▍         | 981/25000 [26:47<10:30:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:22,876 >> Initializing global attention on CLS token...
  4%|▍         | 982/25000 [26:49<10:39:17,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,529 >> Initializing global attention on CLS token...
  4%|▍         | 983/25000 [26:50<10:33:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,081 >> Initializing global attention on CLS token...
  4%|▍         | 984/25000 [26:52<10:29:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,631 >> Initializing global attention on CLS token...
  4%|▍         | 985/25000 [26:53<10:26:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,180 >> Initializing global attention on CLS token...
  4%|▍         | 986/25000 [26:55<10:32:27,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:30,793 >> Initializing global attention on CLS token...
  4%|▍         | 987/25000 [26:56<10:29:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:32,345 >> Initializing global attention on CLS token...
  4%|▍         | 988/25000 [26:58<10:26:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:33,897 >> Initializing global attention on CLS token...
  4%|▍         | 989/25000 [27:00<10:25:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:35,451 >> Initializing global attention on CLS token...
  4%|▍         | 990/25000 [27:01<10:38:46,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:37,126 >> Initializing global attention on CLS token...
  4%|▍         | 991/25000 [27:03<10:33:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:38,677 >> Initializing global attention on CLS token...
  4%|▍         | 992/25000 [27:04<10:29:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:40,227 >> Initializing global attention on CLS token...
  4%|▍         | 993/25000 [27:06<10:36:50,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:41,862 >> Initializing global attention on CLS token...
  4%|▍         | 994/25000 [27:08<10:31:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:43,413 >> Initializing global attention on CLS token...
  4%|▍         | 995/25000 [27:09<10:28:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:44,967 >> Initializing global attention on CLS token...
  4%|▍         | 996/25000 [27:11<10:26:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:46,524 >> Initializing global attention on CLS token...
  4%|▍         | 997/25000 [27:12<10:31:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:48,131 >> Initializing global attention on CLS token...
  4%|▍         | 998/25000 [27:14<10:28:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:49,686 >> Initializing global attention on CLS token...
  4%|▍         | 999/25000 [27:15<10:26:08,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:51,216 >> Initializing global attention on CLS token...
  4%|▍         | 1000/25000 [27:17<10:22:23,  1.56s/it]                                                         4%|▍         | 1000/25000 [27:17<10:22:23,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:45:52,752 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:45:52,755 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:45:52,755 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:45:52,755 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:52,769 >> Initializing global attention on CLS token...
{'eval_loss': 4.143527507781982, 'eval_accuracy': 0.324, 'eval_macro_f1': 0.08208745767281886, 'eval_macro_precision': 0.06799557307707646, 'eval_macro_recall': 0.12436574464261761, 'eval_micro_f1': 0.324, 'eval_micro_precision': 0.324, 'eval_micro_recall': 0.324, 'eval_combined_score': 0.22434982505607332, 'eval_runtime': 7.7309, 'eval_samples_per_second': 64.675, 'eval_steps_per_second': 8.149, 'epoch': 7.0}
{'loss': 2.4842, 'learning_rate': 6.72e-05, 'epoch': 8.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:52,890 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.72it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,010 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,274 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:08,  7.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,393 >> Initializing global attention on CLS token...

  8%|▊         | 5/63 [00:00<00:07,  7.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,518 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:07,  7.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,644 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:07,  7.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,767 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  7.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:53,888 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:06,  8.03it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,008 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,127 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,248 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,369 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,488 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,610 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,733 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,853 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:02<00:05,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,973 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,095 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,215 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,335 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,455 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,577 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,700 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,820 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,942 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,063 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,183 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,304 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,424 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,545 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,665 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,785 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:56,905 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,025 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,145 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,265 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,385 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,505 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,625 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,745 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,865 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,985 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,105 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,226 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,346 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,467 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,586 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,708 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,828 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  6.53it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,064 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  6.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,199 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  7.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,320 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  7.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,440 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  7.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,563 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:01,  7.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,683 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  7.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,804 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:07<00:00,  8.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:59,924 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:46:00,047 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:46:00,167 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:46:00,286 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:46:00,404 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:46:00,548 >> Initializing global attention on CLS token...

100%|██████████| 63/63 [00:07<00:00,  8.73it/s][A/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  4%|▍         | 1000/25000 [27:25<10:22:23,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.73it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:46:00,636 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1000
[INFO|configuration_utils.py:461] 2024-01-22 00:46:00,642 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1000/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:46:01,209 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:46:01,210 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:46:01,210 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1000/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:46:02,296 >> Initializing global attention on CLS token...
  4%|▍         | 1001/25000 [27:28<29:24:33,  4.41s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:03,845 >> Initializing global attention on CLS token...
  4%|▍         | 1002/25000 [27:30<23:55:20,  3.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:05,513 >> Initializing global attention on CLS token...
  4%|▍         | 1003/25000 [27:31<19:50:21,  2.98s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:07,061 >> Initializing global attention on CLS token...
  4%|▍         | 1004/25000 [27:33<16:59:13,  2.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:08,611 >> Initializing global attention on CLS token...
  4%|▍         | 1005/25000 [27:34<14:59:34,  2.25s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:10,162 >> Initializing global attention on CLS token...
  4%|▍         | 1006/25000 [27:36<13:50:16,  2.08s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:11,835 >> Initializing global attention on CLS token...
  4%|▍         | 1007/25000 [27:38<12:47:53,  1.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:13,399 >> Initializing global attention on CLS token...
  4%|▍         | 1008/25000 [27:39<12:05:13,  1.81s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:14,956 >> Initializing global attention on CLS token...
  4%|▍         | 1009/25000 [27:41<11:49:24,  1.77s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:16,639 >> Initializing global attention on CLS token...
  4%|▍         | 1010/25000 [27:42<11:22:25,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:18,188 >> Initializing global attention on CLS token...
  4%|▍         | 1011/25000 [27:44<11:03:34,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:19,738 >> Initializing global attention on CLS token...
  4%|▍         | 1012/25000 [27:45<10:50:29,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:21,288 >> Initializing global attention on CLS token...
  4%|▍         | 1013/25000 [27:47<10:52:27,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:22,932 >> Initializing global attention on CLS token...
  4%|▍         | 1014/25000 [27:49<10:42:26,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:24,481 >> Initializing global attention on CLS token...
  4%|▍         | 1015/25000 [27:50<10:35:47,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:26,032 >> Initializing global attention on CLS token...
  4%|▍         | 1016/25000 [27:52<10:30:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:27,581 >> Initializing global attention on CLS token...
  4%|▍         | 1017/25000 [27:53<10:36:44,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:29,215 >> Initializing global attention on CLS token...
  4%|▍         | 1018/25000 [27:55<10:32:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:30,767 >> Initializing global attention on CLS token...
  4%|▍         | 1019/25000 [27:56<10:29:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:32,320 >> Initializing global attention on CLS token...
  4%|▍         | 1020/25000 [27:58<10:32:02,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:33,920 >> Initializing global attention on CLS token...
  4%|▍         | 1021/25000 [28:00<10:28:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:35,473 >> Initializing global attention on CLS token...
  4%|▍         | 1022/25000 [28:01<10:26:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:37,027 >> Initializing global attention on CLS token...
  4%|▍         | 1023/25000 [28:03<10:24:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:38,580 >> Initializing global attention on CLS token...
  4%|▍         | 1024/25000 [28:04<10:27:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:40,168 >> Initializing global attention on CLS token...
  4%|▍         | 1025/25000 [28:06<10:24:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:41,717 >> Initializing global attention on CLS token...
  4%|▍         | 1026/25000 [28:07<10:23:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:43,265 >> Initializing global attention on CLS token...
  4%|▍         | 1027/25000 [28:09<10:26:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:44,855 >> Initializing global attention on CLS token...
  4%|▍         | 1028/25000 [28:11<10:24:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:46,404 >> Initializing global attention on CLS token...
  4%|▍         | 1029/25000 [28:12<10:22:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:47,956 >> Initializing global attention on CLS token...
  4%|▍         | 1030/25000 [28:14<10:22:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:49,507 >> Initializing global attention on CLS token...
  4%|▍         | 1031/25000 [28:15<10:30:45,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:51,138 >> Initializing global attention on CLS token...
  4%|▍         | 1032/25000 [28:17<10:27:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:52,687 >> Initializing global attention on CLS token...
  4%|▍         | 1033/25000 [28:18<10:24:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:54,238 >> Initializing global attention on CLS token...
  4%|▍         | 1034/25000 [28:20<10:31:38,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:55,859 >> Initializing global attention on CLS token...
  4%|▍         | 1035/25000 [28:22<10:27:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:57,409 >> Initializing global attention on CLS token...
  4%|▍         | 1036/25000 [28:23<10:24:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:58,955 >> Initializing global attention on CLS token...
  4%|▍         | 1037/25000 [28:25<10:22:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:00,504 >> Initializing global attention on CLS token...
  4%|▍         | 1038/25000 [28:26<10:27:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:02,100 >> Initializing global attention on CLS token...
  4%|▍         | 1039/25000 [28:28<10:24:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:03,649 >> Initializing global attention on CLS token...
  4%|▍         | 1040/25000 [28:29<10:22:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:05,198 >> Initializing global attention on CLS token...
  4%|▍         | 1041/25000 [28:31<10:23:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:06,761 >> Initializing global attention on CLS token...
  4%|▍         | 1042/25000 [28:32<10:21:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:08,311 >> Initializing global attention on CLS token...
  4%|▍         | 1043/25000 [28:34<10:20:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:09,859 >> Initializing global attention on CLS token...
  4%|▍         | 1044/25000 [28:36<10:20:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:11,410 >> Initializing global attention on CLS token...
  4%|▍         | 1045/25000 [28:37<10:24:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:13,002 >> Initializing global attention on CLS token...
  4%|▍         | 1046/25000 [28:39<10:23:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:14,552 >> Initializing global attention on CLS token...
  4%|▍         | 1047/25000 [28:40<10:21:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:16,101 >> Initializing global attention on CLS token...
  4%|▍         | 1048/25000 [28:42<10:24:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:17,680 >> Initializing global attention on CLS token...
  4%|▍         | 1049/25000 [28:43<10:22:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:19,228 >> Initializing global attention on CLS token...
  4%|▍         | 1050/25000 [28:45<10:21:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:20,781 >> Initializing global attention on CLS token...
  4%|▍         | 1051/25000 [28:46<10:20:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:22,330 >> Initializing global attention on CLS token...
  4%|▍         | 1052/25000 [28:48<10:31:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:23,978 >> Initializing global attention on CLS token...
  4%|▍         | 1053/25000 [28:50<10:27:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:25,528 >> Initializing global attention on CLS token...
  4%|▍         | 1054/25000 [28:51<10:24:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:27,078 >> Initializing global attention on CLS token...
  4%|▍         | 1055/25000 [28:53<10:27:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:28,664 >> Initializing global attention on CLS token...
  4%|▍         | 1056/25000 [28:54<10:24:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:30,213 >> Initializing global attention on CLS token...
  4%|▍         | 1057/25000 [28:56<10:23:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:31,765 >> Initializing global attention on CLS token...
  4%|▍         | 1058/25000 [28:57<10:22:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:33,322 >> Initializing global attention on CLS token...
  4%|▍         | 1059/25000 [28:59<10:28:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:34,929 >> Initializing global attention on CLS token...
  4%|▍         | 1060/25000 [29:01<10:25:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:36,486 >> Initializing global attention on CLS token...
  4%|▍         | 1061/25000 [29:02<10:23:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:38,037 >> Initializing global attention on CLS token...
  4%|▍         | 1062/25000 [29:04<10:34:25,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:39,687 >> Initializing global attention on CLS token...
  4%|▍         | 1063/25000 [29:05<10:30:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:41,243 >> Initializing global attention on CLS token...
  4%|▍         | 1064/25000 [29:07<10:26:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:42,793 >> Initializing global attention on CLS token...
  4%|▍         | 1065/25000 [29:08<10:23:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:44,342 >> Initializing global attention on CLS token...
  4%|▍         | 1066/25000 [29:10<10:27:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:45,937 >> Initializing global attention on CLS token...
  4%|▍         | 1067/25000 [29:12<10:24:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:47,487 >> Initializing global attention on CLS token...
  4%|▍         | 1068/25000 [29:13<10:22:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:49,038 >> Initializing global attention on CLS token...
  4%|▍         | 1069/25000 [29:15<10:22:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:50,595 >> Initializing global attention on CLS token...
  4%|▍         | 1070/25000 [29:16<10:31:52,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:52,235 >> Initializing global attention on CLS token...
  4%|▍         | 1071/25000 [29:18<10:27:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:53,785 >> Initializing global attention on CLS token...
  4%|▍         | 1072/25000 [29:19<10:25:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:55,338 >> Initializing global attention on CLS token...
  4%|▍         | 1073/25000 [29:21<10:37:47,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:57,010 >> Initializing global attention on CLS token...
  4%|▍         | 1074/25000 [29:23<10:31:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:58,562 >> Initializing global attention on CLS token...
  4%|▍         | 1075/25000 [29:24<10:28:02,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:00,116 >> Initializing global attention on CLS token...
  4%|▍         | 1076/25000 [29:26<10:25:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:01,667 >> Initializing global attention on CLS token...
  4%|▍         | 1077/25000 [29:27<10:31:49,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:03,290 >> Initializing global attention on CLS token...
  4%|▍         | 1078/25000 [29:29<10:27:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:04,840 >> Initializing global attention on CLS token...
  4%|▍         | 1079/25000 [29:31<10:25:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:06,393 >> Initializing global attention on CLS token...
  4%|▍         | 1080/25000 [29:32<10:23:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:07,947 >> Initializing global attention on CLS token...
  4%|▍         | 1081/25000 [29:34<10:23:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:09,513 >> Initializing global attention on CLS token...
  4%|▍         | 1082/25000 [29:35<10:21:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:11,063 >> Initializing global attention on CLS token...
  4%|▍         | 1083/25000 [29:37<10:21:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:12,617 >> Initializing global attention on CLS token...
  4%|▍         | 1084/25000 [29:38<10:22:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:14,183 >> Initializing global attention on CLS token...
  4%|▍         | 1085/25000 [29:40<10:20:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:15,733 >> Initializing global attention on CLS token...
  4%|▍         | 1086/25000 [29:41<10:19:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:17,283 >> Initializing global attention on CLS token...
  4%|▍         | 1087/25000 [29:43<10:20:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:18,845 >> Initializing global attention on CLS token...
  4%|▍         | 1088/25000 [29:45<10:27:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:20,462 >> Initializing global attention on CLS token...
  4%|▍         | 1089/25000 [29:46<10:26:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:22,025 >> Initializing global attention on CLS token...
  4%|▍         | 1090/25000 [29:48<10:23:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:23,577 >> Initializing global attention on CLS token...
  4%|▍         | 1091/25000 [29:49<10:32:29,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:25,215 >> Initializing global attention on CLS token...
  4%|▍         | 1092/25000 [29:51<10:28:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:26,765 >> Initializing global attention on CLS token...
  4%|▍         | 1093/25000 [29:52<10:24:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:28,315 >> Initializing global attention on CLS token...
  4%|▍         | 1094/25000 [29:54<10:22:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:29,864 >> Initializing global attention on CLS token...
  4%|▍         | 1095/25000 [29:56<10:36:25,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:31,543 >> Initializing global attention on CLS token...
  4%|▍         | 1096/25000 [29:57<10:30:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:33,091 >> Initializing global attention on CLS token...
  4%|▍         | 1097/25000 [29:59<10:26:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:34,642 >> Initializing global attention on CLS token...
  4%|▍         | 1098/25000 [30:00<10:35:46,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:36,291 >> Initializing global attention on CLS token...
  4%|▍         | 1099/25000 [30:02<10:30:10,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:37,841 >> Initializing global attention on CLS token...
  4%|▍         | 1100/25000 [30:04<10:28:09,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:39,406 >> Initializing global attention on CLS token...
  4%|▍         | 1101/25000 [30:05<10:25:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:40,962 >> Initializing global attention on CLS token...
  4%|▍         | 1102/25000 [30:07<10:28:16,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:42,555 >> Initializing global attention on CLS token...
  4%|▍         | 1103/25000 [30:08<10:24:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:44,104 >> Initializing global attention on CLS token...
  4%|▍         | 1104/25000 [30:10<10:22:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:45,654 >> Initializing global attention on CLS token...
  4%|▍         | 1105/25000 [30:11<10:21:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:47,208 >> Initializing global attention on CLS token...
  4%|▍         | 1106/25000 [30:13<10:20:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,757 >> Initializing global attention on CLS token...
  4%|▍         | 1107/25000 [30:14<10:19:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,307 >> Initializing global attention on CLS token...
  4%|▍         | 1108/25000 [30:16<10:18:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,858 >> Initializing global attention on CLS token...
  4%|▍         | 1109/25000 [30:18<10:20:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,427 >> Initializing global attention on CLS token...
  4%|▍         | 1110/25000 [30:19<10:19:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,980 >> Initializing global attention on CLS token...
  4%|▍         | 1111/25000 [30:21<10:19:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:56,531 >> Initializing global attention on CLS token...
  4%|▍         | 1112/25000 [30:22<10:25:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:58,136 >> Initializing global attention on CLS token...
  4%|▍         | 1113/25000 [30:24<10:22:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:59,688 >> Initializing global attention on CLS token...
  4%|▍         | 1114/25000 [30:25<10:21:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:01,239 >> Initializing global attention on CLS token...
  4%|▍         | 1115/25000 [30:27<10:20:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:02,792 >> Initializing global attention on CLS token...
  4%|▍         | 1116/25000 [30:29<10:26:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:04,405 >> Initializing global attention on CLS token...
  4%|▍         | 1117/25000 [30:30<10:24:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:05,956 >> Initializing global attention on CLS token...
  4%|▍         | 1118/25000 [30:32<10:21:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:07,505 >> Initializing global attention on CLS token...
  4%|▍         | 1119/25000 [30:33<10:23:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:09,080 >> Initializing global attention on CLS token...
  4%|▍         | 1120/25000 [30:35<10:21:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:10,631 >> Initializing global attention on CLS token...
  4%|▍         | 1121/25000 [30:36<10:20:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:12,184 >> Initializing global attention on CLS token...
  4%|▍         | 1122/25000 [30:38<10:19:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:13,734 >> Initializing global attention on CLS token...
  4%|▍         | 1123/25000 [30:39<10:24:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:15,336 >> Initializing global attention on CLS token...
  4%|▍         | 1124/25000 [30:41<10:22:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:16,868 >> Initializing global attention on CLS token...
  4%|▍         | 1125/25000 [30:43<10:18:30,  1.55s/it]                                                         4%|▍         | 1125/25000 [30:43<10:18:30,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:49:18,398 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:49:18,401 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:49:18,413 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:49:18,413 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:18,426 >> Initializing global attention on CLS token...
{'eval_loss': 4.106122016906738, 'eval_accuracy': 0.338, 'eval_macro_f1': 0.10580913949566427, 'eval_macro_precision': 0.09689807559615865, 'eval_macro_recall': 0.14630030955909548, 'eval_micro_f1': 0.338, 'eval_micro_precision': 0.338, 'eval_micro_recall': 0.338, 'eval_combined_score': 0.24300107495013124, 'eval_runtime': 7.8789, 'eval_samples_per_second': 63.461, 'eval_steps_per_second': 7.996, 'epoch': 8.0}
{'loss': 2.2278, 'learning_rate': 6.684999999999999e-05, 'epoch': 9.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:18,546 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:18,666 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:18,785 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:18,906 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,026 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,145 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,393 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:07,  7.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,514 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:07,  7.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,634 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  7.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,757 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  7.80it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,877 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  7.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:19,996 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,116 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:06,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,237 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,357 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,477 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:02<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,598 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,717 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,837 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,957 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,077 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,197 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,319 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,440 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,560 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,679 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,799 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,920 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,041 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,159 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,279 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,399 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,519 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,642 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,763 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:22,883 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,004 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,126 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,255 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,375 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,498 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,618 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,738 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,858 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,978 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,098 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,220 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,342 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,462 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,583 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  6.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,838 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  6.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,958 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  7.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,078 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  7.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,199 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:01,  7.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,319 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  7.86it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,440 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:07<00:00,  7.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,561 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,683 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,803 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:25,922 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:26,040 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:26,155 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  4%|▍         | 1125/25000 [30:50<10:18:30,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.30it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:49:26,250 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1125
[INFO|configuration_utils.py:461] 2024-01-22 00:49:26,257 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1125/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:49:26,854 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1125/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:49:26,855 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1125/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:49:26,856 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1125/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:49:28,000 >> Initializing global attention on CLS token...
  5%|▍         | 1126/25000 [30:54<29:21:18,  4.43s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:29,547 >> Initializing global attention on CLS token...
  5%|▍         | 1127/25000 [30:55<23:43:30,  3.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:31,144 >> Initializing global attention on CLS token...
  5%|▍         | 1128/25000 [30:57<19:41:00,  2.97s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:32,691 >> Initializing global attention on CLS token...
  5%|▍         | 1129/25000 [30:58<16:52:01,  2.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:34,244 >> Initializing global attention on CLS token...
  5%|▍         | 1130/25000 [31:00<14:55:09,  2.25s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:35,808 >> Initializing global attention on CLS token...
  5%|▍         | 1131/25000 [31:02<13:31:37,  2.04s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:37,359 >> Initializing global attention on CLS token...
  5%|▍         | 1132/25000 [31:03<12:33:38,  1.89s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:38,913 >> Initializing global attention on CLS token...
  5%|▍         | 1133/25000 [31:05<11:52:34,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:40,464 >> Initializing global attention on CLS token...
  5%|▍         | 1134/25000 [31:06<11:25:35,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:42,030 >> Initializing global attention on CLS token...
  5%|▍         | 1135/25000 [31:08<11:04:39,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:43,577 >> Initializing global attention on CLS token...
  5%|▍         | 1136/25000 [31:09<10:50:22,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:45,132 >> Initializing global attention on CLS token...
  5%|▍         | 1137/25000 [31:11<10:40:40,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:46,684 >> Initializing global attention on CLS token...
  5%|▍         | 1138/25000 [31:13<10:49:51,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:48,373 >> Initializing global attention on CLS token...
  5%|▍         | 1139/25000 [31:14<10:40:42,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:49,930 >> Initializing global attention on CLS token...
  5%|▍         | 1140/25000 [31:16<10:33:29,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:51,481 >> Initializing global attention on CLS token...
  5%|▍         | 1141/25000 [31:17<10:39:20,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:53,122 >> Initializing global attention on CLS token...
  5%|▍         | 1142/25000 [31:19<10:32:11,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:54,671 >> Initializing global attention on CLS token...
  5%|▍         | 1143/25000 [31:20<10:27:11,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:56,219 >> Initializing global attention on CLS token...
  5%|▍         | 1144/25000 [31:22<10:23:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:57,769 >> Initializing global attention on CLS token...
  5%|▍         | 1145/25000 [31:24<10:32:13,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:59,408 >> Initializing global attention on CLS token...
  5%|▍         | 1146/25000 [31:25<10:27:49,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:00,962 >> Initializing global attention on CLS token...
  5%|▍         | 1147/25000 [31:27<10:24:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:02,513 >> Initializing global attention on CLS token...
  5%|▍         | 1148/25000 [31:28<10:22:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:04,066 >> Initializing global attention on CLS token...
  5%|▍         | 1149/25000 [31:30<10:33:01,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:05,721 >> Initializing global attention on CLS token...
  5%|▍         | 1150/25000 [31:31<10:28:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:07,278 >> Initializing global attention on CLS token...
  5%|▍         | 1151/25000 [31:33<10:25:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:08,830 >> Initializing global attention on CLS token...
  5%|▍         | 1152/25000 [31:35<10:34:10,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:10,479 >> Initializing global attention on CLS token...
  5%|▍         | 1153/25000 [31:36<10:28:36,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:12,027 >> Initializing global attention on CLS token...
  5%|▍         | 1154/25000 [31:38<10:24:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:13,577 >> Initializing global attention on CLS token...
  5%|▍         | 1155/25000 [31:39<10:22:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:15,129 >> Initializing global attention on CLS token...
  5%|▍         | 1156/25000 [31:41<10:28:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:16,742 >> Initializing global attention on CLS token...
  5%|▍         | 1157/25000 [31:42<10:24:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:18,291 >> Initializing global attention on CLS token...
  5%|▍         | 1158/25000 [31:44<10:21:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:19,841 >> Initializing global attention on CLS token...
  5%|▍         | 1159/25000 [31:46<10:31:21,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:21,487 >> Initializing global attention on CLS token...
  5%|▍         | 1160/25000 [31:47<10:26:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:23,043 >> Initializing global attention on CLS token...
  5%|▍         | 1161/25000 [31:49<10:23:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:24,591 >> Initializing global attention on CLS token...
  5%|▍         | 1162/25000 [31:50<10:21:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:26,142 >> Initializing global attention on CLS token...
  5%|▍         | 1163/25000 [31:52<10:21:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:27,705 >> Initializing global attention on CLS token...
  5%|▍         | 1164/25000 [31:53<10:19:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:29,255 >> Initializing global attention on CLS token...
  5%|▍         | 1165/25000 [31:55<10:19:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:30,813 >> Initializing global attention on CLS token...
  5%|▍         | 1166/25000 [31:57<10:24:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:32,418 >> Initializing global attention on CLS token...
  5%|▍         | 1167/25000 [31:58<10:22:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:33,968 >> Initializing global attention on CLS token...
  5%|▍         | 1168/25000 [32:00<10:20:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:35,519 >> Initializing global attention on CLS token...
  5%|▍         | 1169/25000 [32:01<10:18:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:37,068 >> Initializing global attention on CLS token...
  5%|▍         | 1170/25000 [32:03<10:22:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:38,657 >> Initializing global attention on CLS token...
  5%|▍         | 1171/25000 [32:04<10:20:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:40,207 >> Initializing global attention on CLS token...
  5%|▍         | 1172/25000 [32:06<10:19:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:41,762 >> Initializing global attention on CLS token...
  5%|▍         | 1173/25000 [32:08<10:25:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:43,372 >> Initializing global attention on CLS token...
  5%|▍         | 1174/25000 [32:09<10:22:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:44,923 >> Initializing global attention on CLS token...
  5%|▍         | 1175/25000 [32:11<10:20:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:46,474 >> Initializing global attention on CLS token...
  5%|▍         | 1176/25000 [32:12<10:19:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:48,026 >> Initializing global attention on CLS token...
  5%|▍         | 1177/25000 [32:14<10:27:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:49,652 >> Initializing global attention on CLS token...
  5%|▍         | 1178/25000 [32:15<10:23:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:51,203 >> Initializing global attention on CLS token...
  5%|▍         | 1179/25000 [32:17<10:21:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:52,754 >> Initializing global attention on CLS token...
  5%|▍         | 1180/25000 [32:18<10:21:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:54,319 >> Initializing global attention on CLS token...
  5%|▍         | 1181/25000 [32:20<10:19:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:55,868 >> Initializing global attention on CLS token...
  5%|▍         | 1182/25000 [32:22<10:18:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:57,417 >> Initializing global attention on CLS token...
  5%|▍         | 1183/25000 [32:23<10:17:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:58,967 >> Initializing global attention on CLS token...
  5%|▍         | 1184/25000 [32:25<10:20:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:00,548 >> Initializing global attention on CLS token...
  5%|▍         | 1185/25000 [32:26<10:18:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:02,095 >> Initializing global attention on CLS token...
  5%|▍         | 1186/25000 [32:28<10:17:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:03,649 >> Initializing global attention on CLS token...
  5%|▍         | 1187/25000 [32:29<10:18:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:05,214 >> Initializing global attention on CLS token...
  5%|▍         | 1188/25000 [32:31<10:18:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:06,771 >> Initializing global attention on CLS token...
  5%|▍         | 1189/25000 [32:32<10:17:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:08,322 >> Initializing global attention on CLS token...
  5%|▍         | 1190/25000 [32:34<10:16:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:09,872 >> Initializing global attention on CLS token...
  5%|▍         | 1191/25000 [32:36<10:19:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:11,446 >> Initializing global attention on CLS token...
  5%|▍         | 1192/25000 [32:37<10:17:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:12,995 >> Initializing global attention on CLS token...
  5%|▍         | 1193/25000 [32:39<10:16:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:14,551 >> Initializing global attention on CLS token...
  5%|▍         | 1194/25000 [32:40<10:18:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:16,106 >> Initializing global attention on CLS token...
  5%|▍         | 1195/25000 [32:42<10:16:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:17,656 >> Initializing global attention on CLS token...
  5%|▍         | 1196/25000 [32:43<10:16:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:19,207 >> Initializing global attention on CLS token...
  5%|▍         | 1197/25000 [32:45<10:15:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:20,757 >> Initializing global attention on CLS token...
  5%|▍         | 1198/25000 [32:47<10:24:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:22,385 >> Initializing global attention on CLS token...
  5%|▍         | 1199/25000 [32:48<10:21:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:23,935 >> Initializing global attention on CLS token...
  5%|▍         | 1200/25000 [32:50<10:20:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:25,488 >> Initializing global attention on CLS token...
  5%|▍         | 1201/25000 [32:51<10:18:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:27,041 >> Initializing global attention on CLS token...
  5%|▍         | 1202/25000 [32:53<10:29:07,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:28,688 >> Initializing global attention on CLS token...
  5%|▍         | 1203/25000 [32:54<10:24:45,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:30,238 >> Initializing global attention on CLS token...
  5%|▍         | 1204/25000 [32:56<10:21:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:31,789 >> Initializing global attention on CLS token...
  5%|▍         | 1205/25000 [32:58<10:28:35,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:33,414 >> Initializing global attention on CLS token...
  5%|▍         | 1206/25000 [32:59<10:24:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:34,966 >> Initializing global attention on CLS token...
  5%|▍         | 1207/25000 [33:01<10:21:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:36,517 >> Initializing global attention on CLS token...
  5%|▍         | 1208/25000 [33:02<10:20:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:38,071 >> Initializing global attention on CLS token...
  5%|▍         | 1209/25000 [33:04<10:27:24,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:39,697 >> Initializing global attention on CLS token...
  5%|▍         | 1210/25000 [33:05<10:23:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:41,246 >> Initializing global attention on CLS token...
  5%|▍         | 1211/25000 [33:07<10:20:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:42,797 >> Initializing global attention on CLS token...
  5%|▍         | 1212/25000 [33:08<10:19:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:44,348 >> Initializing global attention on CLS token...
  5%|▍         | 1213/25000 [33:10<10:33:54,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:46,035 >> Initializing global attention on CLS token...
  5%|▍         | 1214/25000 [33:12<10:28:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:47,586 >> Initializing global attention on CLS token...
  5%|▍         | 1215/25000 [33:13<10:23:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:49,136 >> Initializing global attention on CLS token...
  5%|▍         | 1216/25000 [33:15<10:28:25,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:50,747 >> Initializing global attention on CLS token...
  5%|▍         | 1217/25000 [33:16<10:24:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:52,296 >> Initializing global attention on CLS token...
  5%|▍         | 1218/25000 [33:18<10:22:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:53,859 >> Initializing global attention on CLS token...
  5%|▍         | 1219/25000 [33:20<10:20:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:55,409 >> Initializing global attention on CLS token...
  5%|▍         | 1220/25000 [33:21<10:23:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:57,003 >> Initializing global attention on CLS token...
  5%|▍         | 1221/25000 [33:23<10:20:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:58,555 >> Initializing global attention on CLS token...
  5%|▍         | 1222/25000 [33:24<10:19:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:00,106 >> Initializing global attention on CLS token...
  5%|▍         | 1223/25000 [33:26<10:28:14,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:01,746 >> Initializing global attention on CLS token...
  5%|▍         | 1224/25000 [33:27<10:24:14,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:03,297 >> Initializing global attention on CLS token...
  5%|▍         | 1225/25000 [33:29<10:21:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:04,852 >> Initializing global attention on CLS token...
  5%|▍         | 1226/25000 [33:31<10:19:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:06,405 >> Initializing global attention on CLS token...
  5%|▍         | 1227/25000 [33:32<10:22:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:07,991 >> Initializing global attention on CLS token...
  5%|▍         | 1228/25000 [33:34<10:20:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:09,545 >> Initializing global attention on CLS token...
  5%|▍         | 1229/25000 [33:35<10:18:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,094 >> Initializing global attention on CLS token...
  5%|▍         | 1230/25000 [33:37<10:18:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,656 >> Initializing global attention on CLS token...
  5%|▍         | 1231/25000 [33:38<10:17:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,208 >> Initializing global attention on CLS token...
  5%|▍         | 1232/25000 [33:40<10:16:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,757 >> Initializing global attention on CLS token...
  5%|▍         | 1233/25000 [33:41<10:15:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,306 >> Initializing global attention on CLS token...
  5%|▍         | 1234/25000 [33:43<10:16:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,869 >> Initializing global attention on CLS token...
  5%|▍         | 1235/25000 [33:45<10:16:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:20,425 >> Initializing global attention on CLS token...
  5%|▍         | 1236/25000 [33:46<10:15:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:21,975 >> Initializing global attention on CLS token...
  5%|▍         | 1237/25000 [33:48<10:18:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:23,546 >> Initializing global attention on CLS token...
  5%|▍         | 1238/25000 [33:49<10:16:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:25,098 >> Initializing global attention on CLS token...
  5%|▍         | 1239/25000 [33:51<10:15:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:26,648 >> Initializing global attention on CLS token...
  5%|▍         | 1240/25000 [33:52<10:15:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:28,198 >> Initializing global attention on CLS token...
  5%|▍         | 1241/25000 [33:54<10:17:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:29,775 >> Initializing global attention on CLS token...
  5%|▍         | 1242/25000 [33:55<10:17:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:31,330 >> Initializing global attention on CLS token...
  5%|▍         | 1243/25000 [33:57<10:16:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:32,880 >> Initializing global attention on CLS token...
  5%|▍         | 1244/25000 [33:59<10:15:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:34,428 >> Initializing global attention on CLS token...
  5%|▍         | 1245/25000 [34:00<10:19:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:36,022 >> Initializing global attention on CLS token...
  5%|▍         | 1246/25000 [34:02<10:18:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:37,572 >> Initializing global attention on CLS token...
  5%|▍         | 1247/25000 [34:03<10:16:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:39,123 >> Initializing global attention on CLS token...
  5%|▍         | 1248/25000 [34:05<10:27:45,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:40,774 >> Initializing global attention on CLS token...
  5%|▍         | 1249/25000 [34:06<10:23:45,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:42,306 >> Initializing global attention on CLS token...
  5%|▌         | 1250/25000 [34:08<10:19:04,  1.56s/it]                                                         5%|▌         | 1250/25000 [34:08<10:19:04,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:52:43,844 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:52:43,877 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:52:43,877 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:52:43,877 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:43,890 >> Initializing global attention on CLS token...
{'eval_loss': 4.083497524261475, 'eval_accuracy': 0.344, 'eval_macro_f1': 0.10723115505457566, 'eval_macro_precision': 0.0964974564152469, 'eval_macro_recall': 0.14717598695234477, 'eval_micro_f1': 0.344, 'eval_micro_precision': 0.344, 'eval_micro_recall': 0.344, 'eval_combined_score': 0.24670065691745244, 'eval_runtime': 7.8378, 'eval_samples_per_second': 63.793, 'eval_steps_per_second': 8.038, 'epoch': 9.0}
{'loss': 1.9976, 'learning_rate': 6.649999999999999e-05, 'epoch': 10.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,011 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,133 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,253 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,372 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,491 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,613 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,733 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,853 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:44,973 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.67it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,093 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.58it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,214 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:05,  8.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,334 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,454 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,574 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,694 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,818 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,939 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,058 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,197 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:06,  6.59it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,408 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  7.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,528 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  7.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,647 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:05,  7.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,768 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  7.81it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,889 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  7.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,009 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,129 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,249 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,369 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,489 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,609 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,729 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,849 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:47,968 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,089 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,209 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,329 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,448 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,569 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,689 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,809 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,929 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,053 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,173 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,293 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,413 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,533 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,653 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,772 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:49,893 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,013 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,133 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,253 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,373 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,494 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,613 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,734 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,856 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,976 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:51,096 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:51,219 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:51,336 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:51,451 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  5%|▌         | 1250/25000 [34:16<10:19:04,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.35it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:52:51,539 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1250
[INFO|configuration_utils.py:461] 2024-01-22 00:52:51,546 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1250/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:52:52,322 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:52:52,324 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:52:52,324 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1250/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:53,578 >> Initializing global attention on CLS token...
  5%|▌         | 1251/25000 [34:19<29:37:58,  4.49s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:55,186 >> Initializing global attention on CLS token...
  5%|▌         | 1252/25000 [34:21<23:48:51,  3.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:56,738 >> Initializing global attention on CLS token...
  5%|▌         | 1253/25000 [34:22<19:48:05,  3.00s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:58,321 >> Initializing global attention on CLS token...
  5%|▌         | 1254/25000 [34:24<16:55:13,  2.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:59,868 >> Initializing global attention on CLS token...
  5%|▌         | 1255/25000 [34:26<14:55:01,  2.26s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:01,420 >> Initializing global attention on CLS token...
  5%|▌         | 1256/25000 [34:27<13:30:21,  2.05s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:02,970 >> Initializing global attention on CLS token...
  5%|▌         | 1257/25000 [34:29<12:36:13,  1.91s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:04,562 >> Initializing global attention on CLS token...
  5%|▌         | 1258/25000 [34:30<11:53:22,  1.80s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:06,112 >> Initializing global attention on CLS token...
  5%|▌         | 1259/25000 [34:32<11:23:17,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:07,661 >> Initializing global attention on CLS token...
  5%|▌         | 1260/25000 [34:33<11:02:19,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:09,212 >> Initializing global attention on CLS token...
  5%|▌         | 1261/25000 [34:35<10:59:51,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:10,865 >> Initializing global attention on CLS token...
  5%|▌         | 1262/25000 [34:37<10:45:57,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:12,417 >> Initializing global attention on CLS token...
  5%|▌         | 1263/25000 [34:38<10:36:05,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:13,966 >> Initializing global attention on CLS token...
  5%|▌         | 1264/25000 [34:40<10:39:16,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:15,601 >> Initializing global attention on CLS token...
  5%|▌         | 1265/25000 [34:41<10:31:24,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:17,151 >> Initializing global attention on CLS token...
  5%|▌         | 1266/25000 [34:43<10:47:36,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:18,883 >> Initializing global attention on CLS token...
  5%|▌         | 1267/25000 [34:45<10:37:06,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:20,433 >> Initializing global attention on CLS token...
  5%|▌         | 1268/25000 [34:46<10:46:35,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:22,123 >> Initializing global attention on CLS token...
  5%|▌         | 1269/25000 [34:48<10:36:55,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:23,677 >> Initializing global attention on CLS token...
  5%|▌         | 1270/25000 [34:49<10:29:44,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:25,227 >> Initializing global attention on CLS token...
  5%|▌         | 1271/25000 [34:51<10:27:09,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:26,799 >> Initializing global attention on CLS token...
  5%|▌         | 1272/25000 [34:53<10:23:11,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:28,351 >> Initializing global attention on CLS token...
  5%|▌         | 1273/25000 [34:54<10:20:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:29,905 >> Initializing global attention on CLS token...
  5%|▌         | 1274/25000 [34:56<10:18:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:31,454 >> Initializing global attention on CLS token...
  5%|▌         | 1275/25000 [34:57<10:22:54,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:33,057 >> Initializing global attention on CLS token...
  5%|▌         | 1276/25000 [34:59<10:19:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:34,608 >> Initializing global attention on CLS token...
  5%|▌         | 1277/25000 [35:00<10:17:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:36,158 >> Initializing global attention on CLS token...
  5%|▌         | 1278/25000 [35:02<10:16:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:37,711 >> Initializing global attention on CLS token...
  5%|▌         | 1279/25000 [35:03<10:19:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:39,291 >> Initializing global attention on CLS token...
  5%|▌         | 1280/25000 [35:05<10:17:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:40,842 >> Initializing global attention on CLS token...
  5%|▌         | 1281/25000 [35:07<10:16:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:42,394 >> Initializing global attention on CLS token...
  5%|▌         | 1282/25000 [35:08<10:32:53,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:44,094 >> Initializing global attention on CLS token...
  5%|▌         | 1283/25000 [35:10<10:27:19,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:45,647 >> Initializing global attention on CLS token...
  5%|▌         | 1284/25000 [35:11<10:22:49,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:47,198 >> Initializing global attention on CLS token...
  5%|▌         | 1285/25000 [35:13<10:19:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:48,747 >> Initializing global attention on CLS token...
  5%|▌         | 1286/25000 [35:15<10:26:52,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:50,376 >> Initializing global attention on CLS token...
  5%|▌         | 1287/25000 [35:16<10:22:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:51,924 >> Initializing global attention on CLS token...
  5%|▌         | 1288/25000 [35:18<10:19:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:53,479 >> Initializing global attention on CLS token...
  5%|▌         | 1289/25000 [35:19<10:18:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:55,033 >> Initializing global attention on CLS token...
  5%|▌         | 1290/25000 [35:21<10:29:24,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:56,691 >> Initializing global attention on CLS token...
  5%|▌         | 1291/25000 [35:22<10:24:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:58,242 >> Initializing global attention on CLS token...
  5%|▌         | 1292/25000 [35:24<10:20:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:59,790 >> Initializing global attention on CLS token...
  5%|▌         | 1293/25000 [35:26<10:21:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:01,370 >> Initializing global attention on CLS token...
  5%|▌         | 1294/25000 [35:27<10:18:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:02,919 >> Initializing global attention on CLS token...
  5%|▌         | 1295/25000 [35:29<10:16:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:04,468 >> Initializing global attention on CLS token...
  5%|▌         | 1296/25000 [35:30<10:15:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:06,019 >> Initializing global attention on CLS token...
  5%|▌         | 1297/25000 [35:32<10:16:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:07,582 >> Initializing global attention on CLS token...
  5%|▌         | 1298/25000 [35:33<10:15:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:09,136 >> Initializing global attention on CLS token...
  5%|▌         | 1299/25000 [35:35<10:14:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:10,687 >> Initializing global attention on CLS token...
  5%|▌         | 1300/25000 [35:36<10:13:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:12,236 >> Initializing global attention on CLS token...
  5%|▌         | 1301/25000 [35:38<10:28:16,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:13,913 >> Initializing global attention on CLS token...
  5%|▌         | 1302/25000 [35:40<10:23:20,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:15,462 >> Initializing global attention on CLS token...
  5%|▌         | 1303/25000 [35:41<10:20:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:17,014 >> Initializing global attention on CLS token...
  5%|▌         | 1304/25000 [35:43<10:21:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:18,602 >> Initializing global attention on CLS token...
  5%|▌         | 1305/25000 [35:44<10:19:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:20,153 >> Initializing global attention on CLS token...
  5%|▌         | 1306/25000 [35:46<10:17:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:21,704 >> Initializing global attention on CLS token...
  5%|▌         | 1307/25000 [35:47<10:15:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:23,256 >> Initializing global attention on CLS token...
  5%|▌         | 1308/25000 [35:49<10:27:29,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:24,913 >> Initializing global attention on CLS token...
  5%|▌         | 1309/25000 [35:51<10:22:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:26,462 >> Initializing global attention on CLS token...
  5%|▌         | 1310/25000 [35:52<10:19:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:28,013 >> Initializing global attention on CLS token...
  5%|▌         | 1311/25000 [35:54<10:20:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:29,591 >> Initializing global attention on CLS token...
  5%|▌         | 1312/25000 [35:55<10:18:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:31,146 >> Initializing global attention on CLS token...
  5%|▌         | 1313/25000 [35:57<10:17:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:32,701 >> Initializing global attention on CLS token...
  5%|▌         | 1314/25000 [35:58<10:16:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:34,399 >> Initializing global attention on CLS token...
  5%|▌         | 1315/25000 [36:00<10:31:47,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:35,949 >> Initializing global attention on CLS token...
  5%|▌         | 1316/25000 [36:02<10:25:46,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:37,499 >> Initializing global attention on CLS token...
  5%|▌         | 1317/25000 [36:03<10:22:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:39,061 >> Initializing global attention on CLS token...
  5%|▌         | 1318/25000 [36:05<10:20:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:40,616 >> Initializing global attention on CLS token...
  5%|▌         | 1319/25000 [36:06<10:17:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:42,168 >> Initializing global attention on CLS token...
  5%|▌         | 1320/25000 [36:08<10:16:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:43,720 >> Initializing global attention on CLS token...
  5%|▌         | 1321/25000 [36:09<10:23:42,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:45,345 >> Initializing global attention on CLS token...
  5%|▌         | 1322/25000 [36:11<10:20:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:46,900 >> Initializing global attention on CLS token...
  5%|▌         | 1323/25000 [36:13<10:18:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:48,453 >> Initializing global attention on CLS token...
  5%|▌         | 1324/25000 [36:14<10:16:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:50,006 >> Initializing global attention on CLS token...
  5%|▌         | 1325/25000 [36:16<10:19:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:51,589 >> Initializing global attention on CLS token...
  5%|▌         | 1326/25000 [36:17<10:16:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:53,139 >> Initializing global attention on CLS token...
  5%|▌         | 1327/25000 [36:19<10:15:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:54,692 >> Initializing global attention on CLS token...
  5%|▌         | 1328/25000 [36:20<10:21:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:56,299 >> Initializing global attention on CLS token...
  5%|▌         | 1329/25000 [36:22<10:18:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:57,850 >> Initializing global attention on CLS token...
  5%|▌         | 1330/25000 [36:24<10:16:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:59,402 >> Initializing global attention on CLS token...
  5%|▌         | 1331/25000 [36:25<10:15:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:00,955 >> Initializing global attention on CLS token...
  5%|▌         | 1332/25000 [36:27<10:20:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:02,557 >> Initializing global attention on CLS token...
  5%|▌         | 1333/25000 [36:28<10:17:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:04,105 >> Initializing global attention on CLS token...
  5%|▌         | 1334/25000 [36:30<10:16:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:05,661 >> Initializing global attention on CLS token...
  5%|▌         | 1335/25000 [36:31<10:18:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:07,241 >> Initializing global attention on CLS token...
  5%|▌         | 1336/25000 [36:33<10:16:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:08,790 >> Initializing global attention on CLS token...
  5%|▌         | 1337/25000 [36:34<10:14:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:10,342 >> Initializing global attention on CLS token...
  5%|▌         | 1338/25000 [36:36<10:13:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:11,894 >> Initializing global attention on CLS token...
  5%|▌         | 1339/25000 [36:38<10:15:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:13,468 >> Initializing global attention on CLS token...
  5%|▌         | 1340/25000 [36:39<10:14:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:15,019 >> Initializing global attention on CLS token...
  5%|▌         | 1341/25000 [36:41<10:13:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:16,569 >> Initializing global attention on CLS token...
  5%|▌         | 1342/25000 [36:42<10:17:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:18,162 >> Initializing global attention on CLS token...
  5%|▌         | 1343/25000 [36:44<10:16:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:19,713 >> Initializing global attention on CLS token...
  5%|▌         | 1344/25000 [36:45<10:14:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:21,266 >> Initializing global attention on CLS token...
  5%|▌         | 1345/25000 [36:47<10:13:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:22,818 >> Initializing global attention on CLS token...
  5%|▌         | 1346/25000 [36:49<10:25:10,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:24,470 >> Initializing global attention on CLS token...
  5%|▌         | 1347/25000 [36:50<10:20:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:26,021 >> Initializing global attention on CLS token...
  5%|▌         | 1348/25000 [36:52<10:17:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:27,570 >> Initializing global attention on CLS token...
  5%|▌         | 1349/25000 [36:53<10:16:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:29,127 >> Initializing global attention on CLS token...
  5%|▌         | 1350/25000 [36:55<10:14:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:30,688 >> Initializing global attention on CLS token...
  5%|▌         | 1351/25000 [36:56<10:15:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:32,240 >> Initializing global attention on CLS token...
  5%|▌         | 1352/25000 [36:58<10:15:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:33,804 >> Initializing global attention on CLS token...
  5%|▌         | 1353/25000 [37:00<10:15:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,366 >> Initializing global attention on CLS token...
  5%|▌         | 1354/25000 [37:01<10:14:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,918 >> Initializing global attention on CLS token...
  5%|▌         | 1355/25000 [37:03<10:13:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,471 >> Initializing global attention on CLS token...
  5%|▌         | 1356/25000 [37:04<10:13:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,029 >> Initializing global attention on CLS token...
  5%|▌         | 1357/25000 [37:06<10:13:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,580 >> Initializing global attention on CLS token...
  5%|▌         | 1358/25000 [37:07<10:13:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:43,136 >> Initializing global attention on CLS token...
  5%|▌         | 1359/25000 [37:09<10:20:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:44,758 >> Initializing global attention on CLS token...
  5%|▌         | 1360/25000 [37:10<10:17:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:46,310 >> Initializing global attention on CLS token...
  5%|▌         | 1361/25000 [37:12<10:15:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:47,859 >> Initializing global attention on CLS token...
  5%|▌         | 1362/25000 [37:14<10:14:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:49,411 >> Initializing global attention on CLS token...
  5%|▌         | 1363/25000 [37:15<10:17:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:51,000 >> Initializing global attention on CLS token...
  5%|▌         | 1364/25000 [37:17<10:15:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:52,551 >> Initializing global attention on CLS token...
  5%|▌         | 1365/25000 [37:18<10:14:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:54,103 >> Initializing global attention on CLS token...
  5%|▌         | 1366/25000 [37:20<10:26:59,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:55,770 >> Initializing global attention on CLS token...
  5%|▌         | 1367/25000 [37:21<10:22:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:57,323 >> Initializing global attention on CLS token...
  5%|▌         | 1368/25000 [37:23<10:18:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:58,874 >> Initializing global attention on CLS token...
  5%|▌         | 1369/25000 [37:25<10:16:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:00,424 >> Initializing global attention on CLS token...
  5%|▌         | 1370/25000 [37:26<10:23:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:02,050 >> Initializing global attention on CLS token...
  5%|▌         | 1371/25000 [37:28<10:19:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:03,605 >> Initializing global attention on CLS token...
  5%|▌         | 1372/25000 [37:29<10:17:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:05,155 >> Initializing global attention on CLS token...
  5%|▌         | 1373/25000 [37:31<10:15:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:06,709 >> Initializing global attention on CLS token...
  5%|▌         | 1374/25000 [37:33<10:29:19,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:08,368 >> Initializing global attention on CLS token...
  6%|▌         | 1375/25000 [37:34<10:21:47,  1.58s/it]                                                         6%|▌         | 1375/25000 [37:34<10:21:47,  1.58s/it][INFO|trainer.py:738] 2024-01-22 00:56:09,904 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:56:09,907 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:56:09,907 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:56:09,908 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:09,920 >> Initializing global attention on CLS token...
{'eval_loss': 4.022804260253906, 'eval_accuracy': 0.348, 'eval_macro_f1': 0.11294999937201772, 'eval_macro_precision': 0.10582036744929581, 'eval_macro_recall': 0.14473011697782337, 'eval_micro_f1': 0.348, 'eval_micro_precision': 0.348, 'eval_micro_recall': 0.348, 'eval_combined_score': 0.25078578339987667, 'eval_runtime': 7.6581, 'eval_samples_per_second': 65.291, 'eval_steps_per_second': 8.227, 'epoch': 10.0}
{'loss': 1.7673, 'learning_rate': 6.615e-05, 'epoch': 11.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,040 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,161 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,283 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,404 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,523 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,644 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,764 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.90it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,885 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,005 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,127 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,247 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,367 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,487 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,608 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,729 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,849 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:11,969 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,089 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,209 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,329 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,451 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,571 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,691 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:06,  6.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,950 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:05,  6.70it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,070 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:05,  7.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,189 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  7.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,309 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  7.69it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,429 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,549 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,668 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,788 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,908 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,028 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,148 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,269 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,389 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,509 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,630 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,750 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,870 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:14,990 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,111 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,231 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,350 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,469 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,589 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,710 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,830 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,951 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,072 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,192 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,312 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,432 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,551 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,671 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,791 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,912 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:17,032 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:17,152 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:17,272 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:17,389 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:17,504 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  6%|▌         | 1375/25000 [37:42<10:21:47,  1.58s/it]
100%|██████████| 63/63 [00:07<00:00,  8.37it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:56:17,589 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1375
[INFO|configuration_utils.py:461] 2024-01-22 00:56:17,596 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1375/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:56:18,244 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1375/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:56:18,246 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1375/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:56:18,246 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1375/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:56:19,490 >> Initializing global attention on CLS token...
  6%|▌         | 1376/25000 [37:45<29:19:26,  4.47s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:21,133 >> Initializing global attention on CLS token...
  6%|▌         | 1377/25000 [37:47<23:34:40,  3.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:22,684 >> Initializing global attention on CLS token...
  6%|▌         | 1378/25000 [37:49<19:48:42,  3.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:24,364 >> Initializing global attention on CLS token...
  6%|▌         | 1379/25000 [37:50<16:55:05,  2.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:25,914 >> Initializing global attention on CLS token...
  6%|▌         | 1380/25000 [37:52<14:53:27,  2.27s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:27,463 >> Initializing global attention on CLS token...
  6%|▌         | 1381/25000 [37:53<13:28:35,  2.05s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:29,014 >> Initializing global attention on CLS token...
  6%|▌         | 1382/25000 [37:55<12:30:07,  1.91s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:30,573 >> Initializing global attention on CLS token...
  6%|▌         | 1383/25000 [37:56<11:47:51,  1.80s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:32,125 >> Initializing global attention on CLS token...
  6%|▌         | 1384/25000 [37:58<11:19:37,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:33,681 >> Initializing global attention on CLS token...
  6%|▌         | 1385/25000 [37:59<11:04:01,  1.69s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:35,276 >> Initializing global attention on CLS token...
  6%|▌         | 1386/25000 [38:01<10:48:06,  1.65s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:36,828 >> Initializing global attention on CLS token...
  6%|▌         | 1387/25000 [38:03<10:36:23,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:38,377 >> Initializing global attention on CLS token...
  6%|▌         | 1388/25000 [38:04<10:28:26,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:39,926 >> Initializing global attention on CLS token...
  6%|▌         | 1389/25000 [38:06<10:23:57,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:41,485 >> Initializing global attention on CLS token...
  6%|▌         | 1390/25000 [38:07<10:19:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:43,036 >> Initializing global attention on CLS token...
  6%|▌         | 1391/25000 [38:09<10:16:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:44,587 >> Initializing global attention on CLS token...
  6%|▌         | 1392/25000 [38:10<10:16:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:46,149 >> Initializing global attention on CLS token...
  6%|▌         | 1393/25000 [38:12<10:14:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:47,697 >> Initializing global attention on CLS token...
  6%|▌         | 1394/25000 [38:13<10:12:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:49,246 >> Initializing global attention on CLS token...
  6%|▌         | 1395/25000 [38:15<10:12:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:50,798 >> Initializing global attention on CLS token...
  6%|▌         | 1396/25000 [38:17<10:23:14,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:52,449 >> Initializing global attention on CLS token...
  6%|▌         | 1397/25000 [38:18<10:19:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:54,005 >> Initializing global attention on CLS token...
  6%|▌         | 1398/25000 [38:20<10:17:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:55,556 >> Initializing global attention on CLS token...
  6%|▌         | 1399/25000 [38:21<10:16:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:57,123 >> Initializing global attention on CLS token...
  6%|▌         | 1400/25000 [38:23<10:14:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:58,674 >> Initializing global attention on CLS token...
  6%|▌         | 1401/25000 [38:24<10:13:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:00,225 >> Initializing global attention on CLS token...
  6%|▌         | 1402/25000 [38:26<10:12:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:01,773 >> Initializing global attention on CLS token...
  6%|▌         | 1403/25000 [38:28<10:21:49,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:03,413 >> Initializing global attention on CLS token...
  6%|▌         | 1404/25000 [38:29<10:18:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:04,964 >> Initializing global attention on CLS token...
  6%|▌         | 1405/25000 [38:31<10:15:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:06,513 >> Initializing global attention on CLS token...
  6%|▌         | 1406/25000 [38:32<10:16:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:08,083 >> Initializing global attention on CLS token...
  6%|▌         | 1407/25000 [38:34<10:14:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:09,632 >> Initializing global attention on CLS token...
  6%|▌         | 1408/25000 [38:35<10:12:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:11,182 >> Initializing global attention on CLS token...
  6%|▌         | 1409/25000 [38:37<10:14:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:12,759 >> Initializing global attention on CLS token...
  6%|▌         | 1410/25000 [38:38<10:15:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:14,330 >> Initializing global attention on CLS token...
  6%|▌         | 1411/25000 [38:40<10:13:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:15,879 >> Initializing global attention on CLS token...
  6%|▌         | 1412/25000 [38:42<10:12:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:17,430 >> Initializing global attention on CLS token...
  6%|▌         | 1413/25000 [38:43<10:12:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:18,991 >> Initializing global attention on CLS token...
  6%|▌         | 1414/25000 [38:45<10:14:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:20,562 >> Initializing global attention on CLS token...
  6%|▌         | 1415/25000 [38:46<10:12:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:22,112 >> Initializing global attention on CLS token...
  6%|▌         | 1416/25000 [38:48<10:15:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:23,696 >> Initializing global attention on CLS token...
  6%|▌         | 1417/25000 [38:49<10:13:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:25,246 >> Initializing global attention on CLS token...
  6%|▌         | 1418/25000 [38:51<10:12:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:26,803 >> Initializing global attention on CLS token...
  6%|▌         | 1419/25000 [38:53<10:12:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:28,355 >> Initializing global attention on CLS token...
  6%|▌         | 1420/25000 [38:54<10:16:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:29,952 >> Initializing global attention on CLS token...
  6%|▌         | 1421/25000 [38:56<10:14:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:31,504 >> Initializing global attention on CLS token...
  6%|▌         | 1422/25000 [38:57<10:14:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:33,062 >> Initializing global attention on CLS token...
  6%|▌         | 1423/25000 [38:59<10:17:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:34,653 >> Initializing global attention on CLS token...
  6%|▌         | 1424/25000 [39:00<10:15:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:36,205 >> Initializing global attention on CLS token...
  6%|▌         | 1425/25000 [39:02<10:14:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:37,766 >> Initializing global attention on CLS token...
  6%|▌         | 1426/25000 [39:03<10:12:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:39,316 >> Initializing global attention on CLS token...
  6%|▌         | 1427/25000 [39:05<10:29:15,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:41,015 >> Initializing global attention on CLS token...
  6%|▌         | 1428/25000 [39:07<10:23:31,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:42,569 >> Initializing global attention on CLS token...
  6%|▌         | 1429/25000 [39:08<10:18:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:44,117 >> Initializing global attention on CLS token...
  6%|▌         | 1430/25000 [39:10<10:18:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:45,690 >> Initializing global attention on CLS token...
  6%|▌         | 1431/25000 [39:11<10:16:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:47,245 >> Initializing global attention on CLS token...
  6%|▌         | 1432/25000 [39:13<10:14:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:48,795 >> Initializing global attention on CLS token...
  6%|▌         | 1433/25000 [39:14<10:12:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:50,345 >> Initializing global attention on CLS token...
  6%|▌         | 1434/25000 [39:16<10:18:30,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:51,956 >> Initializing global attention on CLS token...
  6%|▌         | 1435/25000 [39:18<10:16:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:53,512 >> Initializing global attention on CLS token...
  6%|▌         | 1436/25000 [39:19<10:14:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:55,063 >> Initializing global attention on CLS token...
  6%|▌         | 1437/25000 [39:21<10:14:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:56,632 >> Initializing global attention on CLS token...
  6%|▌         | 1438/25000 [39:22<10:12:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:58,183 >> Initializing global attention on CLS token...
  6%|▌         | 1439/25000 [39:24<10:12:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:59,736 >> Initializing global attention on CLS token...
  6%|▌         | 1440/25000 [39:25<10:11:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:01,425 >> Initializing global attention on CLS token...
  6%|▌         | 1441/25000 [39:27<10:26:29,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:02,975 >> Initializing global attention on CLS token...
  6%|▌         | 1442/25000 [39:29<10:21:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:04,524 >> Initializing global attention on CLS token...
  6%|▌         | 1443/25000 [39:30<10:17:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:06,074 >> Initializing global attention on CLS token...
  6%|▌         | 1444/25000 [39:32<10:18:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:07,656 >> Initializing global attention on CLS token...
  6%|▌         | 1445/25000 [39:33<10:15:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:09,207 >> Initializing global attention on CLS token...
  6%|▌         | 1446/25000 [39:35<10:13:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:10,760 >> Initializing global attention on CLS token...
  6%|▌         | 1447/25000 [39:37<10:20:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:12,377 >> Initializing global attention on CLS token...
  6%|▌         | 1448/25000 [39:38<10:16:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:13,927 >> Initializing global attention on CLS token...
  6%|▌         | 1449/25000 [39:40<10:14:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:15,479 >> Initializing global attention on CLS token...
  6%|▌         | 1450/25000 [39:41<10:12:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:17,031 >> Initializing global attention on CLS token...
  6%|▌         | 1451/25000 [39:43<10:24:15,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:18,690 >> Initializing global attention on CLS token...
  6%|▌         | 1452/25000 [39:44<10:19:25,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:20,240 >> Initializing global attention on CLS token...
  6%|▌         | 1453/25000 [39:46<10:16:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:21,791 >> Initializing global attention on CLS token...
  6%|▌         | 1454/25000 [39:48<10:17:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:23,372 >> Initializing global attention on CLS token...
  6%|▌         | 1455/25000 [39:49<10:14:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:24,922 >> Initializing global attention on CLS token...
  6%|▌         | 1456/25000 [39:51<10:12:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:26,473 >> Initializing global attention on CLS token...
  6%|▌         | 1457/25000 [39:52<10:11:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:28,027 >> Initializing global attention on CLS token...
  6%|▌         | 1458/25000 [39:54<10:26:00,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:29,707 >> Initializing global attention on CLS token...
  6%|▌         | 1459/25000 [39:55<10:20:45,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:31,258 >> Initializing global attention on CLS token...
  6%|▌         | 1460/25000 [39:57<10:17:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:32,829 >> Initializing global attention on CLS token...
  6%|▌         | 1461/25000 [39:59<10:18:29,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:34,394 >> Initializing global attention on CLS token...
  6%|▌         | 1462/25000 [40:00<10:15:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:35,945 >> Initializing global attention on CLS token...
  6%|▌         | 1463/25000 [40:02<10:13:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:37,495 >> Initializing global attention on CLS token...
  6%|▌         | 1464/25000 [40:03<10:13:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:39,057 >> Initializing global attention on CLS token...
  6%|▌         | 1465/25000 [40:05<10:11:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:40,607 >> Initializing global attention on CLS token...
  6%|▌         | 1466/25000 [40:06<10:10:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:42,156 >> Initializing global attention on CLS token...
  6%|▌         | 1467/25000 [40:08<10:10:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:43,710 >> Initializing global attention on CLS token...
  6%|▌         | 1468/25000 [40:09<10:17:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:45,330 >> Initializing global attention on CLS token...
  6%|▌         | 1469/25000 [40:11<10:15:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:46,887 >> Initializing global attention on CLS token...
  6%|▌         | 1470/25000 [40:13<10:13:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:48,440 >> Initializing global attention on CLS token...
  6%|▌         | 1471/25000 [40:14<10:14:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:50,014 >> Initializing global attention on CLS token...
  6%|▌         | 1472/25000 [40:16<10:12:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:51,567 >> Initializing global attention on CLS token...
  6%|▌         | 1473/25000 [40:17<10:11:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:53,121 >> Initializing global attention on CLS token...
  6%|▌         | 1474/25000 [40:19<10:11:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:54,674 >> Initializing global attention on CLS token...
  6%|▌         | 1475/25000 [40:20<10:17:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:56,285 >> Initializing global attention on CLS token...
  6%|▌         | 1476/25000 [40:22<10:14:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:57,837 >> Initializing global attention on CLS token...
  6%|▌         | 1477/25000 [40:24<10:12:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,388 >> Initializing global attention on CLS token...
  6%|▌         | 1478/25000 [40:25<10:21:10,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,024 >> Initializing global attention on CLS token...
  6%|▌         | 1479/25000 [40:27<10:17:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,576 >> Initializing global attention on CLS token...
  6%|▌         | 1480/25000 [40:28<10:14:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,128 >> Initializing global attention on CLS token...
  6%|▌         | 1481/25000 [40:30<10:13:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,685 >> Initializing global attention on CLS token...
  6%|▌         | 1482/25000 [40:31<10:22:01,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:07,322 >> Initializing global attention on CLS token...
  6%|▌         | 1483/25000 [40:33<10:17:38,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:08,873 >> Initializing global attention on CLS token...
  6%|▌         | 1484/25000 [40:35<10:14:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:10,424 >> Initializing global attention on CLS token...
  6%|▌         | 1485/25000 [40:36<10:17:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:12,017 >> Initializing global attention on CLS token...
  6%|▌         | 1486/25000 [40:38<10:14:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:13,567 >> Initializing global attention on CLS token...
  6%|▌         | 1487/25000 [40:39<10:12:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:15,116 >> Initializing global attention on CLS token...
  6%|▌         | 1488/25000 [40:41<10:10:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:16,666 >> Initializing global attention on CLS token...
  6%|▌         | 1489/25000 [40:43<10:26:29,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:18,358 >> Initializing global attention on CLS token...
  6%|▌         | 1490/25000 [40:44<10:20:52,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:19,910 >> Initializing global attention on CLS token...
  6%|▌         | 1491/25000 [40:46<10:17:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:21,462 >> Initializing global attention on CLS token...
  6%|▌         | 1492/25000 [40:47<10:19:40,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:23,059 >> Initializing global attention on CLS token...
  6%|▌         | 1493/25000 [40:49<10:16:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:24,610 >> Initializing global attention on CLS token...
  6%|▌         | 1494/25000 [40:50<10:13:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:26,159 >> Initializing global attention on CLS token...
  6%|▌         | 1495/25000 [40:52<10:11:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:27,708 >> Initializing global attention on CLS token...
  6%|▌         | 1496/25000 [40:53<10:20:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:29,348 >> Initializing global attention on CLS token...
  6%|▌         | 1497/25000 [40:55<10:16:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:30,900 >> Initializing global attention on CLS token...
  6%|▌         | 1498/25000 [40:57<10:13:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:32,450 >> Initializing global attention on CLS token...
  6%|▌         | 1499/25000 [40:58<10:11:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:33,981 >> Initializing global attention on CLS token...
  6%|▌         | 1500/25000 [41:00<10:18:39,  1.58s/it]                                                         6%|▌         | 1500/25000 [41:00<10:18:39,  1.58s/it][INFO|trainer.py:738] 2024-01-22 00:59:35,601 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:59:35,604 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:59:35,604 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:59:35,604 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:35,616 >> Initializing global attention on CLS token...
{'eval_loss': 4.007693290710449, 'eval_accuracy': 0.366, 'eval_macro_f1': 0.1303620713343708, 'eval_macro_precision': 0.12018114941816241, 'eval_macro_recall': 0.16540657154293517, 'eval_micro_f1': 0.366, 'eval_micro_precision': 0.366, 'eval_micro_recall': 0.366, 'eval_combined_score': 0.2685642560422098, 'eval_runtime': 7.68, 'eval_samples_per_second': 65.104, 'eval_steps_per_second': 8.203, 'epoch': 11.0}
{'loss': 1.5641, 'learning_rate': 6.579999999999999e-05, 'epoch': 12.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:35,737 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.77it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:35,857 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:35,977 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,099 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,219 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,339 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,460 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,580 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,700 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.66it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,820 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.54it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:36,943 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,063 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,184 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,304 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,424 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,544 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,663 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,787 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,906 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,024 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,144 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,264 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,384 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,504 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,624 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,746 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,866 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,986 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,106 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,227 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,347 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,466 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,588 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,708 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:39,828 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:04,  6.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,068 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  6.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,188 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  7.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,308 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:03,  7.54it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,429 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  7.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,549 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  7.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,670 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,790 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,911 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,031 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,150 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,270 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,390 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,509 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,629 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,750 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,870 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:41,990 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,109 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,230 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,350 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,470 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,590 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,709 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,829 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,948 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:43,066 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:43,180 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  6%|▌         | 1500/25000 [41:07<10:18:39,  1.58s/it]
100%|██████████| 63/63 [00:07<00:00,  8.41it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:59:43,264 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1500
[INFO|configuration_utils.py:461] 2024-01-22 00:59:43,270 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1500/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:59:43,873 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:59:43,874 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:59:43,874 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1500/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:45,012 >> Initializing global attention on CLS token...
  6%|▌         | 1501/25000 [41:11<28:40:29,  4.39s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:46,578 >> Initializing global attention on CLS token...
  6%|▌         | 1502/25000 [41:12<23:05:53,  3.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:48,124 >> Initializing global attention on CLS token...
  6%|▌         | 1503/25000 [41:14<19:12:06,  2.94s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:49,673 >> Initializing global attention on CLS token...
  6%|▌         | 1504/25000 [41:15<16:36:47,  2.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:51,293 >> Initializing global attention on CLS token...
  6%|▌         | 1505/25000 [41:17<14:39:44,  2.25s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:52,843 >> Initializing global attention on CLS token...
  6%|▌         | 1506/25000 [41:19<13:18:41,  2.04s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:54,402 >> Initializing global attention on CLS token...
  6%|▌         | 1507/25000 [41:20<12:21:46,  1.89s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:55,955 >> Initializing global attention on CLS token...
  6%|▌         | 1508/25000 [41:22<11:47:19,  1.81s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:57,557 >> Initializing global attention on CLS token...
  6%|▌         | 1509/25000 [41:23<11:17:44,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:59,111 >> Initializing global attention on CLS token...
  6%|▌         | 1510/25000 [41:25<10:57:18,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:00,669 >> Initializing global attention on CLS token...
  6%|▌         | 1511/25000 [41:26<10:49:15,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:02,280 >> Initializing global attention on CLS token...
  6%|▌         | 1512/25000 [41:28<10:36:34,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:03,829 >> Initializing global attention on CLS token...
  6%|▌         | 1513/25000 [41:30<10:29:32,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:05,401 >> Initializing global attention on CLS token...
  6%|▌         | 1514/25000 [41:31<10:24:07,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:06,961 >> Initializing global attention on CLS token...
  6%|▌         | 1515/25000 [41:33<10:22:39,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:08,541 >> Initializing global attention on CLS token...
  6%|▌         | 1516/25000 [41:34<10:18:56,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:10,106 >> Initializing global attention on CLS token...
  6%|▌         | 1517/25000 [41:36<10:17:11,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:11,667 >> Initializing global attention on CLS token...
  6%|▌         | 1518/25000 [41:37<10:14:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:13,220 >> Initializing global attention on CLS token...
  6%|▌         | 1519/25000 [41:39<10:17:19,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:14,816 >> Initializing global attention on CLS token...
  6%|▌         | 1520/25000 [41:41<10:14:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:16,369 >> Initializing global attention on CLS token...
  6%|▌         | 1521/25000 [41:42<10:12:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:17,924 >> Initializing global attention on CLS token...
  6%|▌         | 1522/25000 [41:44<10:18:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:19,534 >> Initializing global attention on CLS token...
  6%|▌         | 1523/25000 [41:45<10:15:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:21,091 >> Initializing global attention on CLS token...
  6%|▌         | 1524/25000 [41:47<10:14:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:22,659 >> Initializing global attention on CLS token...
  6%|▌         | 1525/25000 [41:48<10:15:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:24,235 >> Initializing global attention on CLS token...
  6%|▌         | 1526/25000 [41:50<10:20:31,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:25,853 >> Initializing global attention on CLS token...
  6%|▌         | 1527/25000 [41:52<10:16:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:27,407 >> Initializing global attention on CLS token...
  6%|▌         | 1528/25000 [41:53<10:14:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:28,973 >> Initializing global attention on CLS token...
  6%|▌         | 1529/25000 [41:55<10:30:37,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:30,687 >> Initializing global attention on CLS token...
  6%|▌         | 1530/25000 [41:56<10:27:07,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:32,255 >> Initializing global attention on CLS token...
  6%|▌         | 1531/25000 [41:58<10:21:28,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:33,810 >> Initializing global attention on CLS token...
  6%|▌         | 1532/25000 [42:00<10:17:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:35,367 >> Initializing global attention on CLS token...
  6%|▌         | 1533/25000 [42:01<10:18:18,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:36,951 >> Initializing global attention on CLS token...
  6%|▌         | 1534/25000 [42:03<10:15:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:38,511 >> Initializing global attention on CLS token...
  6%|▌         | 1535/25000 [42:04<10:13:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:40,068 >> Initializing global attention on CLS token...
  6%|▌         | 1536/25000 [42:06<10:12:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:41,624 >> Initializing global attention on CLS token...
  6%|▌         | 1537/25000 [42:07<10:24:44,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:43,304 >> Initializing global attention on CLS token...
  6%|▌         | 1538/25000 [42:09<10:21:56,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:44,871 >> Initializing global attention on CLS token...
  6%|▌         | 1539/25000 [42:11<10:19:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:46,443 >> Initializing global attention on CLS token...
  6%|▌         | 1540/25000 [42:12<10:18:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:48,014 >> Initializing global attention on CLS token...
  6%|▌         | 1541/25000 [42:14<10:15:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:49,570 >> Initializing global attention on CLS token...
  6%|▌         | 1542/25000 [42:15<10:15:30,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:51,159 >> Initializing global attention on CLS token...
  6%|▌         | 1543/25000 [42:17<10:16:29,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:52,742 >> Initializing global attention on CLS token...
  6%|▌         | 1544/25000 [42:18<10:18:55,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:54,330 >> Initializing global attention on CLS token...
  6%|▌         | 1545/25000 [42:20<10:17:27,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:55,900 >> Initializing global attention on CLS token...
  6%|▌         | 1546/25000 [42:22<10:15:54,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:57,465 >> Initializing global attention on CLS token...
  6%|▌         | 1547/25000 [42:23<10:13:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:59,024 >> Initializing global attention on CLS token...
  6%|▌         | 1548/25000 [42:25<10:20:08,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:00,647 >> Initializing global attention on CLS token...
  6%|▌         | 1549/25000 [42:26<10:16:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:02,205 >> Initializing global attention on CLS token...
  6%|▌         | 1550/25000 [42:28<10:22:21,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:03,832 >> Initializing global attention on CLS token...
  6%|▌         | 1551/25000 [42:30<10:21:23,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:05,416 >> Initializing global attention on CLS token...
  6%|▌         | 1552/25000 [42:31<10:18:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:06,982 >> Initializing global attention on CLS token...
  6%|▌         | 1553/25000 [42:33<10:15:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:08,540 >> Initializing global attention on CLS token...
  6%|▌         | 1554/25000 [42:34<10:13:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:10,098 >> Initializing global attention on CLS token...
  6%|▌         | 1555/25000 [42:36<10:19:01,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:11,714 >> Initializing global attention on CLS token...
  6%|▌         | 1556/25000 [42:37<10:16:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:13,277 >> Initializing global attention on CLS token...
  6%|▌         | 1557/25000 [42:39<10:16:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:14,853 >> Initializing global attention on CLS token...
  6%|▌         | 1558/25000 [42:41<10:16:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:16,430 >> Initializing global attention on CLS token...
  6%|▌         | 1559/25000 [42:42<10:23:48,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:18,072 >> Initializing global attention on CLS token...
  6%|▌         | 1560/25000 [42:44<10:21:19,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:19,648 >> Initializing global attention on CLS token...
  6%|▌         | 1561/25000 [42:45<10:17:42,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:21,209 >> Initializing global attention on CLS token...
  6%|▌         | 1562/25000 [42:47<10:19:25,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:22,805 >> Initializing global attention on CLS token...
  6%|▋         | 1563/25000 [42:49<10:16:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:24,362 >> Initializing global attention on CLS token...
  6%|▋         | 1564/25000 [42:50<10:16:28,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:25,944 >> Initializing global attention on CLS token...
  6%|▋         | 1565/25000 [42:52<10:14:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:27,503 >> Initializing global attention on CLS token...
  6%|▋         | 1566/25000 [42:53<10:14:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:29,081 >> Initializing global attention on CLS token...
  6%|▋         | 1567/25000 [42:55<10:13:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:30,641 >> Initializing global attention on CLS token...
  6%|▋         | 1568/25000 [42:56<10:15:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:32,234 >> Initializing global attention on CLS token...
  6%|▋         | 1569/25000 [42:58<10:14:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:33,801 >> Initializing global attention on CLS token...
  6%|▋         | 1570/25000 [43:00<10:12:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:35,360 >> Initializing global attention on CLS token...
  6%|▋         | 1571/25000 [43:01<10:18:08,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:36,975 >> Initializing global attention on CLS token...
  6%|▋         | 1572/25000 [43:03<10:16:19,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:38,542 >> Initializing global attention on CLS token...
  6%|▋         | 1573/25000 [43:04<10:15:06,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:40,110 >> Initializing global attention on CLS token...
  6%|▋         | 1574/25000 [43:06<10:14:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:41,681 >> Initializing global attention on CLS token...
  6%|▋         | 1575/25000 [43:07<10:15:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:43,267 >> Initializing global attention on CLS token...
  6%|▋         | 1576/25000 [43:09<10:14:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:44,834 >> Initializing global attention on CLS token...
  6%|▋         | 1577/25000 [43:11<10:12:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:46,395 >> Initializing global attention on CLS token...
  6%|▋         | 1578/25000 [43:12<10:13:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:47,970 >> Initializing global attention on CLS token...
  6%|▋         | 1579/25000 [43:14<10:16:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:49,567 >> Initializing global attention on CLS token...
  6%|▋         | 1580/25000 [43:15<10:14:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:51,129 >> Initializing global attention on CLS token...
  6%|▋         | 1581/25000 [43:17<10:13:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:52,691 >> Initializing global attention on CLS token...
  6%|▋         | 1582/25000 [43:18<10:13:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:54,270 >> Initializing global attention on CLS token...
  6%|▋         | 1583/25000 [43:20<10:13:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:55,842 >> Initializing global attention on CLS token...
  6%|▋         | 1584/25000 [43:22<10:14:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:57,424 >> Initializing global attention on CLS token...
  6%|▋         | 1585/25000 [43:23<10:13:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:58,985 >> Initializing global attention on CLS token...
  6%|▋         | 1586/25000 [43:25<10:14:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:00,563 >> Initializing global attention on CLS token...
  6%|▋         | 1587/25000 [43:26<10:13:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:02,151 >> Initializing global attention on CLS token...
  6%|▋         | 1588/25000 [43:28<10:13:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:03,709 >> Initializing global attention on CLS token...
  6%|▋         | 1589/25000 [43:30<10:27:07,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:05,395 >> Initializing global attention on CLS token...
  6%|▋         | 1590/25000 [43:31<10:22:00,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:06,960 >> Initializing global attention on CLS token...
  6%|▋         | 1591/25000 [43:33<10:17:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:08,519 >> Initializing global attention on CLS token...
  6%|▋         | 1592/25000 [43:34<10:14:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:10,078 >> Initializing global attention on CLS token...
  6%|▋         | 1593/25000 [43:36<10:17:27,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:11,676 >> Initializing global attention on CLS token...
  6%|▋         | 1594/25000 [43:37<10:15:04,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:13,261 >> Initializing global attention on CLS token...
  6%|▋         | 1595/25000 [43:39<10:18:50,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:14,847 >> Initializing global attention on CLS token...
  6%|▋         | 1596/25000 [43:41<10:15:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:16,405 >> Initializing global attention on CLS token...
  6%|▋         | 1597/25000 [43:42<10:26:09,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:18,091 >> Initializing global attention on CLS token...
  6%|▋         | 1598/25000 [43:44<10:24:14,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:19,664 >> Initializing global attention on CLS token...
  6%|▋         | 1599/25000 [43:45<10:21:46,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:21,243 >> Initializing global attention on CLS token...
  6%|▋         | 1600/25000 [43:47<10:26:58,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:22,883 >> Initializing global attention on CLS token...
  6%|▋         | 1601/25000 [43:49<10:21:50,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,447 >> Initializing global attention on CLS token...
  6%|▋         | 1602/25000 [43:50<10:19:16,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,019 >> Initializing global attention on CLS token...
  6%|▋         | 1603/25000 [43:52<10:15:47,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,578 >> Initializing global attention on CLS token...
  6%|▋         | 1604/25000 [43:53<10:20:08,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,194 >> Initializing global attention on CLS token...
  6%|▋         | 1605/25000 [43:55<10:17:43,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:30,766 >> Initializing global attention on CLS token...
  6%|▋         | 1606/25000 [43:57<10:18:32,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:32,356 >> Initializing global attention on CLS token...
  6%|▋         | 1607/25000 [43:58<10:24:29,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:34,009 >> Initializing global attention on CLS token...
  6%|▋         | 1608/25000 [44:00<10:22:51,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:35,578 >> Initializing global attention on CLS token...
  6%|▋         | 1609/25000 [44:01<10:18:20,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:37,140 >> Initializing global attention on CLS token...
  6%|▋         | 1610/25000 [44:03<10:15:20,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:38,703 >> Initializing global attention on CLS token...
  6%|▋         | 1611/25000 [44:04<10:15:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:40,285 >> Initializing global attention on CLS token...
  6%|▋         | 1612/25000 [44:06<10:13:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:41,844 >> Initializing global attention on CLS token...
  6%|▋         | 1613/25000 [44:08<10:11:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:43,404 >> Initializing global attention on CLS token...
  6%|▋         | 1614/25000 [44:09<10:12:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:44,980 >> Initializing global attention on CLS token...
  6%|▋         | 1615/25000 [44:11<10:11:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:46,541 >> Initializing global attention on CLS token...
  6%|▋         | 1616/25000 [44:12<10:10:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:48,101 >> Initializing global attention on CLS token...
  6%|▋         | 1617/25000 [44:14<10:09:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:49,658 >> Initializing global attention on CLS token...
  6%|▋         | 1618/25000 [44:15<10:15:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:51,279 >> Initializing global attention on CLS token...
  6%|▋         | 1619/25000 [44:17<10:12:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:52,834 >> Initializing global attention on CLS token...
  6%|▋         | 1620/25000 [44:19<10:10:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:54,390 >> Initializing global attention on CLS token...
  6%|▋         | 1621/25000 [44:20<10:16:28,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:56,006 >> Initializing global attention on CLS token...
  6%|▋         | 1622/25000 [44:22<10:13:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:57,560 >> Initializing global attention on CLS token...
  6%|▋         | 1623/25000 [44:23<10:10:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:59,114 >> Initializing global attention on CLS token...
  6%|▋         | 1624/25000 [44:25<10:09:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:00,649 >> Initializing global attention on CLS token...
  6%|▋         | 1625/25000 [44:26<10:12:07,  1.57s/it]                                                         6%|▋         | 1625/25000 [44:26<10:12:07,  1.57s/it][INFO|trainer.py:738] 2024-01-22 01:03:02,239 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:03:02,242 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:03:02,242 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:03:02,242 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,254 >> Initializing global attention on CLS token...
{'eval_loss': 4.03691291809082, 'eval_accuracy': 0.35, 'eval_macro_f1': 0.12180499079842808, 'eval_macro_precision': 0.11128877870757888, 'eval_macro_recall': 0.15374897059028794, 'eval_micro_f1': 0.35, 'eval_micro_precision': 0.35, 'eval_micro_recall': 0.35, 'eval_combined_score': 0.255263248585185, 'eval_runtime': 7.6584, 'eval_samples_per_second': 65.288, 'eval_steps_per_second': 8.226, 'epoch': 12.0}
{'loss': 1.3671, 'learning_rate': 6.544999999999999e-05, 'epoch': 13.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,375 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,495 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,615 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,736 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,855 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:02,976 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,095 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.92it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,217 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,337 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.64it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,459 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.54it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,579 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,704 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,826 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,947 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,067 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,187 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,308 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,428 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,548 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,668 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,789 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,908 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,029 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,152 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,273 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,397 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,518 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,640 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,761 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:05,882 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,004 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,126 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:04,  6.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,380 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:04,  6.69it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,502 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  7.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,654 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  6.90it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,778 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  7.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,899 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  7.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,021 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:03,  7.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,144 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  7.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,267 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:05<00:02,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,389 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,510 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,633 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,755 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:07,877 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,006 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,126 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,246 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,366 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,487 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,609 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,729 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,849 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,970 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,091 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,212 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,332 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,452 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,572 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,693 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,810 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,925 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  6%|▋         | 1625/25000 [44:34<10:12:07,  1.57s/it]
100%|██████████| 63/63 [00:07<00:00,  8.37it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:03:10,012 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1625
[INFO|configuration_utils.py:461] 2024-01-22 01:03:10,024 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1625/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:03:10,613 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1625/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:03:10,614 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1625/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:03:10,614 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1625/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 01:03:11,891 >> Initializing global attention on CLS token...
  7%|▋         | 1626/25000 [44:38<29:03:52,  4.48s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:13,512 >> Initializing global attention on CLS token...
  7%|▋         | 1627/25000 [44:39<23:21:52,  3.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:15,063 >> Initializing global attention on CLS token...
  7%|▋         | 1628/25000 [44:41<19:22:45,  2.98s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:16,616 >> Initializing global attention on CLS token...
  7%|▋         | 1629/25000 [44:42<16:40:10,  2.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:18,212 >> Initializing global attention on CLS token...
  7%|▋         | 1630/25000 [44:44<14:42:09,  2.26s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:19,768 >> Initializing global attention on CLS token...
  7%|▋         | 1631/25000 [44:45<13:19:09,  2.05s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:21,326 >> Initializing global attention on CLS token...
  7%|▋         | 1632/25000 [44:47<12:21:40,  1.90s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:22,881 >> Initializing global attention on CLS token...
  7%|▋         | 1633/25000 [44:49<11:55:00,  1.84s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:24,560 >> Initializing global attention on CLS token...
  7%|▋         | 1634/25000 [44:50<11:22:19,  1.75s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:26,116 >> Initializing global attention on CLS token...
  7%|▋         | 1635/25000 [44:52<10:59:22,  1.69s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:27,675 >> Initializing global attention on CLS token...
  7%|▋         | 1636/25000 [44:53<10:47:52,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:29,267 >> Initializing global attention on CLS token...
  7%|▋         | 1637/25000 [44:55<10:35:13,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:30,824 >> Initializing global attention on CLS token...
  7%|▋         | 1638/25000 [44:57<10:26:28,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:32,379 >> Initializing global attention on CLS token...
  7%|▋         | 1639/25000 [44:58<10:19:55,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:33,932 >> Initializing global attention on CLS token...
  7%|▋         | 1640/25000 [45:00<10:28:45,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:35,600 >> Initializing global attention on CLS token...
  7%|▋         | 1641/25000 [45:01<10:21:24,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:37,152 >> Initializing global attention on CLS token...
  7%|▋         | 1642/25000 [45:03<10:16:28,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:38,707 >> Initializing global attention on CLS token...
  7%|▋         | 1643/25000 [45:04<10:17:43,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:40,302 >> Initializing global attention on CLS token...
  7%|▋         | 1644/25000 [45:06<10:13:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:41,855 >> Initializing global attention on CLS token...
  7%|▋         | 1645/25000 [45:08<10:11:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:43,410 >> Initializing global attention on CLS token...
  7%|▋         | 1646/25000 [45:09<10:09:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:44,967 >> Initializing global attention on CLS token...
  7%|▋         | 1647/25000 [45:11<10:10:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:46,540 >> Initializing global attention on CLS token...
  7%|▋         | 1648/25000 [45:12<10:08:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:48,093 >> Initializing global attention on CLS token...
  7%|▋         | 1649/25000 [45:14<10:07:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:49,648 >> Initializing global attention on CLS token...
  7%|▋         | 1650/25000 [45:15<10:08:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:51,215 >> Initializing global attention on CLS token...
  7%|▋         | 1651/25000 [45:17<10:07:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:52,771 >> Initializing global attention on CLS token...
  7%|▋         | 1652/25000 [45:18<10:07:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:54,332 >> Initializing global attention on CLS token...
  7%|▋         | 1653/25000 [45:20<10:06:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:55,890 >> Initializing global attention on CLS token...
  7%|▋         | 1654/25000 [45:22<10:13:27,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:57,506 >> Initializing global attention on CLS token...
  7%|▋         | 1655/25000 [45:23<10:11:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:59,063 >> Initializing global attention on CLS token...
  7%|▋         | 1656/25000 [45:25<10:09:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:00,617 >> Initializing global attention on CLS token...
  7%|▋         | 1657/25000 [45:26<10:09:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:02,188 >> Initializing global attention on CLS token...
  7%|▋         | 1658/25000 [45:28<10:08:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:03,742 >> Initializing global attention on CLS token...
  7%|▋         | 1659/25000 [45:29<10:06:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:05,295 >> Initializing global attention on CLS token...
  7%|▋         | 1660/25000 [45:31<10:06:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:06,928 >> Initializing global attention on CLS token...
  7%|▋         | 1661/25000 [45:33<10:21:12,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:08,538 >> Initializing global attention on CLS token...
  7%|▋         | 1662/25000 [45:34<10:16:02,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:10,091 >> Initializing global attention on CLS token...
  7%|▋         | 1663/25000 [45:36<10:12:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:11,644 >> Initializing global attention on CLS token...
  7%|▋         | 1664/25000 [45:37<10:17:20,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:13,261 >> Initializing global attention on CLS token...
  7%|▋         | 1665/25000 [45:39<10:13:36,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:14,816 >> Initializing global attention on CLS token...
  7%|▋         | 1666/25000 [45:41<10:10:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:16,369 >> Initializing global attention on CLS token...
  7%|▋         | 1667/25000 [45:42<10:14:46,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:17,975 >> Initializing global attention on CLS token...
  7%|▋         | 1668/25000 [45:44<10:11:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:19,532 >> Initializing global attention on CLS token...
  7%|▋         | 1669/25000 [45:45<10:09:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:21,085 >> Initializing global attention on CLS token...
  7%|▋         | 1670/25000 [45:47<10:07:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:22,639 >> Initializing global attention on CLS token...
  7%|▋         | 1671/25000 [45:48<10:14:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:24,261 >> Initializing global attention on CLS token...
  7%|▋         | 1672/25000 [45:50<10:11:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:25,814 >> Initializing global attention on CLS token...
  7%|▋         | 1673/25000 [45:52<10:09:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:27,402 >> Initializing global attention on CLS token...
  7%|▋         | 1674/25000 [45:53<10:12:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:28,964 >> Initializing global attention on CLS token...
  7%|▋         | 1675/25000 [45:55<10:10:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:30,518 >> Initializing global attention on CLS token...
  7%|▋         | 1676/25000 [45:56<10:08:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:32,073 >> Initializing global attention on CLS token...
  7%|▋         | 1677/25000 [45:58<10:10:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:33,661 >> Initializing global attention on CLS token...
  7%|▋         | 1678/25000 [45:59<10:09:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:35,218 >> Initializing global attention on CLS token...
  7%|▋         | 1679/25000 [46:01<10:07:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:36,773 >> Initializing global attention on CLS token...
  7%|▋         | 1680/25000 [46:02<10:07:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:38,333 >> Initializing global attention on CLS token...
  7%|▋         | 1681/25000 [46:04<10:11:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:39,934 >> Initializing global attention on CLS token...
  7%|▋         | 1682/25000 [46:06<10:09:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:41,488 >> Initializing global attention on CLS token...
  7%|▋         | 1683/25000 [46:07<10:07:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:43,045 >> Initializing global attention on CLS token...
  7%|▋         | 1684/25000 [46:09<10:12:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:44,652 >> Initializing global attention on CLS token...
  7%|▋         | 1685/25000 [46:10<10:10:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:46,209 >> Initializing global attention on CLS token...
  7%|▋         | 1686/25000 [46:12<10:08:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:47,766 >> Initializing global attention on CLS token...
  7%|▋         | 1687/25000 [46:13<10:07:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:49,322 >> Initializing global attention on CLS token...
  7%|▋         | 1688/25000 [46:15<10:09:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:50,903 >> Initializing global attention on CLS token...
  7%|▋         | 1689/25000 [46:17<10:07:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:52,457 >> Initializing global attention on CLS token...
  7%|▋         | 1690/25000 [46:18<10:06:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:54,012 >> Initializing global attention on CLS token...
  7%|▋         | 1691/25000 [46:20<10:14:21,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:55,639 >> Initializing global attention on CLS token...
  7%|▋         | 1692/25000 [46:21<10:11:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:57,210 >> Initializing global attention on CLS token...
  7%|▋         | 1693/25000 [46:23<10:11:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:58,775 >> Initializing global attention on CLS token...
  7%|▋         | 1694/25000 [46:25<10:11:47,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:00,346 >> Initializing global attention on CLS token...
  7%|▋         | 1695/25000 [46:26<10:09:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:01,902 >> Initializing global attention on CLS token...
  7%|▋         | 1696/25000 [46:28<10:07:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:03,460 >> Initializing global attention on CLS token...
  7%|▋         | 1697/25000 [46:29<10:06:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:05,012 >> Initializing global attention on CLS token...
  7%|▋         | 1698/25000 [46:31<10:06:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:06,577 >> Initializing global attention on CLS token...
  7%|▋         | 1699/25000 [46:32<10:05:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:08,131 >> Initializing global attention on CLS token...
  7%|▋         | 1700/25000 [46:34<10:05:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:09,687 >> Initializing global attention on CLS token...
  7%|▋         | 1701/25000 [46:35<10:13:01,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:11,312 >> Initializing global attention on CLS token...
  7%|▋         | 1702/25000 [46:37<10:10:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:12,868 >> Initializing global attention on CLS token...
  7%|▋         | 1703/25000 [46:39<10:08:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:14,421 >> Initializing global attention on CLS token...
  7%|▋         | 1704/25000 [46:40<10:06:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:15,978 >> Initializing global attention on CLS token...
  7%|▋         | 1705/25000 [46:42<10:13:02,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:17,595 >> Initializing global attention on CLS token...
  7%|▋         | 1706/25000 [46:43<10:10:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:19,150 >> Initializing global attention on CLS token...
  7%|▋         | 1707/25000 [46:45<10:08:30,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:20,706 >> Initializing global attention on CLS token...
  7%|▋         | 1708/25000 [46:46<10:13:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:22,318 >> Initializing global attention on CLS token...
  7%|▋         | 1709/25000 [46:48<10:10:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:23,871 >> Initializing global attention on CLS token...
  7%|▋         | 1710/25000 [46:50<10:08:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:25,430 >> Initializing global attention on CLS token...
  7%|▋         | 1711/25000 [46:51<10:14:40,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:27,049 >> Initializing global attention on CLS token...
  7%|▋         | 1712/25000 [46:53<10:11:19,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:28,605 >> Initializing global attention on CLS token...
  7%|▋         | 1713/25000 [46:54<10:09:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:30,166 >> Initializing global attention on CLS token...
  7%|▋         | 1714/25000 [46:56<10:08:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:31,725 >> Initializing global attention on CLS token...
  7%|▋         | 1715/25000 [46:57<10:08:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:33,295 >> Initializing global attention on CLS token...
  7%|▋         | 1716/25000 [46:59<10:07:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:34,855 >> Initializing global attention on CLS token...
  7%|▋         | 1717/25000 [47:01<10:06:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:36,411 >> Initializing global attention on CLS token...
  7%|▋         | 1718/25000 [47:02<10:09:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:38,002 >> Initializing global attention on CLS token...
  7%|▋         | 1719/25000 [47:04<10:08:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:39,566 >> Initializing global attention on CLS token...
  7%|▋         | 1720/25000 [47:05<10:07:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:41,126 >> Initializing global attention on CLS token...
  7%|▋         | 1721/25000 [47:07<10:06:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:42,682 >> Initializing global attention on CLS token...
  7%|▋         | 1722/25000 [47:08<10:09:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:44,271 >> Initializing global attention on CLS token...
  7%|▋         | 1723/25000 [47:10<10:07:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:45,825 >> Initializing global attention on CLS token...
  7%|▋         | 1724/25000 [47:12<10:06:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,380 >> Initializing global attention on CLS token...
  7%|▋         | 1725/25000 [47:13<10:05:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,933 >> Initializing global attention on CLS token...
  7%|▋         | 1726/25000 [47:15<10:04:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,488 >> Initializing global attention on CLS token...
  7%|▋         | 1727/25000 [47:16<10:03:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,041 >> Initializing global attention on CLS token...
  7%|▋         | 1728/25000 [47:18<10:03:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,596 >> Initializing global attention on CLS token...
  7%|▋         | 1729/25000 [47:19<10:10:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:55,212 >> Initializing global attention on CLS token...
  7%|▋         | 1730/25000 [47:21<10:08:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:56,769 >> Initializing global attention on CLS token...
  7%|▋         | 1731/25000 [47:22<10:07:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:58,328 >> Initializing global attention on CLS token...
  7%|▋         | 1732/25000 [47:24<10:07:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:59,895 >> Initializing global attention on CLS token...
  7%|▋         | 1733/25000 [47:26<10:06:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:01,450 >> Initializing global attention on CLS token...
  7%|▋         | 1734/25000 [47:27<10:05:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:03,004 >> Initializing global attention on CLS token...
  7%|▋         | 1735/25000 [47:29<10:05:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:04,566 >> Initializing global attention on CLS token...
  7%|▋         | 1736/25000 [47:30<10:04:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:06,123 >> Initializing global attention on CLS token...
  7%|▋         | 1737/25000 [47:32<10:04:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:07,680 >> Initializing global attention on CLS token...
  7%|▋         | 1738/25000 [47:33<10:03:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:09,235 >> Initializing global attention on CLS token...
  7%|▋         | 1739/25000 [47:35<10:04:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:10,801 >> Initializing global attention on CLS token...
  7%|▋         | 1740/25000 [47:37<10:04:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:12,355 >> Initializing global attention on CLS token...
  7%|▋         | 1741/25000 [47:38<10:03:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:13,909 >> Initializing global attention on CLS token...
  7%|▋         | 1742/25000 [47:40<10:06:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:15,494 >> Initializing global attention on CLS token...
  7%|▋         | 1743/25000 [47:41<10:05:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:17,052 >> Initializing global attention on CLS token...
  7%|▋         | 1744/25000 [47:43<10:04:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:18,625 >> Initializing global attention on CLS token...
  7%|▋         | 1745/25000 [47:44<10:06:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:20,180 >> Initializing global attention on CLS token...
  7%|▋         | 1746/25000 [47:46<10:13:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:21,805 >> Initializing global attention on CLS token...
  7%|▋         | 1747/25000 [47:48<10:10:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:23,359 >> Initializing global attention on CLS token...
  7%|▋         | 1748/25000 [47:49<10:07:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:24,916 >> Initializing global attention on CLS token...
  7%|▋         | 1749/25000 [47:51<10:11:24,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:26,494 >> Initializing global attention on CLS token...
  7%|▋         | 1750/25000 [47:52<10:06:27,  1.57s/it]                                                         7%|▋         | 1750/25000 [47:52<10:06:27,  1.57s/it][INFO|trainer.py:738] 2024-01-22 01:06:28,029 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:06:28,033 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:06:28,034 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:06:28,034 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,048 >> Initializing global attention on CLS token...
{'eval_loss': 3.9933876991271973, 'eval_accuracy': 0.362, 'eval_macro_f1': 0.13015618673122956, 'eval_macro_precision': 0.12433183341364118, 'eval_macro_recall': 0.1548439864229338, 'eval_micro_f1': 0.362, 'eval_micro_precision': 0.362, 'eval_micro_recall': 0.362, 'eval_combined_score': 0.26533314379540063, 'eval_runtime': 7.7692, 'eval_samples_per_second': 64.357, 'eval_steps_per_second': 8.109, 'epoch': 13.0}
{'loss': 1.1976, 'learning_rate': 6.51e-05, 'epoch': 14.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,169 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.61it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,291 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,440 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:06,  9.60it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,560 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,680 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  8.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,800 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.79it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,922 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,042 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,168 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,288 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,409 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,529 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,650 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,770 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:29,891 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,011 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,132 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,253 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,374 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,496 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,616 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,738 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,859 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,980 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,118 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  7.77it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,252 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  7.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,374 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.00it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,494 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.92it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,624 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,744 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,867 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,988 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,109 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,230 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,351 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,471 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,593 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,713 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,834 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:32,954 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,074 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,195 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,316 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,438 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,559 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,681 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:02,  7.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,855 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:02,  7.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,980 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  7.69it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,101 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  7.86it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,222 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,343 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,465 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,585 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,706 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,827 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:34,948 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,071 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,191 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,311 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,432 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,561 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,677 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  7%|▋         | 1750/25000 [48:00<10:06:27,  1.57s/it]
100%|██████████| 63/63 [00:07<00:00,  8.13it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:06:35,766 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1750
[INFO|configuration_utils.py:461] 2024-01-22 01:06:35,773 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1750/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:06:36,339 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:06:36,340 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:06:36,340 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1750/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 01:06:37,539 >> Initializing global attention on CLS token...
  7%|▋         | 1751/25000 [48:03<28:27:56,  4.41s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:39,090 >> Initializing global attention on CLS token...
  7%|▋         | 1752/25000 [48:05<22:58:00,  3.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:40,660 >> Initializing global attention on CLS token...
  7%|▋         | 1753/25000 [48:06<19:05:50,  2.96s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:42,221 >> Initializing global attention on CLS token...
  7%|▋         | 1754/25000 [48:08<16:22:48,  2.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:43,775 >> Initializing global attention on CLS token...
  7%|▋         | 1755/25000 [48:09<14:28:59,  2.24s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:45,333 >> Initializing global attention on CLS token...
  7%|▋         | 1756/25000 [48:11<13:08:38,  2.04s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:46,885 >> Initializing global attention on CLS token...
  7%|▋         | 1757/25000 [48:13<12:23:16,  1.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:48,531 >> Initializing global attention on CLS token...
  7%|▋         | 1758/25000 [48:14<11:41:06,  1.81s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:50,087 >> Initializing global attention on CLS token...
  7%|▋         | 1759/25000 [48:16<11:11:19,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:51,641 >> Initializing global attention on CLS token...
  7%|▋         | 1760/25000 [48:17<10:55:02,  1.69s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:53,234 >> Initializing global attention on CLS token...
  7%|▋         | 1761/25000 [48:19<10:39:08,  1.65s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:54,788 >> Initializing global attention on CLS token...
  7%|▋         | 1762/25000 [48:20<10:28:09,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:56,345 >> Initializing global attention on CLS token...
  7%|▋         | 1763/25000 [48:22<10:29:28,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:57,976 >> Initializing global attention on CLS token...
  7%|▋         | 1764/25000 [48:24<10:21:09,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:59,532 >> Initializing global attention on CLS token...
  7%|▋         | 1765/25000 [48:25<10:15:43,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:01,090 >> Initializing global attention on CLS token...
  7%|▋         | 1766/25000 [48:27<10:11:47,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:02,646 >> Initializing global attention on CLS token...
  7%|▋         | 1767/25000 [48:28<10:18:10,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:04,281 >> Initializing global attention on CLS token...
  7%|▋         | 1768/25000 [48:30<10:13:24,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:05,836 >> Initializing global attention on CLS token...
  7%|▋         | 1769/25000 [48:32<10:10:16,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:07,392 >> Initializing global attention on CLS token...
  7%|▋         | 1770/25000 [48:33<10:08:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:08,963 >> Initializing global attention on CLS token...
  7%|▋         | 1771/25000 [48:35<10:07:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:10,516 >> Initializing global attention on CLS token...
  7%|▋         | 1772/25000 [48:36<10:05:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:12,072 >> Initializing global attention on CLS token...
  7%|▋         | 1773/25000 [48:38<10:04:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:13,627 >> Initializing global attention on CLS token...
  7%|▋         | 1774/25000 [48:39<10:17:34,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:15,301 >> Initializing global attention on CLS token...
  7%|▋         | 1775/25000 [48:41<10:13:19,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:16,860 >> Initializing global attention on CLS token...
  7%|▋         | 1776/25000 [48:43<10:09:51,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:18,415 >> Initializing global attention on CLS token...
  7%|▋         | 1777/25000 [48:44<10:16:12,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:20,045 >> Initializing global attention on CLS token...
  7%|▋         | 1778/25000 [48:46<10:12:05,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:21,602 >> Initializing global attention on CLS token...
  7%|▋         | 1779/25000 [48:47<10:09:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:23,157 >> Initializing global attention on CLS token...
  7%|▋         | 1780/25000 [48:49<10:10:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:24,743 >> Initializing global attention on CLS token...
  7%|▋         | 1781/25000 [48:50<10:07:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:26,298 >> Initializing global attention on CLS token...
  7%|▋         | 1782/25000 [48:52<10:05:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:27,850 >> Initializing global attention on CLS token...
  7%|▋         | 1783/25000 [48:54<10:04:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:29,408 >> Initializing global attention on CLS token...
  7%|▋         | 1784/25000 [48:55<10:06:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:30,989 >> Initializing global attention on CLS token...
  7%|▋         | 1785/25000 [48:57<10:05:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:32,546 >> Initializing global attention on CLS token...
  7%|▋         | 1786/25000 [48:58<10:04:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:34,106 >> Initializing global attention on CLS token...
  7%|▋         | 1787/25000 [49:00<10:10:24,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:35,718 >> Initializing global attention on CLS token...
  7%|▋         | 1788/25000 [49:01<10:07:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:37,271 >> Initializing global attention on CLS token...
  7%|▋         | 1789/25000 [49:03<10:06:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:38,843 >> Initializing global attention on CLS token...
  7%|▋         | 1790/25000 [49:05<10:05:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:40,397 >> Initializing global attention on CLS token...
  7%|▋         | 1791/25000 [49:06<10:16:55,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:42,057 >> Initializing global attention on CLS token...
  7%|▋         | 1792/25000 [49:08<10:12:27,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:43,614 >> Initializing global attention on CLS token...
  7%|▋         | 1793/25000 [49:09<10:09:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:45,169 >> Initializing global attention on CLS token...
  7%|▋         | 1794/25000 [49:11<10:12:58,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:46,777 >> Initializing global attention on CLS token...
  7%|▋         | 1795/25000 [49:12<10:09:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:48,333 >> Initializing global attention on CLS token...
  7%|▋         | 1796/25000 [49:14<10:08:16,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:49,898 >> Initializing global attention on CLS token...
  7%|▋         | 1797/25000 [49:16<10:06:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:51,457 >> Initializing global attention on CLS token...
  7%|▋         | 1798/25000 [49:17<10:14:30,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:53,094 >> Initializing global attention on CLS token...
  7%|▋         | 1799/25000 [49:19<10:11:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:54,657 >> Initializing global attention on CLS token...
  7%|▋         | 1800/25000 [49:20<10:08:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:56,213 >> Initializing global attention on CLS token...
  7%|▋         | 1801/25000 [49:22<10:09:47,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:57,806 >> Initializing global attention on CLS token...
  7%|▋         | 1802/25000 [49:24<10:09:05,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:59,370 >> Initializing global attention on CLS token...
  7%|▋         | 1803/25000 [49:25<10:17:31,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:01,018 >> Initializing global attention on CLS token...
  7%|▋         | 1804/25000 [49:27<10:13:40,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:02,582 >> Initializing global attention on CLS token...
  7%|▋         | 1805/25000 [49:28<10:10:37,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:04,147 >> Initializing global attention on CLS token...
  7%|▋         | 1806/25000 [49:30<10:08:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:05,704 >> Initializing global attention on CLS token...
  7%|▋         | 1807/25000 [49:31<10:06:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:07,257 >> Initializing global attention on CLS token...
  7%|▋         | 1808/25000 [49:33<10:19:02,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:08,937 >> Initializing global attention on CLS token...
  7%|▋         | 1809/25000 [49:35<10:13:31,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:10,492 >> Initializing global attention on CLS token...
  7%|▋         | 1810/25000 [49:36<10:09:38,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:12,046 >> Initializing global attention on CLS token...
  7%|▋         | 1811/25000 [49:38<10:09:11,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:13,620 >> Initializing global attention on CLS token...
  7%|▋         | 1812/25000 [49:39<10:06:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:15,175 >> Initializing global attention on CLS token...
  7%|▋         | 1813/25000 [49:41<10:05:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:16,731 >> Initializing global attention on CLS token...
  7%|▋         | 1814/25000 [49:42<10:04:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:18,297 >> Initializing global attention on CLS token...
  7%|▋         | 1815/25000 [49:44<10:15:41,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:19,952 >> Initializing global attention on CLS token...
  7%|▋         | 1816/25000 [49:46<10:14:49,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:21,542 >> Initializing global attention on CLS token...
  7%|▋         | 1817/25000 [49:47<10:11:05,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:23,097 >> Initializing global attention on CLS token...
  7%|▋         | 1818/25000 [49:49<10:08:53,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:24,660 >> Initializing global attention on CLS token...
  7%|▋         | 1819/25000 [49:50<10:06:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:26,216 >> Initializing global attention on CLS token...
  7%|▋         | 1820/25000 [49:52<10:10:54,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:27,823 >> Initializing global attention on CLS token...
  7%|▋         | 1821/25000 [49:54<10:09:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:29,394 >> Initializing global attention on CLS token...
  7%|▋         | 1822/25000 [49:55<10:07:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:30,949 >> Initializing global attention on CLS token...
  7%|▋         | 1823/25000 [49:57<10:06:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:32,512 >> Initializing global attention on CLS token...
  7%|▋         | 1824/25000 [49:58<10:04:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:34,068 >> Initializing global attention on CLS token...
  7%|▋         | 1825/25000 [50:00<10:04:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:35,630 >> Initializing global attention on CLS token...
  7%|▋         | 1826/25000 [50:01<10:03:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:37,185 >> Initializing global attention on CLS token...
  7%|▋         | 1827/25000 [50:03<10:04:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:38,758 >> Initializing global attention on CLS token...
  7%|▋         | 1828/25000 [50:04<10:04:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:40,322 >> Initializing global attention on CLS token...
  7%|▋         | 1829/25000 [50:06<10:02:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:41,875 >> Initializing global attention on CLS token...
  7%|▋         | 1830/25000 [50:08<10:06:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:43,468 >> Initializing global attention on CLS token...
  7%|▋         | 1831/25000 [50:09<10:04:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:45,023 >> Initializing global attention on CLS token...
  7%|▋         | 1832/25000 [50:11<10:05:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:46,599 >> Initializing global attention on CLS token...
  7%|▋         | 1833/25000 [50:12<10:08:44,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:48,194 >> Initializing global attention on CLS token...
  7%|▋         | 1834/25000 [50:14<10:06:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:49,747 >> Initializing global attention on CLS token...
  7%|▋         | 1835/25000 [50:15<10:04:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:51,306 >> Initializing global attention on CLS token...
  7%|▋         | 1836/25000 [50:17<10:03:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:52,862 >> Initializing global attention on CLS token...
  7%|▋         | 1837/25000 [50:19<10:03:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:54,421 >> Initializing global attention on CLS token...
  7%|▋         | 1838/25000 [50:20<10:02:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:55,978 >> Initializing global attention on CLS token...
  7%|▋         | 1839/25000 [50:22<10:07:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:57,581 >> Initializing global attention on CLS token...
  7%|▋         | 1840/25000 [50:23<10:05:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:59,140 >> Initializing global attention on CLS token...
  7%|▋         | 1841/25000 [50:25<10:03:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:00,694 >> Initializing global attention on CLS token...
  7%|▋         | 1842/25000 [50:26<10:05:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:02,268 >> Initializing global attention on CLS token...
  7%|▋         | 1843/25000 [50:28<10:03:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:03,824 >> Initializing global attention on CLS token...
  7%|▋         | 1844/25000 [50:30<10:03:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:05,388 >> Initializing global attention on CLS token...
  7%|▋         | 1845/25000 [50:31<10:02:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:06,940 >> Initializing global attention on CLS token...
  7%|▋         | 1846/25000 [50:33<10:12:28,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:08,589 >> Initializing global attention on CLS token...
  7%|▋         | 1847/25000 [50:34<10:09:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:10,154 >> Initializing global attention on CLS token...
  7%|▋         | 1848/25000 [50:36<10:06:34,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,706 >> Initializing global attention on CLS token...
  7%|▋         | 1849/25000 [50:37<10:06:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,274 >> Initializing global attention on CLS token...
  7%|▋         | 1850/25000 [50:39<10:03:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,825 >> Initializing global attention on CLS token...
  7%|▋         | 1851/25000 [50:41<10:04:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,392 >> Initializing global attention on CLS token...
  7%|▋         | 1852/25000 [50:42<10:02:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:18,013 >> Initializing global attention on CLS token...
  7%|▋         | 1853/25000 [50:44<10:10:01,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:19,571 >> Initializing global attention on CLS token...
  7%|▋         | 1854/25000 [50:45<10:10:35,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,157 >> Initializing global attention on CLS token...
  7%|▋         | 1855/25000 [50:47<10:06:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,708 >> Initializing global attention on CLS token...
  7%|▋         | 1856/25000 [50:48<10:13:05,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,338 >> Initializing global attention on CLS token...
  7%|▋         | 1857/25000 [50:50<10:09:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,892 >> Initializing global attention on CLS token...
  7%|▋         | 1858/25000 [50:52<10:08:47,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,467 >> Initializing global attention on CLS token...
  7%|▋         | 1859/25000 [50:53<10:05:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,015 >> Initializing global attention on CLS token...
  7%|▋         | 1860/25000 [50:55<10:02:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,564 >> Initializing global attention on CLS token...
  7%|▋         | 1861/25000 [50:56<10:06:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,159 >> Initializing global attention on CLS token...
  7%|▋         | 1862/25000 [50:58<10:04:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,711 >> Initializing global attention on CLS token...
  7%|▋         | 1863/25000 [50:59<10:02:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,268 >> Initializing global attention on CLS token...
  7%|▋         | 1864/25000 [51:01<10:01:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:36,821 >> Initializing global attention on CLS token...
  7%|▋         | 1865/25000 [51:03<10:04:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:38,405 >> Initializing global attention on CLS token...
  7%|▋         | 1866/25000 [51:04<10:06:08,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:39,989 >> Initializing global attention on CLS token...
  7%|▋         | 1867/25000 [51:06<10:04:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:41,543 >> Initializing global attention on CLS token...
  7%|▋         | 1868/25000 [51:07<10:06:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:43,131 >> Initializing global attention on CLS token...
  7%|▋         | 1869/25000 [51:09<10:03:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:44,681 >> Initializing global attention on CLS token...
  7%|▋         | 1870/25000 [51:10<10:03:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:46,246 >> Initializing global attention on CLS token...
  7%|▋         | 1871/25000 [51:12<10:02:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:47,799 >> Initializing global attention on CLS token...
  7%|▋         | 1872/25000 [51:14<10:00:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:49,351 >> Initializing global attention on CLS token...
  7%|▋         | 1873/25000 [51:15<10:02:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:50,921 >> Initializing global attention on CLS token...
  7%|▋         | 1874/25000 [51:17<10:00:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:52,451 >> Initializing global attention on CLS token...
  8%|▊         | 1875/25000 [51:18<9:59:02,  1.55s/it]                                                         8%|▊         | 1875/25000 [51:18<9:59:02,  1.55s/it][INFO|trainer.py:738] 2024-01-22 01:09:53,997 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:09:54,000 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:09:54,001 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:09:54,001 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,013 >> Initializing global attention on CLS token...
{'eval_loss': 4.079432964324951, 'eval_accuracy': 0.34, 'eval_macro_f1': 0.12542196821266588, 'eval_macro_precision': 0.12258915531160222, 'eval_macro_recall': 0.14597919038326015, 'eval_micro_f1': 0.34, 'eval_micro_precision': 0.34, 'eval_micro_recall': 0.34, 'eval_combined_score': 0.25057004484393264, 'eval_runtime': 7.7269, 'eval_samples_per_second': 64.709, 'eval_steps_per_second': 8.153, 'epoch': 14.0}
{'loss': 1.0374, 'learning_rate': 6.475e-05, 'epoch': 15.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,134 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.70it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,254 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,374 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,494 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,613 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,734 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,855 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:54,976 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,097 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,218 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,340 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,461 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,582 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:06,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,718 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,843 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:55,964 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,083 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,203 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,324 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,443 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,564 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,684 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,804 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:56,927 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,046 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,167 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,287 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,406 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,527 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,646 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,767 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:57,886 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,009 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,130 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,250 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.00it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,388 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  7.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,512 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,633 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,756 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,877 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:58,998 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,118 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,238 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,358 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,479 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,600 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,719 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,840 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:59,960 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,080 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,202 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,322 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,443 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,563 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,683 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,803 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:00,923 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:01,057 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  7.86it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:01,191 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  7.96it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:01,313 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:01,430 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:01,546 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  8%|▊         | 1875/25000 [51:26<9:59:02,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.17it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:10:01,631 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1875
[INFO|configuration_utils.py:461] 2024-01-22 01:10:01,638 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1875/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:10:02,186 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1875/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:10:02,187 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1875/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:10:02,188 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1875/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 01:10:03,296 >> Initializing global attention on CLS token...
  8%|▊         | 1876/25000 [51:29<27:52:12,  4.34s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:04,852 >> Initializing global attention on CLS token...
  8%|▊         | 1877/25000 [51:31<22:29:20,  3.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:06,399 >> Initializing global attention on CLS token...
  8%|▊         | 1878/25000 [51:32<18:43:36,  2.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:07,948 >> Initializing global attention on CLS token...
  8%|▊         | 1879/25000 [51:34<16:09:55,  2.52s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:09,535 >> Initializing global attention on CLS token...
  8%|▊         | 1880/25000 [51:35<14:18:17,  2.23s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:11,087 >> Initializing global attention on CLS token...
  8%|▊         | 1881/25000 [51:37<13:00:17,  2.03s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:12,639 >> Initializing global attention on CLS token...
  8%|▊         | 1882/25000 [51:38<12:18:23,  1.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:14,300 >> Initializing global attention on CLS token...
  8%|▊         | 1883/25000 [51:40<11:35:37,  1.81s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:15,849 >> Initializing global attention on CLS token...
  8%|▊         | 1884/25000 [51:42<11:05:55,  1.73s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:17,398 >> Initializing global attention on CLS token...
  8%|▊         | 1885/25000 [51:43<10:46:00,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:18,955 >> Initializing global attention on CLS token...
  8%|▊         | 1886/25000 [51:45<10:35:58,  1.65s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:20,545 >> Initializing global attention on CLS token...
  8%|▊         | 1887/25000 [51:46<10:24:15,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:22,094 >> Initializing global attention on CLS token...
  8%|▊         | 1888/25000 [51:48<10:16:05,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:23,644 >> Initializing global attention on CLS token...
  8%|▊         | 1889/25000 [51:49<10:18:02,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:25,261 >> Initializing global attention on CLS token...
  8%|▊         | 1890/25000 [51:51<10:12:07,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:26,814 >> Initializing global attention on CLS token...
  8%|▊         | 1891/25000 [51:53<10:07:26,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:28,368 >> Initializing global attention on CLS token...
  8%|▊         | 1892/25000 [51:54<10:05:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:29,927 >> Initializing global attention on CLS token...
  8%|▊         | 1893/25000 [51:56<10:05:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:31,499 >> Initializing global attention on CLS token...
  8%|▊         | 1894/25000 [51:57<10:03:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:33,048 >> Initializing global attention on CLS token...
  8%|▊         | 1895/25000 [51:59<10:01:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:34,599 >> Initializing global attention on CLS token...
  8%|▊         | 1896/25000 [52:00<10:01:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:36,160 >> Initializing global attention on CLS token...
  8%|▊         | 1897/25000 [52:02<10:00:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:37,712 >> Initializing global attention on CLS token...
  8%|▊         | 1898/25000 [52:03<9:59:00,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:10:39,262 >> Initializing global attention on CLS token...
  8%|▊         | 1899/25000 [52:05<9:59:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:40,824 >> Initializing global attention on CLS token...
  8%|▊         | 1900/25000 [52:07<9:59:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:42,376 >> Initializing global attention on CLS token...
  8%|▊         | 1901/25000 [52:08<9:58:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:43,928 >> Initializing global attention on CLS token...
  8%|▊         | 1902/25000 [52:10<9:58:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:45,479 >> Initializing global attention on CLS token...
  8%|▊         | 1903/25000 [52:11<10:06:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:47,104 >> Initializing global attention on CLS token...
  8%|▊         | 1904/25000 [52:13<10:03:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:48,655 >> Initializing global attention on CLS token...
  8%|▊         | 1905/25000 [52:14<10:01:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:50,210 >> Initializing global attention on CLS token...
  8%|▊         | 1906/25000 [52:16<10:04:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:51,794 >> Initializing global attention on CLS token...
  8%|▊         | 1907/25000 [52:17<10:02:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:53,348 >> Initializing global attention on CLS token...
  8%|▊         | 1908/25000 [52:19<10:00:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:54,898 >> Initializing global attention on CLS token...
  8%|▊         | 1909/25000 [52:21<10:07:58,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:56,522 >> Initializing global attention on CLS token...
  8%|▊         | 1910/25000 [52:22<10:05:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:58,076 >> Initializing global attention on CLS token...
  8%|▊         | 1911/25000 [52:24<10:02:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:10:59,629 >> Initializing global attention on CLS token...
  8%|▊         | 1912/25000 [52:25<10:01:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:01,181 >> Initializing global attention on CLS token...
  8%|▊         | 1913/25000 [52:27<10:01:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:02,746 >> Initializing global attention on CLS token...
  8%|▊         | 1914/25000 [52:28<10:00:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:04,301 >> Initializing global attention on CLS token...
  8%|▊         | 1915/25000 [52:30<9:59:29,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:11:05,854 >> Initializing global attention on CLS token...
  8%|▊         | 1916/25000 [52:32<10:01:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:07,427 >> Initializing global attention on CLS token...
  8%|▊         | 1917/25000 [52:33<10:00:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:08,984 >> Initializing global attention on CLS token...
  8%|▊         | 1918/25000 [52:35<10:02:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:10,566 >> Initializing global attention on CLS token...
  8%|▊         | 1919/25000 [52:36<10:03:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:12,131 >> Initializing global attention on CLS token...
  8%|▊         | 1920/25000 [52:38<10:00:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:13,683 >> Initializing global attention on CLS token...
  8%|▊         | 1921/25000 [52:39<10:02:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:15,258 >> Initializing global attention on CLS token...
  8%|▊         | 1922/25000 [52:41<10:00:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:16,812 >> Initializing global attention on CLS token...
  8%|▊         | 1923/25000 [52:43<10:00:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:18,371 >> Initializing global attention on CLS token...
  8%|▊         | 1924/25000 [52:44<9:59:44,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:11:19,926 >> Initializing global attention on CLS token...
  8%|▊         | 1925/25000 [52:46<10:08:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:21,564 >> Initializing global attention on CLS token...
  8%|▊         | 1926/25000 [52:47<10:11:05,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:23,167 >> Initializing global attention on CLS token...
  8%|▊         | 1927/25000 [52:49<10:07:36,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:24,727 >> Initializing global attention on CLS token...
  8%|▊         | 1928/25000 [52:50<10:06:27,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:26,297 >> Initializing global attention on CLS token...
  8%|▊         | 1929/25000 [52:52<10:03:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:27,852 >> Initializing global attention on CLS token...
  8%|▊         | 1930/25000 [52:54<10:08:58,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:29,467 >> Initializing global attention on CLS token...
  8%|▊         | 1931/25000 [52:55<10:07:55,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:31,041 >> Initializing global attention on CLS token...
  8%|▊         | 1932/25000 [52:57<10:04:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:32,609 >> Initializing global attention on CLS token...
  8%|▊         | 1933/25000 [52:58<10:06:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:34,186 >> Initializing global attention on CLS token...
  8%|▊         | 1934/25000 [53:00<10:04:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:35,741 >> Initializing global attention on CLS token...
  8%|▊         | 1935/25000 [53:02<10:08:16,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:37,350 >> Initializing global attention on CLS token...
  8%|▊         | 1936/25000 [53:03<10:06:11,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:38,920 >> Initializing global attention on CLS token...
  8%|▊         | 1937/25000 [53:05<10:04:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:40,473 >> Initializing global attention on CLS token...
  8%|▊         | 1938/25000 [53:06<10:09:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:42,089 >> Initializing global attention on CLS token...
  8%|▊         | 1939/25000 [53:08<10:05:36,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:43,643 >> Initializing global attention on CLS token...
  8%|▊         | 1940/25000 [53:09<10:06:20,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:45,225 >> Initializing global attention on CLS token...
  8%|▊         | 1941/25000 [53:11<10:07:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:46,807 >> Initializing global attention on CLS token...
  8%|▊         | 1942/25000 [53:13<10:04:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:48,363 >> Initializing global attention on CLS token...
  8%|▊         | 1943/25000 [53:14<10:04:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:49,938 >> Initializing global attention on CLS token...
  8%|▊         | 1944/25000 [53:16<10:02:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:51,490 >> Initializing global attention on CLS token...
  8%|▊         | 1945/25000 [53:17<10:02:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:53,061 >> Initializing global attention on CLS token...
  8%|▊         | 1946/25000 [53:19<10:01:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:54,617 >> Initializing global attention on CLS token...
  8%|▊         | 1947/25000 [53:20<10:09:57,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:56,259 >> Initializing global attention on CLS token...
  8%|▊         | 1948/25000 [53:22<10:06:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:57,812 >> Initializing global attention on CLS token...
  8%|▊         | 1949/25000 [53:24<10:03:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:11:59,364 >> Initializing global attention on CLS token...
  8%|▊         | 1950/25000 [53:25<10:01:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:00,923 >> Initializing global attention on CLS token...
  8%|▊         | 1951/25000 [53:27<10:00:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:02,477 >> Initializing global attention on CLS token...
  8%|▊         | 1952/25000 [53:28<9:59:26,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:12:04,033 >> Initializing global attention on CLS token...
  8%|▊         | 1953/25000 [53:30<9:59:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:05,598 >> Initializing global attention on CLS token...
  8%|▊         | 1954/25000 [53:31<9:58:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:07,151 >> Initializing global attention on CLS token...
  8%|▊         | 1955/25000 [53:33<9:58:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:08,704 >> Initializing global attention on CLS token...
  8%|▊         | 1956/25000 [53:34<9:57:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:10,257 >> Initializing global attention on CLS token...
  8%|▊         | 1957/25000 [53:36<9:59:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:11,834 >> Initializing global attention on CLS token...
  8%|▊         | 1958/25000 [53:38<9:58:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:13,386 >> Initializing global attention on CLS token...
  8%|▊         | 1959/25000 [53:39<9:58:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:14,939 >> Initializing global attention on CLS token...
  8%|▊         | 1960/25000 [53:41<10:04:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:16,551 >> Initializing global attention on CLS token...
  8%|▊         | 1961/25000 [53:42<10:01:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:18,102 >> Initializing global attention on CLS token...
  8%|▊         | 1962/25000 [53:44<10:00:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:19,657 >> Initializing global attention on CLS token...
  8%|▊         | 1963/25000 [53:45<10:04:52,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:21,261 >> Initializing global attention on CLS token...
  8%|▊         | 1964/25000 [53:47<10:02:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:22,813 >> Initializing global attention on CLS token...
  8%|▊         | 1965/25000 [53:49<10:00:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:24,368 >> Initializing global attention on CLS token...
  8%|▊         | 1966/25000 [53:50<9:59:21,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:12:25,921 >> Initializing global attention on CLS token...
  8%|▊         | 1967/25000 [53:52<10:04:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:27,523 >> Initializing global attention on CLS token...
  8%|▊         | 1968/25000 [53:53<10:04:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:29,100 >> Initializing global attention on CLS token...
  8%|▊         | 1969/25000 [53:55<10:01:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:30,650 >> Initializing global attention on CLS token...
  8%|▊         | 1970/25000 [53:56<10:00:42,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:32,210 >> Initializing global attention on CLS token...
  8%|▊         | 1971/25000 [53:58<9:59:05,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:12:33,761 >> Initializing global attention on CLS token...
  8%|▊         | 1972/25000 [53:59<10:01:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:35,350 >> Initializing global attention on CLS token...
  8%|▊         | 1973/25000 [54:01<10:00:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:36,905 >> Initializing global attention on CLS token...
  8%|▊         | 1974/25000 [54:03<10:04:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:38,504 >> Initializing global attention on CLS token...
  8%|▊         | 1975/25000 [54:04<10:04:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:40,077 >> Initializing global attention on CLS token...
  8%|▊         | 1976/25000 [54:06<10:01:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:41,629 >> Initializing global attention on CLS token...
  8%|▊         | 1977/25000 [54:07<10:00:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:43,191 >> Initializing global attention on CLS token...
  8%|▊         | 1978/25000 [54:09<9:59:28,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:12:44,744 >> Initializing global attention on CLS token...
  8%|▊         | 1979/25000 [54:10<10:00:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:46,316 >> Initializing global attention on CLS token...
  8%|▊         | 1980/25000 [54:12<10:02:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:47,899 >> Initializing global attention on CLS token...
  8%|▊         | 1981/25000 [54:14<10:00:28,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:49,451 >> Initializing global attention on CLS token...
  8%|▊         | 1982/25000 [54:15<10:02:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:51,033 >> Initializing global attention on CLS token...
  8%|▊         | 1983/25000 [54:17<10:00:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:52,585 >> Initializing global attention on CLS token...
  8%|▊         | 1984/25000 [54:18<9:59:54,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:12:54,147 >> Initializing global attention on CLS token...
  8%|▊         | 1985/25000 [54:20<9:58:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:55,699 >> Initializing global attention on CLS token...
  8%|▊         | 1986/25000 [54:21<9:58:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:57,265 >> Initializing global attention on CLS token...
  8%|▊         | 1987/25000 [54:23<10:01:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:12:58,848 >> Initializing global attention on CLS token...
  8%|▊         | 1988/25000 [54:25<9:59:32,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:13:00,399 >> Initializing global attention on CLS token...
  8%|▊         | 1989/25000 [54:26<10:02:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:01,987 >> Initializing global attention on CLS token...
  8%|▊         | 1990/25000 [54:28<10:00:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:03,539 >> Initializing global attention on CLS token...
  8%|▊         | 1991/25000 [54:29<10:00:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:05,104 >> Initializing global attention on CLS token...
  8%|▊         | 1992/25000 [54:31<10:03:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:06,698 >> Initializing global attention on CLS token...
  8%|▊         | 1993/25000 [54:32<10:01:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:08,251 >> Initializing global attention on CLS token...
  8%|▊         | 1994/25000 [54:34<10:01:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:09,826 >> Initializing global attention on CLS token...
  8%|▊         | 1995/25000 [54:36<9:59:49,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:13:11,379 >> Initializing global attention on CLS token...
  8%|▊         | 1996/25000 [54:37<10:08:33,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:13,019 >> Initializing global attention on CLS token...
  8%|▊         | 1997/25000 [54:39<10:04:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:14,568 >> Initializing global attention on CLS token...
  8%|▊         | 1998/25000 [54:40<10:01:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:16,119 >> Initializing global attention on CLS token...
  8%|▊         | 1999/25000 [54:42<9:59:54,  1.56s/it] [INFO|modeling_longformer.py:1920] 2024-01-22 01:13:17,656 >> Initializing global attention on CLS token...
  8%|▊         | 2000/25000 [54:43<9:56:09,  1.56s/it]                                                        8%|▊         | 2000/25000 [54:43<9:56:09,  1.56s/it][INFO|trainer.py:738] 2024-01-22 01:13:19,191 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:13:19,194 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:13:19,194 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:13:19,194 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,206 >> Initializing global attention on CLS token...
{'eval_loss': 4.068019390106201, 'eval_accuracy': 0.342, 'eval_macro_f1': 0.12226866712400448, 'eval_macro_precision': 0.12026853604150245, 'eval_macro_recall': 0.14132020796326744, 'eval_micro_f1': 0.342, 'eval_micro_precision': 0.342, 'eval_micro_recall': 0.342, 'eval_combined_score': 0.2502653444469678, 'eval_runtime': 7.6285, 'eval_samples_per_second': 65.544, 'eval_steps_per_second': 8.259, 'epoch': 15.0}
{'loss': 0.8875, 'learning_rate': 6.44e-05, 'epoch': 16.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,327 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.69it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,448 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,568 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,688 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,810 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  8.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:19,968 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,089 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,208 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:06,  8.44it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,329 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,449 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,569 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,689 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,809 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:20,930 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,050 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,172 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,293 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,413 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,533 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,660 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,781 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:21,902 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,022 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,143 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,263 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,382 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:05,  7.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,564 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  7.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,684 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.73it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,805 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  7.89it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:22,925 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.00it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,046 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,166 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,286 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,406 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,526 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,646 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,769 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:23,889 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,009 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,128 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,248 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,369 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,489 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,610 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,731 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,852 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:24,972 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,095 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:02,  6.90it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,296 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  7.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,417 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  7.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,537 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  7.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,658 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  7.88it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,780 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  7.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:25,901 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,025 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,145 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,265 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,385 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,505 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,626 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,744 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:26,860 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  8%|▊         | 2000/25000 [54:51<9:56:09,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.31it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:13:26,945 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-2000
[INFO|configuration_utils.py:461] 2024-01-22 01:13:26,983 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-2000/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:13:27,716 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-2000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:13:27,717 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:13:27,717 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:1955] 2024-01-22 01:13:28,962 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2139] 2024-01-22 01:13:28,962 >> Loading best model from /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/checkpoint-1625 (score: 3.9933876991271973).
                                                        8%|▊         | 2000/25000 [54:53<9:56:09,  1.56s/it]  8%|▊         | 2000/25000 [54:53<10:31:18,  1.65s/it]
[INFO|trainer.py:2881] 2024-01-22 01:13:29,098 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7
[INFO|configuration_utils.py:461] 2024-01-22 01:13:29,104 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:13:29,798 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:13:29,800 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:13:29,800 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/special_tokens_map.json
{'eval_loss': 4.064248561859131, 'eval_accuracy': 0.378, 'eval_macro_f1': 0.15321143318489625, 'eval_macro_precision': 0.15204539026012295, 'eval_macro_recall': 0.17326409973468795, 'eval_micro_f1': 0.378, 'eval_micro_precision': 0.378, 'eval_micro_recall': 0.378, 'eval_combined_score': 0.28436013188281534, 'eval_runtime': 7.7496, 'eval_samples_per_second': 64.519, 'eval_steps_per_second': 8.129, 'epoch': 16.0}
{'train_runtime': 3304.5719, 'train_samples_per_second': 242.089, 'train_steps_per_second': 7.565, 'train_loss': 2.6651060295104982, 'epoch': 16.0}
***** train metrics *****
  epoch                    =       16.0
  train_loss               =     2.6651
  train_runtime            = 0:55:04.57
  train_samples            =       4000
  train_samples_per_second =    242.089
  train_steps_per_second   =      7.565
01/22/2024 01:13:29 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:738] 2024-01-22 01:13:29,811 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:13:29,813 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:13:29,813 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:13:29,813 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:29,826 >> Initializing global attention on CLS token...
  0%|          | 0/63 [00:00<?, ?it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:29,998 >> Initializing global attention on CLS token...
  3%|▎         | 2/63 [00:00<00:03, 15.62it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,127 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,249 >> Initializing global attention on CLS token...
  6%|▋         | 4/63 [00:00<00:05, 10.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,369 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,500 >> Initializing global attention on CLS token...
 10%|▉         | 6/63 [00:00<00:06,  8.92it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,628 >> Initializing global attention on CLS token...
 11%|█         | 7/63 [00:00<00:06,  8.76it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,748 >> Initializing global attention on CLS token...
 13%|█▎        | 8/63 [00:00<00:06,  8.64it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,869 >> Initializing global attention on CLS token...
 14%|█▍        | 9/63 [00:00<00:06,  8.53it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:30,989 >> Initializing global attention on CLS token...
 16%|█▌        | 10/63 [00:01<00:06,  8.47it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,109 >> Initializing global attention on CLS token...
 17%|█▋        | 11/63 [00:01<00:06,  8.42it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,230 >> Initializing global attention on CLS token...
 19%|█▉        | 12/63 [00:01<00:06,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,351 >> Initializing global attention on CLS token...
 21%|██        | 13/63 [00:01<00:05,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,470 >> Initializing global attention on CLS token...
 22%|██▏       | 14/63 [00:01<00:05,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,590 >> Initializing global attention on CLS token...
 24%|██▍       | 15/63 [00:01<00:05,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,710 >> Initializing global attention on CLS token...
 25%|██▌       | 16/63 [00:01<00:05,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,830 >> Initializing global attention on CLS token...
 27%|██▋       | 17/63 [00:01<00:05,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:31,950 >> Initializing global attention on CLS token...
 29%|██▊       | 18/63 [00:02<00:05,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,070 >> Initializing global attention on CLS token...
 30%|███       | 19/63 [00:02<00:05,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,191 >> Initializing global attention on CLS token...
 32%|███▏      | 20/63 [00:02<00:05,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,311 >> Initializing global attention on CLS token...
 33%|███▎      | 21/63 [00:02<00:05,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,431 >> Initializing global attention on CLS token...
 35%|███▍      | 22/63 [00:02<00:04,  8.33it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,551 >> Initializing global attention on CLS token...
 37%|███▋      | 23/63 [00:02<00:04,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,671 >> Initializing global attention on CLS token...
 38%|███▊      | 24/63 [00:02<00:04,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,792 >> Initializing global attention on CLS token...
 40%|███▉      | 25/63 [00:02<00:04,  8.33it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:32,912 >> Initializing global attention on CLS token...
 41%|████▏     | 26/63 [00:03<00:04,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,032 >> Initializing global attention on CLS token...
 43%|████▎     | 27/63 [00:03<00:04,  7.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,208 >> Initializing global attention on CLS token...
 44%|████▍     | 28/63 [00:03<00:04,  7.59it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,328 >> Initializing global attention on CLS token...
 46%|████▌     | 29/63 [00:03<00:04,  7.77it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,449 >> Initializing global attention on CLS token...
 48%|████▊     | 30/63 [00:03<00:04,  7.93it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,568 >> Initializing global attention on CLS token...
 49%|████▉     | 31/63 [00:03<00:03,  8.06it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,688 >> Initializing global attention on CLS token...
 51%|█████     | 32/63 [00:03<00:03,  8.15it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,808 >> Initializing global attention on CLS token...
 52%|█████▏    | 33/63 [00:03<00:03,  8.21it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:33,927 >> Initializing global attention on CLS token...
 54%|█████▍    | 34/63 [00:04<00:03,  8.26it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,047 >> Initializing global attention on CLS token...
 56%|█████▌    | 35/63 [00:04<00:03,  8.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,166 >> Initializing global attention on CLS token...
 57%|█████▋    | 36/63 [00:04<00:03,  8.31it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,286 >> Initializing global attention on CLS token...
 59%|█████▊    | 37/63 [00:04<00:03,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,406 >> Initializing global attention on CLS token...
 60%|██████    | 38/63 [00:04<00:02,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,525 >> Initializing global attention on CLS token...
 62%|██████▏   | 39/63 [00:04<00:02,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,646 >> Initializing global attention on CLS token...
 63%|██████▎   | 40/63 [00:04<00:02,  8.33it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,765 >> Initializing global attention on CLS token...
 65%|██████▌   | 41/63 [00:04<00:02,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:34,886 >> Initializing global attention on CLS token...
 67%|██████▋   | 42/63 [00:05<00:02,  8.33it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,006 >> Initializing global attention on CLS token...
 68%|██████▊   | 43/63 [00:05<00:02,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,125 >> Initializing global attention on CLS token...
 70%|██████▉   | 44/63 [00:05<00:02,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,245 >> Initializing global attention on CLS token...
 71%|███████▏  | 45/63 [00:05<00:02,  8.33it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,365 >> Initializing global attention on CLS token...
 73%|███████▎  | 46/63 [00:05<00:02,  8.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,488 >> Initializing global attention on CLS token...
 75%|███████▍  | 47/63 [00:05<00:01,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,609 >> Initializing global attention on CLS token...
 76%|███████▌  | 48/63 [00:05<00:01,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,728 >> Initializing global attention on CLS token...
 78%|███████▊  | 49/63 [00:05<00:01,  7.15it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:35,914 >> Initializing global attention on CLS token...
 79%|███████▉  | 50/63 [00:06<00:01,  7.46it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,034 >> Initializing global attention on CLS token...
 81%|████████  | 51/63 [00:06<00:01,  7.69it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,155 >> Initializing global attention on CLS token...
 83%|████████▎ | 52/63 [00:06<00:01,  7.85it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,277 >> Initializing global attention on CLS token...
 84%|████████▍ | 53/63 [00:06<00:01,  7.97it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,397 >> Initializing global attention on CLS token...
 86%|████████▌ | 54/63 [00:06<00:01,  8.08it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,517 >> Initializing global attention on CLS token...
 87%|████████▋ | 55/63 [00:06<00:00,  8.16it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,636 >> Initializing global attention on CLS token...
 89%|████████▉ | 56/63 [00:06<00:00,  8.20it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,758 >> Initializing global attention on CLS token...
 90%|█████████ | 57/63 [00:06<00:00,  8.21it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,879 >> Initializing global attention on CLS token...
 92%|█████████▏| 58/63 [00:06<00:00,  8.26it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:36,998 >> Initializing global attention on CLS token...
 94%|█████████▎| 59/63 [00:07<00:00,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,117 >> Initializing global attention on CLS token...
 95%|█████████▌| 60/63 [00:07<00:00,  8.31it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,237 >> Initializing global attention on CLS token...
 97%|█████████▋| 61/63 [00:07<00:00,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,354 >> Initializing global attention on CLS token...
 98%|█████████▊| 62/63 [00:07<00:00,  8.39it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,468 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
100%|██████████| 63/63 [00:07<00:00,  8.34it/s]
***** eval metrics *****
  epoch                   =       16.0
  eval_accuracy           =      0.362
  eval_combined_score     =     0.2653
  eval_loss               =     3.9934
  eval_macro_f1           =     0.1302
  eval_macro_precision    =     0.1243
  eval_macro_recall       =     0.1548
  eval_micro_f1           =      0.362
  eval_micro_precision    =      0.362
  eval_micro_recall       =      0.362
  eval_runtime            = 0:00:07.73
  eval_samples            =        500
  eval_samples_per_second =     64.626
  eval_steps_per_second   =      8.143
01/22/2024 01:13:37 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:738] 2024-01-22 01:13:37,570 >> The following columns in the test set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article. If sentence, defendant_judgement, article_content, province, imprisonment, defendant, accusation,  verdict, date, fact, relevant_article are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:13:37,572 >> ***** Running Prediction *****
[INFO|trainer.py:3160] 2024-01-22 01:13:37,572 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:13:37,572 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,584 >> Initializing global attention on CLS token...
  0%|          | 0/63 [00:00<?, ?it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,704 >> Initializing global attention on CLS token...
  3%|▎         | 2/63 [00:00<00:03, 16.73it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,824 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:37,943 >> Initializing global attention on CLS token...
  6%|▋         | 4/63 [00:00<00:05, 10.52it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,064 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,183 >> Initializing global attention on CLS token...
 10%|▉         | 6/63 [00:00<00:06,  9.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,303 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,429 >> Initializing global attention on CLS token...
 13%|█▎        | 8/63 [00:00<00:06,  8.64it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,564 >> Initializing global attention on CLS token...
 14%|█▍        | 9/63 [00:00<00:06,  8.55it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,684 >> Initializing global attention on CLS token...
 16%|█▌        | 10/63 [00:01<00:06,  8.51it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,803 >> Initializing global attention on CLS token...
 17%|█▋        | 11/63 [00:01<00:06,  8.47it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:38,923 >> Initializing global attention on CLS token...
 19%|█▉        | 12/63 [00:01<00:06,  8.44it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,043 >> Initializing global attention on CLS token...
 21%|██        | 13/63 [00:01<00:05,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,163 >> Initializing global attention on CLS token...
 22%|██▏       | 14/63 [00:01<00:05,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,281 >> Initializing global attention on CLS token...
 24%|██▍       | 15/63 [00:01<00:05,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,401 >> Initializing global attention on CLS token...
 25%|██▌       | 16/63 [00:01<00:05,  8.38it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,521 >> Initializing global attention on CLS token...
 27%|██▋       | 17/63 [00:01<00:05,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,641 >> Initializing global attention on CLS token...
 29%|██▊       | 18/63 [00:02<00:05,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,761 >> Initializing global attention on CLS token...
 30%|███       | 19/63 [00:02<00:05,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,880 >> Initializing global attention on CLS token...
 32%|███▏      | 20/63 [00:02<00:05,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:39,999 >> Initializing global attention on CLS token...
 33%|███▎      | 21/63 [00:02<00:05,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,119 >> Initializing global attention on CLS token...
 35%|███▍      | 22/63 [00:02<00:04,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,238 >> Initializing global attention on CLS token...
 37%|███▋      | 23/63 [00:02<00:04,  8.38it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,358 >> Initializing global attention on CLS token...
 38%|███▊      | 24/63 [00:02<00:04,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,479 >> Initializing global attention on CLS token...
 40%|███▉      | 25/63 [00:02<00:04,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,598 >> Initializing global attention on CLS token...
 41%|████▏     | 26/63 [00:03<00:04,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,718 >> Initializing global attention on CLS token...
 43%|████▎     | 27/63 [00:03<00:04,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,837 >> Initializing global attention on CLS token...
 44%|████▍     | 28/63 [00:03<00:04,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:40,957 >> Initializing global attention on CLS token...
 46%|████▌     | 29/63 [00:03<00:05,  6.80it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,168 >> Initializing global attention on CLS token...
 48%|████▊     | 30/63 [00:03<00:04,  7.22it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,287 >> Initializing global attention on CLS token...
 49%|████▉     | 31/63 [00:03<00:04,  7.52it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,407 >> Initializing global attention on CLS token...
 51%|█████     | 32/63 [00:03<00:03,  7.76it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,526 >> Initializing global attention on CLS token...
 52%|█████▏    | 33/63 [00:03<00:03,  7.92it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,646 >> Initializing global attention on CLS token...
 54%|█████▍    | 34/63 [00:04<00:03,  7.91it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,774 >> Initializing global attention on CLS token...
 56%|█████▌    | 35/63 [00:04<00:03,  8.01it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:41,894 >> Initializing global attention on CLS token...
 57%|█████▋    | 36/63 [00:04<00:03,  8.11it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,014 >> Initializing global attention on CLS token...
 59%|█████▊    | 37/63 [00:04<00:03,  8.18it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,134 >> Initializing global attention on CLS token...
 60%|██████    | 38/63 [00:04<00:03,  8.23it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,254 >> Initializing global attention on CLS token...
 62%|██████▏   | 39/63 [00:04<00:02,  8.25it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,374 >> Initializing global attention on CLS token...
 63%|██████▎   | 40/63 [00:04<00:02,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,495 >> Initializing global attention on CLS token...
 65%|██████▌   | 41/63 [00:04<00:02,  8.27it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,616 >> Initializing global attention on CLS token...
 67%|██████▋   | 42/63 [00:05<00:02,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,737 >> Initializing global attention on CLS token...
 68%|██████▊   | 43/63 [00:05<00:02,  8.27it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,858 >> Initializing global attention on CLS token...
 70%|██████▉   | 44/63 [00:05<00:02,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:42,980 >> Initializing global attention on CLS token...
 71%|███████▏  | 45/63 [00:05<00:02,  8.25it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,100 >> Initializing global attention on CLS token...
 73%|███████▎  | 46/63 [00:05<00:02,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,220 >> Initializing global attention on CLS token...
 75%|███████▍  | 47/63 [00:05<00:01,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,342 >> Initializing global attention on CLS token...
 76%|███████▌  | 48/63 [00:05<00:01,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,462 >> Initializing global attention on CLS token...
 78%|███████▊  | 49/63 [00:05<00:01,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,582 >> Initializing global attention on CLS token...
 79%|███████▉  | 50/63 [00:05<00:01,  8.31it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,701 >> Initializing global attention on CLS token...
 81%|████████  | 51/63 [00:06<00:01,  7.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,877 >> Initializing global attention on CLS token...
 83%|████████▎ | 52/63 [00:06<00:01,  7.58it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:43,997 >> Initializing global attention on CLS token...
 84%|████████▍ | 53/63 [00:06<00:01,  7.76it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,120 >> Initializing global attention on CLS token...
 86%|████████▌ | 54/63 [00:06<00:01,  7.91it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,239 >> Initializing global attention on CLS token...
 87%|████████▋ | 55/63 [00:06<00:00,  8.05it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,359 >> Initializing global attention on CLS token...
 89%|████████▉ | 56/63 [00:06<00:00,  8.14it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,478 >> Initializing global attention on CLS token...
 90%|█████████ | 57/63 [00:06<00:00,  8.20it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,598 >> Initializing global attention on CLS token...
 92%|█████████▏| 58/63 [00:07<00:00,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,718 >> Initializing global attention on CLS token...
 94%|█████████▎| 59/63 [00:07<00:00,  8.26it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,838 >> Initializing global attention on CLS token...
 95%|█████████▌| 60/63 [00:07<00:00,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:44,957 >> Initializing global attention on CLS token...
 97%|█████████▋| 61/63 [00:07<00:00,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:45,075 >> Initializing global attention on CLS token...
 98%|█████████▊| 62/63 [00:07<00:00,  8.38it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:13:45,189 >> Initializing global attention on CLS token...
100%|██████████| 63/63 [00:07<00:00,  8.33it/s]
01/22/2024 01:13:45 - INFO - __main__ - ***** Predict results *****
01/22/2024 01:13:45 - INFO - __main__ - Predict results saved at /local/xiaowang/LJP Task/baseline_output/Task2/Data2/lawformer_lr7/thunlp_Lawformer_predict_results.txt
[INFO|modelcard.py:452] 2024-01-22 01:13:45,447 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.362}]}
wandb: - 0.029 MB of 0.029 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedwandb: - 0.029 MB of 0.225 MB uploadedwandb: \ 0.280 MB of 0.569 MB uploadedwandb: | 0.280 MB of 0.569 MB uploadedwandb: / 0.569 MB of 0.569 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▃▄▅▆▅▆▆▇▇█▇▇▇▇█▇
wandb:            eval/combined_score ▁▃▄▄▅▅▅▆▆▇▇▇▇▇▇█▇
wandb:                      eval/loss █▆▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:                  eval/macro_f1 ▁▂▂▃▄▄▄▆▆▆▇▆▇▇▆█▇
wandb:           eval/macro_precision ▁▂▂▂▃▃▄▅▅▆▆▆▇▆▆█▇
wandb:              eval/macro_recall ▁▂▃▄▅▅▅▇▇▇█▇▇▇▆█▇
wandb:                  eval/micro_f1 ▁▃▄▅▆▅▆▆▇▇█▇▇▇▇█▇
wandb:           eval/micro_precision ▁▃▄▅▆▅▆▆▇▇█▇▇▇▇█▇
wandb:              eval/micro_recall ▁▃▄▅▆▅▆▆▇▇█▇▇▇▇█▇
wandb:                   eval/runtime ▅▄█▁▆▆▄█▇▂▃▂▅▄▁▄▄
wandb:        eval/samples_per_second ▄▅▁█▃▃▅▁▂▇▆▇▄▅█▄▅
wandb:          eval/steps_per_second ▄▅▁█▃▃▅▁▂▇▆▇▄▅█▄▅
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████
wandb:              train/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████
wandb:            train/learning_rate ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁
wandb:                     train/loss █▆▆▅▄▄▄▃▃▃▂▂▂▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.362
wandb:            eval/combined_score 0.26533
wandb:                      eval/loss 3.99339
wandb:                  eval/macro_f1 0.13016
wandb:           eval/macro_precision 0.12433
wandb:              eval/macro_recall 0.15484
wandb:                  eval/micro_f1 0.362
wandb:           eval/micro_precision 0.362
wandb:              eval/micro_recall 0.362
wandb:                   eval/runtime 7.7368
wandb:        eval/samples_per_second 64.626
wandb:          eval/steps_per_second 8.143
wandb:                    train/epoch 16.0
wandb:              train/global_step 2000
wandb:            train/learning_rate 6e-05
wandb:                     train/loss 0.8875
wandb:               train/total_flos 2.1222755401728e+16
wandb:               train/train_loss 2.66511
wandb:            train/train_runtime 3304.5719
wandb: train/train_samples_per_second 242.089
wandb:   train/train_steps_per_second 7.565
wandb: 
wandb: 🚀 View run lawformer_data2_t2_bs32_lr7e-5 at: https://wandb.ai/loss4wang/LJP_baselines_task2/runs/koezwwet
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240122_001827-koezwwet/logs
