01/22/2024 00:21:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
01/22/2024 00:21:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=7e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/runs/Jan22_00-21-01_tony-4,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=200.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
01/22/2024 00:21:03 - INFO - __main__ - load a local file for train: /local/xiaowang/LJP Task/Data/Datasets/train_data4.csv
01/22/2024 00:21:03 - INFO - __main__ - load a local file for validation: /local/xiaowang/LJP Task/Data/Datasets/val_data4.csv
01/22/2024 00:21:03 - INFO - __main__ - load a local file for test: /local/xiaowang/LJP Task/Data/Datasets/test_data4.csv
Using custom data configuration default-09e0feb6500006c1
01/22/2024 00:21:04 - INFO - datasets.builder - Using custom data configuration default-09e0feb6500006c1
Loading Dataset Infos from /local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/datasets/packaged_modules/csv
01/22/2024 00:21:04 - INFO - datasets.info - Loading Dataset Infos from /local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/datasets/packaged_modules/csv
Generating dataset csv (/home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
01/22/2024 00:21:04 - INFO - datasets.builder - Generating dataset csv (/home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
Downloading and preparing dataset csv/default to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...
01/22/2024 00:21:04 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 25679.41it/s]
Downloading took 0.0 min
01/22/2024 00:21:04 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
01/22/2024 00:21:04 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2199.43it/s]
Generating train split
01/22/2024 00:21:04 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4000 examples [00:03, 1039.24 examples/s]Generating train split: 4000 examples [00:03, 1034.50 examples/s]
Generating validation split
01/22/2024 00:21:08 - INFO - datasets.builder - Generating validation split
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 500 examples [00:00, 1052.32 examples/s]Generating validation split: 500 examples [00:00, 1044.23 examples/s]
Generating test split
01/22/2024 00:21:08 - INFO - datasets.builder - Generating test split
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 1043.23 examples/s]Generating test split: 500 examples [00:00, 1036.66 examples/s]
Unable to verify splits sizes.
01/22/2024 00:21:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.
01/22/2024 00:21:09 - INFO - datasets.builder - Dataset csv downloaded and prepared to /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.
Map:   0%|          | 0/4000 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-03f7a7101fb96050.arrow
01/22/2024 00:21:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-03f7a7101fb96050.arrow
Map:  25%|██▌       | 1000/4000 [00:00<00:00, 5090.82 examples/s]Map:  50%|█████     | 2000/4000 [00:00<00:00, 5177.68 examples/s]Map:  74%|███████▍  | 2975/4000 [00:00<00:00, 4221.62 examples/s]Map:  90%|█████████ | 3608/4000 [00:00<00:00, 4100.54 examples/s]Map: 100%|██████████| 4000/4000 [00:00<00:00, 4058.02 examples/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-197aea529ff2f200.arrow
01/22/2024 00:21:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-197aea529ff2f200.arrow
Map: 100%|██████████| 500/500 [00:00<00:00, 4807.56 examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 4668.20 examples/s]
Map:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-21ed8231333aee44.arrow
01/22/2024 00:21:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-21ed8231333aee44.arrow
Map: 100%|██████████| 500/500 [00:00<00:00, 4994.86 examples/s]Map: 100%|██████████| 500/500 [00:00<00:00, 4859.88 examples/s]
01/22/2024 00:21:10 - WARNING - __main__ - Labels {"['寻衅滋事', '故意伤害', '故意毁坏财物', '行贿', '骗取贷款、票据承兑、金融票证']", "['盗窃', '寻衅滋事', '妨害公务']", "['组织、领导、参加黑社会性质组织', '故意伤害', '寻衅滋事', '开设赌场']", "['寻衅滋事', '抢夺']", "['引诱、容留、介绍卖淫', '诈骗']", "['保险诈骗', '非法拘禁']", "['寻衅滋事', '非法拘禁', '故意伤害']", "['非法拘禁', '故意毁坏财物', '故意伤害', '打击报复证人']", "['寻衅滋事', '故意伤害', '赌博']", "['组织未成年人进行违反治安管理活动', '寻衅滋事', '非法拘禁', '强制侮辱', '介绍卖淫']", "['聚众斗殴', '盗伐林木', '滥伐林木']", "['介绍卖淫', '敲诈勒索']", "['贪污', '挪用资金']", "['非法吸收公众存款', '非法占用农用地']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '骗取贷款、票据承兑、金融票证']", "['组织、领导、参加黑社会性质组织', '开设赌场', '敲诈勒索', '聚众斗殴', '寻衅滋事']", "['敲诈勒索', '窝藏、包庇']", "['非法采矿', '非法持有毒品']", "['故意伤害', '组织、强迫、引诱、容留、介绍卖淫', '非法拘禁', '猥亵儿童']", "['非法制造、出售非法制造的发票', '伪造、变造、买卖国家机关公文、证件、印章']", "['妨害作证', '危险驾驶']", "['组织、领导传销活动', '非法拘禁', '抢劫']", "['强迫交易', '敲诈勒索', '赌博']", "['组织卖淫', '收买被拐卖的妇女']", "['非法采矿', '窝藏、包庇']", "['妨害作证', '寻衅滋事']", "['诈骗', '高利转贷']", "['生产、销售假药', '走私普通货物、物品']", "['滥用职权', '受贿', '行贿']", "['容留他人吸毒', '非法持有、私藏枪支、弹药']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '非法吸收公众存款']", "['敲诈勒索', '伪造公司、企业、事业单位、人民团体印章', '妨害信用卡管理']", "['诈骗', '寻衅滋事', '非法拘禁', '敲诈勒索']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '寻衅滋事']", "['非法占用农用地', '窝藏、包庇']", "['污染环境', '诈骗']", "['贪污', '国有公司、企业、事业单位人员失职']", "['赌博', '寻衅滋事', '非法拘禁']", "['组织、领导传销活动', '敲诈勒索', '非法拘禁']", "['集资诈骗', '非法吸收公众存款', '虚假出资、抽逃出资']", "['敲诈勒索', '诈骗', '非法拘禁', '诬告陷害']", "['信用卡诈骗', '信用卡诈骗', '骗取贷款、票据承兑、金融票证']", "['寻衅滋事', '妨害公务', '故意伤害']", "['非法转让、倒卖土地使用权', '对非国家工作人员行贿', '行贿']", "['寻衅滋事', '开设赌场', '强迫交易']", "['故意伤害', '非法买卖制毒物品']", "['盗伐林木', '故意毁坏财物']", "['招摇撞骗', '走私、贩卖、运输、制造毒品']", "['敲诈勒索', '聚众斗殴', '开设赌场']", "['非法经营', '伪造公司、企业、事业单位、人民团体印章']", "['组织、领导、参加黑社会性质组织', '开设赌场', '寻衅滋事', '聚众斗殴', '非法制造、买卖、运输、邮寄、储存枪支、弹药、爆炸物']", "['操纵证券、期货市场', '职务侵占']", "['组织卖淫', '开设赌场']", "['违法发放贷款', '违法发放贷款']", "['强迫交易', '行贿']", "['寻衅滋事', '诬告陷害']", "['敲诈勒索', '非法拘禁', '窝藏、包庇']", "['伪造、倒卖伪造的有价票证', '持有伪造的发票']", "['走私、贩卖、运输、制造毒品', '以危险方法危害公共安全']", "['聚众冲击国家机关', '妨害公务']", "['非法吸收公众存款', '赌博']", "['寻衅滋事', '故意毁坏财物', '重婚']", "['非法转让、倒卖土地使用权', '寻衅滋事']", "['洗钱', '窝藏、包庇']", "['伪造、变造、买卖国家机关公文、证件、印章', '虚假诉讼']", "['交通肇事', '职务侵占']", "['挪用特定款物', '贪污']", "['开设赌场', '寻衅滋事', '危险驾驶']", "['非法吸收公众存款', '窝藏、包庇']", "['走私、贩卖、运输、制造毒品', '伪造身份证件']", "['盗掘古文化遗址、古墓葬', '危险驾驶']", "['走私、贩卖、运输、制造毒品', '妨害公务']", "['操纵证券、期货市场', '非法持有、私藏枪支、弹药']", "['抢劫', '寻衅滋事', '聚众斗殴']", "['敲诈勒索', '盗掘古墓葬']", "['寻衅滋事', '妨害公务', '妨害作证']", "['寻衅滋事', '强迫交易', '非法采矿', '敲诈勒索']", "['组织、领导、参加黑社会性质组织', '寻衅滋事', '非法吸收公众存款', '非法拘禁', '引诱、容留、介绍卖淫', '高利转贷', '开设赌场']", "['参加黑社会性质组织', '聚众斗殴', '非法拘禁']", "['组织、领导、参加黑社会性质组织', '聚众斗殴', '寻衅滋事', '开设赌场', '非法拘禁']", "['破坏生产经营', '聚众扰乱公共场所秩序、交通秩序']", "['非法狩猎', '非法猎捕、杀害珍贵、濒危野生动物']", "['非法持有、私藏枪支、弹药', '聚众斗殴']", "['故意毁坏财物', '强迫交易']", "['聚众斗殴', '强迫交易', '故意伤害']", "['污染环境', '行贿']", "['职务侵占', '贪污', '受贿']", "['聚众扰乱社会秩序', '寻衅滋事', '破坏生产经营']", "['非法转让、倒卖土地使用权', '非法占用农用地']", "['组织、领导、参加黑社会性质组织', '开设赌场', '非法拘禁']", "['盗伐林木', '盗窃']", "['故意伤害', '强迫交易']", "['非法拘禁', '非法持有、私藏枪支、弹药', '开设赌场']", "['走私、贩卖、运输、制造毒品', '非法持有毒品', '故意伤害']", "['滥用职权', '行贿']", "['组织、领导、参加黑社会性质组织', '抢劫', '敲诈勒索']", "['串通投标', '伪造公司、企业、事业单位、人民团体印章']", "['受贿', '对非国家工作人员行贿']", "['非法拘禁', '信用卡诈骗', '抢劫']", "['开设赌场', '对有影响力的人行贿']", "['组织、领导、参加黑社会性质组织', '寻衅滋事', '赌博']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '盗窃']", "['组织、领导、参加黑社会性质组织', '敲诈勒索', '非法制造、买卖、运输、邮寄、储存枪支、弹药、爆炸物']", "['贪污', '受贿', '职务侵占']", "['赌博', '非法侵入住宅']", "['滥用职权', '赌博']", "['聚众冲击国家机关', '寻衅滋事']", "['绑架', '盗窃']", "['寻衅滋事', '敲诈勒索', '开设赌场']", "['寻衅滋事', '故意伤害', '危险驾驶']", "['诈骗', '买卖国家机关证件']", "['故意伤害', '非法侵入住宅', '非法拘禁']", "['投放危险物质', '污染环境']", "['抢劫', '伪造身份证件']", "['非法拘禁', '敲诈勒索', '妨害作证']", "['走私珍贵动物、珍贵动物制品', '走私普通货物、物品']", "['妨害信用卡管理', '盗窃', '非法拘禁']", "['开设赌场', '非法拘禁', '寻衅滋事', '信用卡诈骗', '信用卡诈骗']", "['聚众扰乱社会秩序', '聚众冲击国家机关']", "['单位行贿', '挪用资金']", "['拐卖妇女、儿童', '伪造身份证件']", "['信用卡诈骗', '窃取、非法提供他人信用卡信息']", "['交通肇事', '伪造、变造、买卖国家机关公文、证件、印章']"} in validation set but not in training set, adding them to the label list
01/22/2024 00:21:10 - WARNING - __main__ - Labels {"['盗掘古文化遗址、古墓葬', '贪污', '徇私舞弊不移交刑事案件']", "['妨害公务', '诈骗']", "['组织、领导、参加黑社会性质组织', '放火', '寻衅滋事', '非法制造、买卖、运输、邮寄、储存枪支、弹药、爆炸物', '故意伤害', '非法持有、私藏枪支、弹药', '破坏生产经营']", "['故意伤害', '诬告陷害']", "['重大责任事故', '窝藏、包庇']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '危险驾驶']", "['诈骗', '盗窃', '信用卡诈骗']", "['妨害作证', '敲诈勒索']", "['非法侵入住宅', '诈骗']", "['职务侵占', '非法采矿']", "['引诱幼女卖淫', '引诱、容留、介绍卖淫']", "['寻衅滋事', '敲诈勒索', '开设赌场', '故意毁坏财物']", "['非法占用农用地', '盗伐林木']", "['构成非法制造注册商标标识', '非法处置查封、扣押、冻结的财产']", "['虚开增值税专用发票', '对非国家工作人员行贿']", "['寻衅滋事', '诈骗', '敲诈勒索']", "['非法持有枪支', '失火']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '持有伪造的发票']", "['寻衅滋事', '拒不执行判决、裁定']", "['寻衅滋事', '敲诈勒索', '诈骗']", "['盗伐林木', '非法采矿']", "['赌博', '开设赌场', '寻衅滋事']", "['盗窃', '信用卡诈骗', '信用卡诈骗']", "['非法转让、倒卖土地使用权', '危险驾驶']", "['强迫交易', '故意毁坏财物']", "['诈骗', '虚开增值税专用发票、用于骗取出口退税、抵扣税款发票']", "['非法采矿', '寻衅滋事', '串通投标']", "['高利转贷', '非法拘禁', '寻衅滋事', '开设赌场']", "['故意伤害', '寻衅滋事', '聚众斗殴']", "['寻衅滋事', '非法拘禁', '非法持有毒品', '非法持有、私藏枪支、弹药']", "['非法买卖制毒物品', '容留他人吸毒']", "['敲诈勒索', '容留他人吸食毒品']", "['寻衅滋事', '高利转贷', '挪用资金']", "['故意杀人', '妨害作证']", "['绑架', '故意伤害']", "['诈骗', '赌博', '寻衅滋事', '开设赌场']", "['盗窃', '生产、销售假药']", "['容留他人吸毒', '非法持有毒品']", "['非法采矿', '妨害公务']", "['走私、贩卖、运输、制造毒品', '窝藏、包庇']", "['伪造、变造金融票证', '非国家工作人员受贿']", "['组织、领导、参加黑社会性质组织', '非法拘禁', '寻衅滋事', '敲诈勒索', '强迫交易']", "['窝藏、包庇', '敲诈勒索']", "['非法拘禁', '聚众斗殴', '寻衅滋事']", "['盗掘古文化遗址、古墓葬', '过失以危险方法危害公共安全']", "['逃税', '串通投标']", "['强迫交易', '妨害公务']", "['故意伤害', '敲诈勒索', '破坏生产经营']", "['盗窃', '强奸']", "['协助组织卖淫', '引诱、容留、介绍卖淫', '开设赌场']", "['伪造公司、企业、事业单位、人民团体印章', '非国家工作人员受贿', '职务侵占']", "['故意伤害', '盗掘古文化遗址、古墓葬']", "['抢劫', '盗窃', '破坏电力设备']", "['重大责任事故', '交通肇事']", "['非法拘禁', '寻衅滋事', '妨害公务']", "['赌博', '保险诈骗']", "['敲诈勒索', '故意毁坏财物', '诈骗']", "['诈骗', '抢夺', '敲诈勒索']", "['诈骗', '开设赌场', '寻衅滋事']", "['过失致人死亡', '盗窃']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '骗取出口退税']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '隐匿、故意销毁会计凭证、会计账簿']", "['开设赌场', '组织卖淫']", "['帮助毁灭、伪造证据', '寻衅滋事', '妨害作证']", "['保险诈骗', '放火', '敲诈勒索', '寻衅滋事']", "['贪污', '受贿', '单位行贿']", "['受贿', '职务侵占']", "['敲诈勒索', '寻衅滋事', '非法拘禁', '妨害作证']", "['窃取、收买、非法提供信用卡信息', '诈骗']", "['组织、领导、参加黑社会性质组织', '非法拘禁', '寻衅滋事', '非法侵入住宅']", "['受贿', '贪污', '挪用公款', '串通投标']", "['挪用公款', '贪污', '受贿']", "['挪用公款', '非法经营同类营业', '非法持有、私藏枪支、弹药', '受贿']", "['寻衅滋事', '妨害公务', '敲诈勒索', '非法拘禁', '强迫交易', '赌博']", "['寻衅滋事', '引诱、教唆、欺骗他人吸毒']", "['强奸', '非法拘禁']", "['非法采矿', '寻衅滋事', '盗伐林木']", "['故意毁坏财物', '故意伤害', '妨害公务']", "['非法狩猎', '过失致人重伤']", "['非国家工作人员受贿', '诬告陷害']", "['重大责任事故', '非法采矿']", "['敲诈勒索', '抢劫', '诈骗', '非法拘禁', '寻衅滋事', '盗窃']", "['非法猎捕、杀害珍贵、濒危野生动物', '非法持有枪支']", "['伪造、变造、买卖国家机关公文、证件、印章', '寻衅滋事']", "['强奸', '强迫卖淫', '非法拘禁']", "['虚开发票', '故意伤害']", "['寻衅滋事', '聚众斗殴', '故意伤害', '开设赌场']", "['组织、领导、参加黑社会性质组织', '寻衅滋事', '开设赌场', '非法拘禁']", "['容留他人吸毒', '危险驾驶']", "['敲诈勒索', '诈骗', '引诱、容留、介绍卖淫']", "['非法拘禁', '重婚']", "['走私、贩卖、运输、制造毒品', '行贿']", "['非寻衅滋事', '非法占用农用地']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '妨害作证']", "['寻衅滋事', '抢劫', '敲诈勒索']", "['开设赌场', '聚众斗殴', '窝藏、包庇']", "['寻衅滋事', '开设赌场', '容留他人吸毒']", "['虚开发票', '诈骗']", "['保险诈骗', '敲诈勒索']", "['虚假诉讼', '非法侵入住宅']", "['贪污', '洗钱']", "['受贿', '挪用资金']", "['非法拘禁', '聚众斗殴', '窃取、收买、非法提供信用卡信息']", "['寻衅滋事', '开设赌场', '故意伤害']", "['挪用资金', '受贿', '行贿']", "['开设赌场', '强迫交易']", "['妨害作证', '帮助毁灭、伪造证据']", "['强迫卖淫', '非法拘禁']", "['非法侵入住宅', '寻衅滋事']", "['危险驾驶', '敲诈勒索']", "['开设赌场', '敲诈勒索', '非法拘禁']", "['走私、贩卖、运输、制造毒品', '贩容留他人吸毒']", "['聚众斗殴', '非法占用农用地']", "['销售伪劣产品', '故意伤害']", "['盗窃', '寻衅滋事', '聚众斗殴']", "['窃取、收买、非法提供信用卡信息', '伪造公司、企业、事业单位、人民团体印章']", "['聚众斗殴', '盗窃', '诈骗']", "['盗窃', '倒卖文物']", "['虚开增值税专用发票、用于骗取出口退税、抵扣税款发票', '非法购买增值税专用发票']"} in test set but not in training set, adding them to the label list
[INFO|configuration_utils.py:717] 2024-01-22 00:21:10,630 >> loading configuration file config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/config.json
[INFO|configuration_utils.py:777] 2024-01-22 00:21:10,667 >> Model config LongformerConfig {
  "_name_or_path": "thunlp/Lawformer",
  "architectures": [
    "LongformerForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "finetuning_task": "text-classification",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95",
    "96": "LABEL_96",
    "97": "LABEL_97",
    "98": "LABEL_98",
    "99": "LABEL_99",
    "100": "LABEL_100",
    "101": "LABEL_101",
    "102": "LABEL_102",
    "103": "LABEL_103",
    "104": "LABEL_104",
    "105": "LABEL_105",
    "106": "LABEL_106",
    "107": "LABEL_107",
    "108": "LABEL_108",
    "109": "LABEL_109",
    "110": "LABEL_110",
    "111": "LABEL_111",
    "112": "LABEL_112",
    "113": "LABEL_113",
    "114": "LABEL_114",
    "115": "LABEL_115",
    "116": "LABEL_116",
    "117": "LABEL_117",
    "118": "LABEL_118",
    "119": "LABEL_119",
    "120": "LABEL_120",
    "121": "LABEL_121",
    "122": "LABEL_122",
    "123": "LABEL_123",
    "124": "LABEL_124",
    "125": "LABEL_125",
    "126": "LABEL_126",
    "127": "LABEL_127",
    "128": "LABEL_128",
    "129": "LABEL_129",
    "130": "LABEL_130",
    "131": "LABEL_131",
    "132": "LABEL_132",
    "133": "LABEL_133",
    "134": "LABEL_134",
    "135": "LABEL_135",
    "136": "LABEL_136",
    "137": "LABEL_137",
    "138": "LABEL_138",
    "139": "LABEL_139",
    "140": "LABEL_140",
    "141": "LABEL_141",
    "142": "LABEL_142",
    "143": "LABEL_143",
    "144": "LABEL_144",
    "145": "LABEL_145",
    "146": "LABEL_146",
    "147": "LABEL_147",
    "148": "LABEL_148",
    "149": "LABEL_149",
    "150": "LABEL_150",
    "151": "LABEL_151",
    "152": "LABEL_152",
    "153": "LABEL_153",
    "154": "LABEL_154",
    "155": "LABEL_155",
    "156": "LABEL_156",
    "157": "LABEL_157",
    "158": "LABEL_158",
    "159": "LABEL_159",
    "160": "LABEL_160",
    "161": "LABEL_161",
    "162": "LABEL_162",
    "163": "LABEL_163",
    "164": "LABEL_164",
    "165": "LABEL_165",
    "166": "LABEL_166",
    "167": "LABEL_167",
    "168": "LABEL_168",
    "169": "LABEL_169",
    "170": "LABEL_170",
    "171": "LABEL_171",
    "172": "LABEL_172",
    "173": "LABEL_173",
    "174": "LABEL_174",
    "175": "LABEL_175",
    "176": "LABEL_176",
    "177": "LABEL_177",
    "178": "LABEL_178",
    "179": "LABEL_179",
    "180": "LABEL_180",
    "181": "LABEL_181",
    "182": "LABEL_182",
    "183": "LABEL_183",
    "184": "LABEL_184",
    "185": "LABEL_185",
    "186": "LABEL_186",
    "187": "LABEL_187",
    "188": "LABEL_188",
    "189": "LABEL_189",
    "190": "LABEL_190",
    "191": "LABEL_191",
    "192": "LABEL_192",
    "193": "LABEL_193",
    "194": "LABEL_194",
    "195": "LABEL_195",
    "196": "LABEL_196",
    "197": "LABEL_197",
    "198": "LABEL_198",
    "199": "LABEL_199",
    "200": "LABEL_200",
    "201": "LABEL_201",
    "202": "LABEL_202",
    "203": "LABEL_203",
    "204": "LABEL_204",
    "205": "LABEL_205",
    "206": "LABEL_206",
    "207": "LABEL_207",
    "208": "LABEL_208",
    "209": "LABEL_209",
    "210": "LABEL_210",
    "211": "LABEL_211",
    "212": "LABEL_212",
    "213": "LABEL_213",
    "214": "LABEL_214",
    "215": "LABEL_215",
    "216": "LABEL_216",
    "217": "LABEL_217",
    "218": "LABEL_218",
    "219": "LABEL_219",
    "220": "LABEL_220",
    "221": "LABEL_221",
    "222": "LABEL_222",
    "223": "LABEL_223",
    "224": "LABEL_224",
    "225": "LABEL_225",
    "226": "LABEL_226",
    "227": "LABEL_227",
    "228": "LABEL_228",
    "229": "LABEL_229",
    "230": "LABEL_230",
    "231": "LABEL_231",
    "232": "LABEL_232",
    "233": "LABEL_233",
    "234": "LABEL_234",
    "235": "LABEL_235",
    "236": "LABEL_236",
    "237": "LABEL_237",
    "238": "LABEL_238",
    "239": "LABEL_239",
    "240": "LABEL_240",
    "241": "LABEL_241",
    "242": "LABEL_242",
    "243": "LABEL_243",
    "244": "LABEL_244",
    "245": "LABEL_245",
    "246": "LABEL_246",
    "247": "LABEL_247",
    "248": "LABEL_248",
    "249": "LABEL_249",
    "250": "LABEL_250",
    "251": "LABEL_251",
    "252": "LABEL_252",
    "253": "LABEL_253",
    "254": "LABEL_254",
    "255": "LABEL_255",
    "256": "LABEL_256",
    "257": "LABEL_257",
    "258": "LABEL_258",
    "259": "LABEL_259",
    "260": "LABEL_260",
    "261": "LABEL_261",
    "262": "LABEL_262",
    "263": "LABEL_263",
    "264": "LABEL_264",
    "265": "LABEL_265",
    "266": "LABEL_266",
    "267": "LABEL_267",
    "268": "LABEL_268",
    "269": "LABEL_269",
    "270": "LABEL_270",
    "271": "LABEL_271",
    "272": "LABEL_272",
    "273": "LABEL_273",
    "274": "LABEL_274",
    "275": "LABEL_275",
    "276": "LABEL_276",
    "277": "LABEL_277",
    "278": "LABEL_278",
    "279": "LABEL_279",
    "280": "LABEL_280",
    "281": "LABEL_281",
    "282": "LABEL_282",
    "283": "LABEL_283",
    "284": "LABEL_284",
    "285": "LABEL_285",
    "286": "LABEL_286",
    "287": "LABEL_287",
    "288": "LABEL_288",
    "289": "LABEL_289",
    "290": "LABEL_290",
    "291": "LABEL_291",
    "292": "LABEL_292",
    "293": "LABEL_293",
    "294": "LABEL_294",
    "295": "LABEL_295",
    "296": "LABEL_296",
    "297": "LABEL_297",
    "298": "LABEL_298",
    "299": "LABEL_299",
    "300": "LABEL_300",
    "301": "LABEL_301",
    "302": "LABEL_302",
    "303": "LABEL_303",
    "304": "LABEL_304",
    "305": "LABEL_305",
    "306": "LABEL_306",
    "307": "LABEL_307",
    "308": "LABEL_308",
    "309": "LABEL_309",
    "310": "LABEL_310",
    "311": "LABEL_311",
    "312": "LABEL_312",
    "313": "LABEL_313",
    "314": "LABEL_314",
    "315": "LABEL_315",
    "316": "LABEL_316",
    "317": "LABEL_317",
    "318": "LABEL_318",
    "319": "LABEL_319",
    "320": "LABEL_320",
    "321": "LABEL_321",
    "322": "LABEL_322",
    "323": "LABEL_323",
    "324": "LABEL_324",
    "325": "LABEL_325",
    "326": "LABEL_326",
    "327": "LABEL_327",
    "328": "LABEL_328",
    "329": "LABEL_329",
    "330": "LABEL_330",
    "331": "LABEL_331",
    "332": "LABEL_332",
    "333": "LABEL_333",
    "334": "LABEL_334",
    "335": "LABEL_335",
    "336": "LABEL_336",
    "337": "LABEL_337",
    "338": "LABEL_338",
    "339": "LABEL_339",
    "340": "LABEL_340",
    "341": "LABEL_341",
    "342": "LABEL_342",
    "343": "LABEL_343",
    "344": "LABEL_344",
    "345": "LABEL_345",
    "346": "LABEL_346",
    "347": "LABEL_347",
    "348": "LABEL_348",
    "349": "LABEL_349",
    "350": "LABEL_350",
    "351": "LABEL_351",
    "352": "LABEL_352",
    "353": "LABEL_353",
    "354": "LABEL_354",
    "355": "LABEL_355",
    "356": "LABEL_356",
    "357": "LABEL_357",
    "358": "LABEL_358",
    "359": "LABEL_359",
    "360": "LABEL_360",
    "361": "LABEL_361",
    "362": "LABEL_362",
    "363": "LABEL_363",
    "364": "LABEL_364",
    "365": "LABEL_365",
    "366": "LABEL_366",
    "367": "LABEL_367",
    "368": "LABEL_368",
    "369": "LABEL_369",
    "370": "LABEL_370",
    "371": "LABEL_371",
    "372": "LABEL_372",
    "373": "LABEL_373",
    "374": "LABEL_374",
    "375": "LABEL_375",
    "376": "LABEL_376",
    "377": "LABEL_377",
    "378": "LABEL_378",
    "379": "LABEL_379",
    "380": "LABEL_380",
    "381": "LABEL_381",
    "382": "LABEL_382",
    "383": "LABEL_383",
    "384": "LABEL_384",
    "385": "LABEL_385",
    "386": "LABEL_386",
    "387": "LABEL_387",
    "388": "LABEL_388",
    "389": "LABEL_389",
    "390": "LABEL_390",
    "391": "LABEL_391",
    "392": "LABEL_392",
    "393": "LABEL_393",
    "394": "LABEL_394",
    "395": "LABEL_395",
    "396": "LABEL_396",
    "397": "LABEL_397",
    "398": "LABEL_398",
    "399": "LABEL_399",
    "400": "LABEL_400",
    "401": "LABEL_401",
    "402": "LABEL_402",
    "403": "LABEL_403",
    "404": "LABEL_404",
    "405": "LABEL_405",
    "406": "LABEL_406",
    "407": "LABEL_407",
    "408": "LABEL_408",
    "409": "LABEL_409",
    "410": "LABEL_410",
    "411": "LABEL_411",
    "412": "LABEL_412",
    "413": "LABEL_413",
    "414": "LABEL_414",
    "415": "LABEL_415",
    "416": "LABEL_416",
    "417": "LABEL_417",
    "418": "LABEL_418",
    "419": "LABEL_419",
    "420": "LABEL_420",
    "421": "LABEL_421",
    "422": "LABEL_422",
    "423": "LABEL_423",
    "424": "LABEL_424",
    "425": "LABEL_425",
    "426": "LABEL_426",
    "427": "LABEL_427",
    "428": "LABEL_428",
    "429": "LABEL_429",
    "430": "LABEL_430",
    "431": "LABEL_431",
    "432": "LABEL_432",
    "433": "LABEL_433",
    "434": "LABEL_434",
    "435": "LABEL_435",
    "436": "LABEL_436",
    "437": "LABEL_437",
    "438": "LABEL_438",
    "439": "LABEL_439",
    "440": "LABEL_440",
    "441": "LABEL_441",
    "442": "LABEL_442",
    "443": "LABEL_443",
    "444": "LABEL_444",
    "445": "LABEL_445",
    "446": "LABEL_446",
    "447": "LABEL_447",
    "448": "LABEL_448",
    "449": "LABEL_449",
    "450": "LABEL_450",
    "451": "LABEL_451",
    "452": "LABEL_452",
    "453": "LABEL_453",
    "454": "LABEL_454",
    "455": "LABEL_455",
    "456": "LABEL_456",
    "457": "LABEL_457",
    "458": "LABEL_458",
    "459": "LABEL_459",
    "460": "LABEL_460",
    "461": "LABEL_461",
    "462": "LABEL_462",
    "463": "LABEL_463",
    "464": "LABEL_464",
    "465": "LABEL_465",
    "466": "LABEL_466",
    "467": "LABEL_467",
    "468": "LABEL_468",
    "469": "LABEL_469",
    "470": "LABEL_470",
    "471": "LABEL_471",
    "472": "LABEL_472",
    "473": "LABEL_473",
    "474": "LABEL_474",
    "475": "LABEL_475",
    "476": "LABEL_476",
    "477": "LABEL_477",
    "478": "LABEL_478",
    "479": "LABEL_479",
    "480": "LABEL_480",
    "481": "LABEL_481",
    "482": "LABEL_482",
    "483": "LABEL_483",
    "484": "LABEL_484",
    "485": "LABEL_485",
    "486": "LABEL_486",
    "487": "LABEL_487",
    "488": "LABEL_488",
    "489": "LABEL_489",
    "490": "LABEL_490",
    "491": "LABEL_491",
    "492": "LABEL_492",
    "493": "LABEL_493",
    "494": "LABEL_494",
    "495": "LABEL_495",
    "496": "LABEL_496",
    "497": "LABEL_497",
    "498": "LABEL_498",
    "499": "LABEL_499",
    "500": "LABEL_500",
    "501": "LABEL_501",
    "502": "LABEL_502",
    "503": "LABEL_503",
    "504": "LABEL_504",
    "505": "LABEL_505",
    "506": "LABEL_506",
    "507": "LABEL_507",
    "508": "LABEL_508",
    "509": "LABEL_509",
    "510": "LABEL_510",
    "511": "LABEL_511",
    "512": "LABEL_512",
    "513": "LABEL_513",
    "514": "LABEL_514",
    "515": "LABEL_515",
    "516": "LABEL_516",
    "517": "LABEL_517",
    "518": "LABEL_518",
    "519": "LABEL_519",
    "520": "LABEL_520",
    "521": "LABEL_521",
    "522": "LABEL_522",
    "523": "LABEL_523",
    "524": "LABEL_524",
    "525": "LABEL_525",
    "526": "LABEL_526",
    "527": "LABEL_527",
    "528": "LABEL_528",
    "529": "LABEL_529",
    "530": "LABEL_530",
    "531": "LABEL_531",
    "532": "LABEL_532",
    "533": "LABEL_533",
    "534": "LABEL_534",
    "535": "LABEL_535",
    "536": "LABEL_536",
    "537": "LABEL_537",
    "538": "LABEL_538",
    "539": "LABEL_539",
    "540": "LABEL_540",
    "541": "LABEL_541",
    "542": "LABEL_542",
    "543": "LABEL_543",
    "544": "LABEL_544",
    "545": "LABEL_545",
    "546": "LABEL_546",
    "547": "LABEL_547",
    "548": "LABEL_548",
    "549": "LABEL_549",
    "550": "LABEL_550",
    "551": "LABEL_551",
    "552": "LABEL_552",
    "553": "LABEL_553",
    "554": "LABEL_554",
    "555": "LABEL_555",
    "556": "LABEL_556",
    "557": "LABEL_557",
    "558": "LABEL_558",
    "559": "LABEL_559",
    "560": "LABEL_560",
    "561": "LABEL_561",
    "562": "LABEL_562",
    "563": "LABEL_563",
    "564": "LABEL_564",
    "565": "LABEL_565",
    "566": "LABEL_566",
    "567": "LABEL_567",
    "568": "LABEL_568",
    "569": "LABEL_569",
    "570": "LABEL_570",
    "571": "LABEL_571",
    "572": "LABEL_572",
    "573": "LABEL_573",
    "574": "LABEL_574",
    "575": "LABEL_575",
    "576": "LABEL_576",
    "577": "LABEL_577",
    "578": "LABEL_578",
    "579": "LABEL_579",
    "580": "LABEL_580",
    "581": "LABEL_581",
    "582": "LABEL_582",
    "583": "LABEL_583",
    "584": "LABEL_584",
    "585": "LABEL_585",
    "586": "LABEL_586",
    "587": "LABEL_587",
    "588": "LABEL_588",
    "589": "LABEL_589",
    "590": "LABEL_590",
    "591": "LABEL_591",
    "592": "LABEL_592",
    "593": "LABEL_593",
    "594": "LABEL_594",
    "595": "LABEL_595",
    "596": "LABEL_596",
    "597": "LABEL_597",
    "598": "LABEL_598",
    "599": "LABEL_599",
    "600": "LABEL_600",
    "601": "LABEL_601",
    "602": "LABEL_602",
    "603": "LABEL_603",
    "604": "LABEL_604",
    "605": "LABEL_605",
    "606": "LABEL_606",
    "607": "LABEL_607",
    "608": "LABEL_608",
    "609": "LABEL_609",
    "610": "LABEL_610",
    "611": "LABEL_611",
    "612": "LABEL_612",
    "613": "LABEL_613",
    "614": "LABEL_614",
    "615": "LABEL_615",
    "616": "LABEL_616",
    "617": "LABEL_617",
    "618": "LABEL_618",
    "619": "LABEL_619",
    "620": "LABEL_620",
    "621": "LABEL_621",
    "622": "LABEL_622",
    "623": "LABEL_623",
    "624": "LABEL_624",
    "625": "LABEL_625",
    "626": "LABEL_626",
    "627": "LABEL_627",
    "628": "LABEL_628",
    "629": "LABEL_629",
    "630": "LABEL_630",
    "631": "LABEL_631",
    "632": "LABEL_632",
    "633": "LABEL_633",
    "634": "LABEL_634",
    "635": "LABEL_635",
    "636": "LABEL_636",
    "637": "LABEL_637",
    "638": "LABEL_638",
    "639": "LABEL_639",
    "640": "LABEL_640",
    "641": "LABEL_641",
    "642": "LABEL_642",
    "643": "LABEL_643",
    "644": "LABEL_644",
    "645": "LABEL_645",
    "646": "LABEL_646",
    "647": "LABEL_647",
    "648": "LABEL_648",
    "649": "LABEL_649",
    "650": "LABEL_650",
    "651": "LABEL_651",
    "652": "LABEL_652",
    "653": "LABEL_653",
    "654": "LABEL_654",
    "655": "LABEL_655",
    "656": "LABEL_656",
    "657": "LABEL_657",
    "658": "LABEL_658",
    "659": "LABEL_659",
    "660": "LABEL_660",
    "661": "LABEL_661",
    "662": "LABEL_662",
    "663": "LABEL_663",
    "664": "LABEL_664",
    "665": "LABEL_665",
    "666": "LABEL_666",
    "667": "LABEL_667",
    "668": "LABEL_668",
    "669": "LABEL_669",
    "670": "LABEL_670",
    "671": "LABEL_671",
    "672": "LABEL_672",
    "673": "LABEL_673",
    "674": "LABEL_674",
    "675": "LABEL_675",
    "676": "LABEL_676",
    "677": "LABEL_677",
    "678": "LABEL_678",
    "679": "LABEL_679",
    "680": "LABEL_680",
    "681": "LABEL_681",
    "682": "LABEL_682",
    "683": "LABEL_683",
    "684": "LABEL_684",
    "685": "LABEL_685",
    "686": "LABEL_686",
    "687": "LABEL_687",
    "688": "LABEL_688",
    "689": "LABEL_689",
    "690": "LABEL_690",
    "691": "LABEL_691",
    "692": "LABEL_692",
    "693": "LABEL_693",
    "694": "LABEL_694",
    "695": "LABEL_695",
    "696": "LABEL_696",
    "697": "LABEL_697",
    "698": "LABEL_698",
    "699": "LABEL_699",
    "700": "LABEL_700",
    "701": "LABEL_701",
    "702": "LABEL_702",
    "703": "LABEL_703",
    "704": "LABEL_704",
    "705": "LABEL_705",
    "706": "LABEL_706",
    "707": "LABEL_707",
    "708": "LABEL_708",
    "709": "LABEL_709",
    "710": "LABEL_710",
    "711": "LABEL_711",
    "712": "LABEL_712",
    "713": "LABEL_713",
    "714": "LABEL_714",
    "715": "LABEL_715",
    "716": "LABEL_716",
    "717": "LABEL_717",
    "718": "LABEL_718",
    "719": "LABEL_719",
    "720": "LABEL_720",
    "721": "LABEL_721",
    "722": "LABEL_722",
    "723": "LABEL_723",
    "724": "LABEL_724",
    "725": "LABEL_725",
    "726": "LABEL_726",
    "727": "LABEL_727",
    "728": "LABEL_728",
    "729": "LABEL_729",
    "730": "LABEL_730",
    "731": "LABEL_731",
    "732": "LABEL_732",
    "733": "LABEL_733",
    "734": "LABEL_734",
    "735": "LABEL_735",
    "736": "LABEL_736",
    "737": "LABEL_737",
    "738": "LABEL_738",
    "739": "LABEL_739",
    "740": "LABEL_740",
    "741": "LABEL_741",
    "742": "LABEL_742",
    "743": "LABEL_743",
    "744": "LABEL_744",
    "745": "LABEL_745",
    "746": "LABEL_746",
    "747": "LABEL_747",
    "748": "LABEL_748",
    "749": "LABEL_749",
    "750": "LABEL_750",
    "751": "LABEL_751",
    "752": "LABEL_752",
    "753": "LABEL_753",
    "754": "LABEL_754",
    "755": "LABEL_755",
    "756": "LABEL_756",
    "757": "LABEL_757",
    "758": "LABEL_758",
    "759": "LABEL_759",
    "760": "LABEL_760",
    "761": "LABEL_761",
    "762": "LABEL_762",
    "763": "LABEL_763",
    "764": "LABEL_764",
    "765": "LABEL_765",
    "766": "LABEL_766",
    "767": "LABEL_767",
    "768": "LABEL_768",
    "769": "LABEL_769",
    "770": "LABEL_770",
    "771": "LABEL_771",
    "772": "LABEL_772",
    "773": "LABEL_773",
    "774": "LABEL_774",
    "775": "LABEL_775",
    "776": "LABEL_776",
    "777": "LABEL_777",
    "778": "LABEL_778",
    "779": "LABEL_779",
    "780": "LABEL_780",
    "781": "LABEL_781",
    "782": "LABEL_782",
    "783": "LABEL_783",
    "784": "LABEL_784",
    "785": "LABEL_785",
    "786": "LABEL_786",
    "787": "LABEL_787",
    "788": "LABEL_788",
    "789": "LABEL_789",
    "790": "LABEL_790",
    "791": "LABEL_791",
    "792": "LABEL_792",
    "793": "LABEL_793",
    "794": "LABEL_794",
    "795": "LABEL_795",
    "796": "LABEL_796",
    "797": "LABEL_797",
    "798": "LABEL_798",
    "799": "LABEL_799",
    "800": "LABEL_800",
    "801": "LABEL_801",
    "802": "LABEL_802",
    "803": "LABEL_803",
    "804": "LABEL_804",
    "805": "LABEL_805",
    "806": "LABEL_806",
    "807": "LABEL_807",
    "808": "LABEL_808",
    "809": "LABEL_809",
    "810": "LABEL_810",
    "811": "LABEL_811",
    "812": "LABEL_812",
    "813": "LABEL_813",
    "814": "LABEL_814",
    "815": "LABEL_815",
    "816": "LABEL_816",
    "817": "LABEL_817",
    "818": "LABEL_818",
    "819": "LABEL_819",
    "820": "LABEL_820",
    "821": "LABEL_821",
    "822": "LABEL_822",
    "823": "LABEL_823",
    "824": "LABEL_824",
    "825": "LABEL_825",
    "826": "LABEL_826",
    "827": "LABEL_827",
    "828": "LABEL_828",
    "829": "LABEL_829",
    "830": "LABEL_830",
    "831": "LABEL_831",
    "832": "LABEL_832",
    "833": "LABEL_833",
    "834": "LABEL_834",
    "835": "LABEL_835",
    "836": "LABEL_836",
    "837": "LABEL_837",
    "838": "LABEL_838",
    "839": "LABEL_839",
    "840": "LABEL_840",
    "841": "LABEL_841",
    "842": "LABEL_842",
    "843": "LABEL_843",
    "844": "LABEL_844",
    "845": "LABEL_845",
    "846": "LABEL_846",
    "847": "LABEL_847",
    "848": "LABEL_848",
    "849": "LABEL_849",
    "850": "LABEL_850",
    "851": "LABEL_851",
    "852": "LABEL_852",
    "853": "LABEL_853",
    "854": "LABEL_854",
    "855": "LABEL_855",
    "856": "LABEL_856",
    "857": "LABEL_857",
    "858": "LABEL_858",
    "859": "LABEL_859",
    "860": "LABEL_860",
    "861": "LABEL_861",
    "862": "LABEL_862",
    "863": "LABEL_863",
    "864": "LABEL_864",
    "865": "LABEL_865",
    "866": "LABEL_866",
    "867": "LABEL_867",
    "868": "LABEL_868",
    "869": "LABEL_869",
    "870": "LABEL_870",
    "871": "LABEL_871",
    "872": "LABEL_872",
    "873": "LABEL_873",
    "874": "LABEL_874",
    "875": "LABEL_875",
    "876": "LABEL_876",
    "877": "LABEL_877",
    "878": "LABEL_878",
    "879": "LABEL_879",
    "880": "LABEL_880",
    "881": "LABEL_881",
    "882": "LABEL_882",
    "883": "LABEL_883",
    "884": "LABEL_884",
    "885": "LABEL_885",
    "886": "LABEL_886",
    "887": "LABEL_887",
    "888": "LABEL_888",
    "889": "LABEL_889",
    "890": "LABEL_890",
    "891": "LABEL_891",
    "892": "LABEL_892",
    "893": "LABEL_893",
    "894": "LABEL_894",
    "895": "LABEL_895",
    "896": "LABEL_896",
    "897": "LABEL_897",
    "898": "LABEL_898",
    "899": "LABEL_899",
    "900": "LABEL_900",
    "901": "LABEL_901",
    "902": "LABEL_902",
    "903": "LABEL_903",
    "904": "LABEL_904",
    "905": "LABEL_905",
    "906": "LABEL_906",
    "907": "LABEL_907",
    "908": "LABEL_908",
    "909": "LABEL_909",
    "910": "LABEL_910",
    "911": "LABEL_911",
    "912": "LABEL_912",
    "913": "LABEL_913",
    "914": "LABEL_914",
    "915": "LABEL_915",
    "916": "LABEL_916",
    "917": "LABEL_917",
    "918": "LABEL_918",
    "919": "LABEL_919",
    "920": "LABEL_920",
    "921": "LABEL_921",
    "922": "LABEL_922",
    "923": "LABEL_923",
    "924": "LABEL_924",
    "925": "LABEL_925",
    "926": "LABEL_926",
    "927": "LABEL_927",
    "928": "LABEL_928",
    "929": "LABEL_929",
    "930": "LABEL_930",
    "931": "LABEL_931",
    "932": "LABEL_932",
    "933": "LABEL_933",
    "934": "LABEL_934",
    "935": "LABEL_935",
    "936": "LABEL_936",
    "937": "LABEL_937",
    "938": "LABEL_938",
    "939": "LABEL_939",
    "940": "LABEL_940",
    "941": "LABEL_941",
    "942": "LABEL_942",
    "943": "LABEL_943",
    "944": "LABEL_944",
    "945": "LABEL_945",
    "946": "LABEL_946",
    "947": "LABEL_947",
    "948": "LABEL_948",
    "949": "LABEL_949",
    "950": "LABEL_950",
    "951": "LABEL_951",
    "952": "LABEL_952",
    "953": "LABEL_953",
    "954": "LABEL_954",
    "955": "LABEL_955",
    "956": "LABEL_956",
    "957": "LABEL_957",
    "958": "LABEL_958",
    "959": "LABEL_959",
    "960": "LABEL_960",
    "961": "LABEL_961",
    "962": "LABEL_962",
    "963": "LABEL_963",
    "964": "LABEL_964",
    "965": "LABEL_965",
    "966": "LABEL_966",
    "967": "LABEL_967",
    "968": "LABEL_968",
    "969": "LABEL_969",
    "970": "LABEL_970",
    "971": "LABEL_971",
    "972": "LABEL_972",
    "973": "LABEL_973",
    "974": "LABEL_974",
    "975": "LABEL_975",
    "976": "LABEL_976",
    "977": "LABEL_977",
    "978": "LABEL_978",
    "979": "LABEL_979",
    "980": "LABEL_980",
    "981": "LABEL_981",
    "982": "LABEL_982",
    "983": "LABEL_983",
    "984": "LABEL_984",
    "985": "LABEL_985",
    "986": "LABEL_986",
    "987": "LABEL_987",
    "988": "LABEL_988",
    "989": "LABEL_989",
    "990": "LABEL_990",
    "991": "LABEL_991",
    "992": "LABEL_992",
    "993": "LABEL_993",
    "994": "LABEL_994",
    "995": "LABEL_995",
    "996": "LABEL_996",
    "997": "LABEL_997",
    "998": "LABEL_998",
    "999": "LABEL_999",
    "1000": "LABEL_1000",
    "1001": "LABEL_1001",
    "1002": "LABEL_1002",
    "1003": "LABEL_1003",
    "1004": "LABEL_1004",
    "1005": "LABEL_1005",
    "1006": "LABEL_1006",
    "1007": "LABEL_1007",
    "1008": "LABEL_1008",
    "1009": "LABEL_1009",
    "1010": "LABEL_1010",
    "1011": "LABEL_1011",
    "1012": "LABEL_1012",
    "1013": "LABEL_1013",
    "1014": "LABEL_1014",
    "1015": "LABEL_1015",
    "1016": "LABEL_1016",
    "1017": "LABEL_1017",
    "1018": "LABEL_1018",
    "1019": "LABEL_1019",
    "1020": "LABEL_1020",
    "1021": "LABEL_1021",
    "1022": "LABEL_1022",
    "1023": "LABEL_1023",
    "1024": "LABEL_1024",
    "1025": "LABEL_1025",
    "1026": "LABEL_1026",
    "1027": "LABEL_1027",
    "1028": "LABEL_1028",
    "1029": "LABEL_1029",
    "1030": "LABEL_1030",
    "1031": "LABEL_1031",
    "1032": "LABEL_1032",
    "1033": "LABEL_1033",
    "1034": "LABEL_1034",
    "1035": "LABEL_1035",
    "1036": "LABEL_1036",
    "1037": "LABEL_1037",
    "1038": "LABEL_1038",
    "1039": "LABEL_1039",
    "1040": "LABEL_1040",
    "1041": "LABEL_1041",
    "1042": "LABEL_1042",
    "1043": "LABEL_1043",
    "1044": "LABEL_1044",
    "1045": "LABEL_1045",
    "1046": "LABEL_1046",
    "1047": "LABEL_1047",
    "1048": "LABEL_1048",
    "1049": "LABEL_1049",
    "1050": "LABEL_1050",
    "1051": "LABEL_1051",
    "1052": "LABEL_1052",
    "1053": "LABEL_1053",
    "1054": "LABEL_1054",
    "1055": "LABEL_1055",
    "1056": "LABEL_1056",
    "1057": "LABEL_1057",
    "1058": "LABEL_1058",
    "1059": "LABEL_1059",
    "1060": "LABEL_1060",
    "1061": "LABEL_1061",
    "1062": "LABEL_1062",
    "1063": "LABEL_1063",
    "1064": "LABEL_1064",
    "1065": "LABEL_1065",
    "1066": "LABEL_1066",
    "1067": "LABEL_1067",
    "1068": "LABEL_1068",
    "1069": "LABEL_1069",
    "1070": "LABEL_1070",
    "1071": "LABEL_1071",
    "1072": "LABEL_1072",
    "1073": "LABEL_1073",
    "1074": "LABEL_1074",
    "1075": "LABEL_1075",
    "1076": "LABEL_1076",
    "1077": "LABEL_1077",
    "1078": "LABEL_1078",
    "1079": "LABEL_1079",
    "1080": "LABEL_1080",
    "1081": "LABEL_1081",
    "1082": "LABEL_1082",
    "1083": "LABEL_1083",
    "1084": "LABEL_1084",
    "1085": "LABEL_1085",
    "1086": "LABEL_1086",
    "1087": "LABEL_1087",
    "1088": "LABEL_1088",
    "1089": "LABEL_1089",
    "1090": "LABEL_1090",
    "1091": "LABEL_1091",
    "1092": "LABEL_1092",
    "1093": "LABEL_1093",
    "1094": "LABEL_1094",
    "1095": "LABEL_1095",
    "1096": "LABEL_1096",
    "1097": "LABEL_1097",
    "1098": "LABEL_1098",
    "1099": "LABEL_1099",
    "1100": "LABEL_1100",
    "1101": "LABEL_1101",
    "1102": "LABEL_1102",
    "1103": "LABEL_1103",
    "1104": "LABEL_1104",
    "1105": "LABEL_1105",
    "1106": "LABEL_1106",
    "1107": "LABEL_1107",
    "1108": "LABEL_1108",
    "1109": "LABEL_1109",
    "1110": "LABEL_1110",
    "1111": "LABEL_1111",
    "1112": "LABEL_1112",
    "1113": "LABEL_1113",
    "1114": "LABEL_1114",
    "1115": "LABEL_1115",
    "1116": "LABEL_1116",
    "1117": "LABEL_1117",
    "1118": "LABEL_1118",
    "1119": "LABEL_1119",
    "1120": "LABEL_1120",
    "1121": "LABEL_1121",
    "1122": "LABEL_1122",
    "1123": "LABEL_1123",
    "1124": "LABEL_1124",
    "1125": "LABEL_1125",
    "1126": "LABEL_1126",
    "1127": "LABEL_1127",
    "1128": "LABEL_1128",
    "1129": "LABEL_1129",
    "1130": "LABEL_1130",
    "1131": "LABEL_1131",
    "1132": "LABEL_1132",
    "1133": "LABEL_1133",
    "1134": "LABEL_1134",
    "1135": "LABEL_1135",
    "1136": "LABEL_1136",
    "1137": "LABEL_1137",
    "1138": "LABEL_1138",
    "1139": "LABEL_1139",
    "1140": "LABEL_1140",
    "1141": "LABEL_1141",
    "1142": "LABEL_1142",
    "1143": "LABEL_1143",
    "1144": "LABEL_1144",
    "1145": "LABEL_1145",
    "1146": "LABEL_1146",
    "1147": "LABEL_1147",
    "1148": "LABEL_1148",
    "1149": "LABEL_1149",
    "1150": "LABEL_1150",
    "1151": "LABEL_1151",
    "1152": "LABEL_1152",
    "1153": "LABEL_1153",
    "1154": "LABEL_1154",
    "1155": "LABEL_1155",
    "1156": "LABEL_1156",
    "1157": "LABEL_1157",
    "1158": "LABEL_1158",
    "1159": "LABEL_1159",
    "1160": "LABEL_1160",
    "1161": "LABEL_1161",
    "1162": "LABEL_1162",
    "1163": "LABEL_1163",
    "1164": "LABEL_1164",
    "1165": "LABEL_1165",
    "1166": "LABEL_1166",
    "1167": "LABEL_1167",
    "1168": "LABEL_1168",
    "1169": "LABEL_1169",
    "1170": "LABEL_1170",
    "1171": "LABEL_1171",
    "1172": "LABEL_1172",
    "1173": "LABEL_1173",
    "1174": "LABEL_1174",
    "1175": "LABEL_1175",
    "1176": "LABEL_1176",
    "1177": "LABEL_1177",
    "1178": "LABEL_1178",
    "1179": "LABEL_1179",
    "1180": "LABEL_1180",
    "1181": "LABEL_1181",
    "1182": "LABEL_1182",
    "1183": "LABEL_1183",
    "1184": "LABEL_1184",
    "1185": "LABEL_1185",
    "1186": "LABEL_1186",
    "1187": "LABEL_1187",
    "1188": "LABEL_1188",
    "1189": "LABEL_1189",
    "1190": "LABEL_1190",
    "1191": "LABEL_1191",
    "1192": "LABEL_1192",
    "1193": "LABEL_1193",
    "1194": "LABEL_1194",
    "1195": "LABEL_1195",
    "1196": "LABEL_1196",
    "1197": "LABEL_1197",
    "1198": "LABEL_1198",
    "1199": "LABEL_1199",
    "1200": "LABEL_1200",
    "1201": "LABEL_1201",
    "1202": "LABEL_1202",
    "1203": "LABEL_1203",
    "1204": "LABEL_1204",
    "1205": "LABEL_1205",
    "1206": "LABEL_1206",
    "1207": "LABEL_1207",
    "1208": "LABEL_1208",
    "1209": "LABEL_1209",
    "1210": "LABEL_1210",
    "1211": "LABEL_1211",
    "1212": "LABEL_1212",
    "1213": "LABEL_1213",
    "1214": "LABEL_1214",
    "1215": "LABEL_1215",
    "1216": "LABEL_1216",
    "1217": "LABEL_1217",
    "1218": "LABEL_1218",
    "1219": "LABEL_1219",
    "1220": "LABEL_1220",
    "1221": "LABEL_1221",
    "1222": "LABEL_1222",
    "1223": "LABEL_1223",
    "1224": "LABEL_1224",
    "1225": "LABEL_1225",
    "1226": "LABEL_1226",
    "1227": "LABEL_1227",
    "1228": "LABEL_1228",
    "1229": "LABEL_1229",
    "1230": "LABEL_1230",
    "1231": "LABEL_1231",
    "1232": "LABEL_1232",
    "1233": "LABEL_1233",
    "1234": "LABEL_1234",
    "1235": "LABEL_1235",
    "1236": "LABEL_1236",
    "1237": "LABEL_1237",
    "1238": "LABEL_1238",
    "1239": "LABEL_1239",
    "1240": "LABEL_1240",
    "1241": "LABEL_1241",
    "1242": "LABEL_1242",
    "1243": "LABEL_1243",
    "1244": "LABEL_1244",
    "1245": "LABEL_1245",
    "1246": "LABEL_1246",
    "1247": "LABEL_1247",
    "1248": "LABEL_1248",
    "1249": "LABEL_1249",
    "1250": "LABEL_1250",
    "1251": "LABEL_1251",
    "1252": "LABEL_1252",
    "1253": "LABEL_1253",
    "1254": "LABEL_1254",
    "1255": "LABEL_1255",
    "1256": "LABEL_1256",
    "1257": "LABEL_1257",
    "1258": "LABEL_1258",
    "1259": "LABEL_1259",
    "1260": "LABEL_1260",
    "1261": "LABEL_1261",
    "1262": "LABEL_1262",
    "1263": "LABEL_1263",
    "1264": "LABEL_1264",
    "1265": "LABEL_1265",
    "1266": "LABEL_1266",
    "1267": "LABEL_1267",
    "1268": "LABEL_1268",
    "1269": "LABEL_1269",
    "1270": "LABEL_1270",
    "1271": "LABEL_1271",
    "1272": "LABEL_1272",
    "1273": "LABEL_1273",
    "1274": "LABEL_1274",
    "1275": "LABEL_1275",
    "1276": "LABEL_1276",
    "1277": "LABEL_1277",
    "1278": "LABEL_1278",
    "1279": "LABEL_1279",
    "1280": "LABEL_1280",
    "1281": "LABEL_1281",
    "1282": "LABEL_1282",
    "1283": "LABEL_1283",
    "1284": "LABEL_1284",
    "1285": "LABEL_1285",
    "1286": "LABEL_1286",
    "1287": "LABEL_1287",
    "1288": "LABEL_1288",
    "1289": "LABEL_1289",
    "1290": "LABEL_1290",
    "1291": "LABEL_1291",
    "1292": "LABEL_1292",
    "1293": "LABEL_1293",
    "1294": "LABEL_1294",
    "1295": "LABEL_1295",
    "1296": "LABEL_1296",
    "1297": "LABEL_1297",
    "1298": "LABEL_1298",
    "1299": "LABEL_1299",
    "1300": "LABEL_1300",
    "1301": "LABEL_1301",
    "1302": "LABEL_1302",
    "1303": "LABEL_1303",
    "1304": "LABEL_1304",
    "1305": "LABEL_1305",
    "1306": "LABEL_1306",
    "1307": "LABEL_1307",
    "1308": "LABEL_1308",
    "1309": "LABEL_1309",
    "1310": "LABEL_1310",
    "1311": "LABEL_1311",
    "1312": "LABEL_1312",
    "1313": "LABEL_1313",
    "1314": "LABEL_1314",
    "1315": "LABEL_1315",
    "1316": "LABEL_1316",
    "1317": "LABEL_1317",
    "1318": "LABEL_1318",
    "1319": "LABEL_1319",
    "1320": "LABEL_1320",
    "1321": "LABEL_1321",
    "1322": "LABEL_1322",
    "1323": "LABEL_1323",
    "1324": "LABEL_1324",
    "1325": "LABEL_1325",
    "1326": "LABEL_1326",
    "1327": "LABEL_1327",
    "1328": "LABEL_1328",
    "1329": "LABEL_1329",
    "1330": "LABEL_1330",
    "1331": "LABEL_1331",
    "1332": "LABEL_1332",
    "1333": "LABEL_1333",
    "1334": "LABEL_1334",
    "1335": "LABEL_1335",
    "1336": "LABEL_1336",
    "1337": "LABEL_1337",
    "1338": "LABEL_1338",
    "1339": "LABEL_1339",
    "1340": "LABEL_1340",
    "1341": "LABEL_1341",
    "1342": "LABEL_1342",
    "1343": "LABEL_1343",
    "1344": "LABEL_1344",
    "1345": "LABEL_1345",
    "1346": "LABEL_1346",
    "1347": "LABEL_1347",
    "1348": "LABEL_1348",
    "1349": "LABEL_1349",
    "1350": "LABEL_1350",
    "1351": "LABEL_1351",
    "1352": "LABEL_1352",
    "1353": "LABEL_1353",
    "1354": "LABEL_1354",
    "1355": "LABEL_1355",
    "1356": "LABEL_1356",
    "1357": "LABEL_1357",
    "1358": "LABEL_1358",
    "1359": "LABEL_1359",
    "1360": "LABEL_1360",
    "1361": "LABEL_1361",
    "1362": "LABEL_1362",
    "1363": "LABEL_1363",
    "1364": "LABEL_1364",
    "1365": "LABEL_1365",
    "1366": "LABEL_1366",
    "1367": "LABEL_1367",
    "1368": "LABEL_1368",
    "1369": "LABEL_1369",
    "1370": "LABEL_1370",
    "1371": "LABEL_1371",
    "1372": "LABEL_1372",
    "1373": "LABEL_1373",
    "1374": "LABEL_1374",
    "1375": "LABEL_1375",
    "1376": "LABEL_1376",
    "1377": "LABEL_1377",
    "1378": "LABEL_1378",
    "1379": "LABEL_1379",
    "1380": "LABEL_1380",
    "1381": "LABEL_1381",
    "1382": "LABEL_1382",
    "1383": "LABEL_1383",
    "1384": "LABEL_1384",
    "1385": "LABEL_1385",
    "1386": "LABEL_1386",
    "1387": "LABEL_1387",
    "1388": "LABEL_1388",
    "1389": "LABEL_1389",
    "1390": "LABEL_1390",
    "1391": "LABEL_1391",
    "1392": "LABEL_1392",
    "1393": "LABEL_1393",
    "1394": "LABEL_1394",
    "1395": "LABEL_1395",
    "1396": "LABEL_1396",
    "1397": "LABEL_1397",
    "1398": "LABEL_1398",
    "1399": "LABEL_1399",
    "1400": "LABEL_1400",
    "1401": "LABEL_1401",
    "1402": "LABEL_1402",
    "1403": "LABEL_1403",
    "1404": "LABEL_1404",
    "1405": "LABEL_1405",
    "1406": "LABEL_1406",
    "1407": "LABEL_1407",
    "1408": "LABEL_1408",
    "1409": "LABEL_1409",
    "1410": "LABEL_1410",
    "1411": "LABEL_1411",
    "1412": "LABEL_1412",
    "1413": "LABEL_1413",
    "1414": "LABEL_1414",
    "1415": "LABEL_1415",
    "1416": "LABEL_1416",
    "1417": "LABEL_1417",
    "1418": "LABEL_1418",
    "1419": "LABEL_1419",
    "1420": "LABEL_1420",
    "1421": "LABEL_1421",
    "1422": "LABEL_1422",
    "1423": "LABEL_1423",
    "1424": "LABEL_1424",
    "1425": "LABEL_1425",
    "1426": "LABEL_1426",
    "1427": "LABEL_1427",
    "1428": "LABEL_1428",
    "1429": "LABEL_1429",
    "1430": "LABEL_1430",
    "1431": "LABEL_1431",
    "1432": "LABEL_1432",
    "1433": "LABEL_1433",
    "1434": "LABEL_1434",
    "1435": "LABEL_1435",
    "1436": "LABEL_1436",
    "1437": "LABEL_1437",
    "1438": "LABEL_1438",
    "1439": "LABEL_1439",
    "1440": "LABEL_1440",
    "1441": "LABEL_1441",
    "1442": "LABEL_1442",
    "1443": "LABEL_1443",
    "1444": "LABEL_1444",
    "1445": "LABEL_1445",
    "1446": "LABEL_1446",
    "1447": "LABEL_1447",
    "1448": "LABEL_1448",
    "1449": "LABEL_1449",
    "1450": "LABEL_1450",
    "1451": "LABEL_1451",
    "1452": "LABEL_1452",
    "1453": "LABEL_1453",
    "1454": "LABEL_1454",
    "1455": "LABEL_1455",
    "1456": "LABEL_1456",
    "1457": "LABEL_1457",
    "1458": "LABEL_1458",
    "1459": "LABEL_1459",
    "1460": "LABEL_1460",
    "1461": "LABEL_1461",
    "1462": "LABEL_1462",
    "1463": "LABEL_1463",
    "1464": "LABEL_1464",
    "1465": "LABEL_1465",
    "1466": "LABEL_1466",
    "1467": "LABEL_1467",
    "1468": "LABEL_1468",
    "1469": "LABEL_1469",
    "1470": "LABEL_1470",
    "1471": "LABEL_1471",
    "1472": "LABEL_1472",
    "1473": "LABEL_1473",
    "1474": "LABEL_1474",
    "1475": "LABEL_1475",
    "1476": "LABEL_1476",
    "1477": "LABEL_1477",
    "1478": "LABEL_1478",
    "1479": "LABEL_1479",
    "1480": "LABEL_1480",
    "1481": "LABEL_1481",
    "1482": "LABEL_1482",
    "1483": "LABEL_1483",
    "1484": "LABEL_1484",
    "1485": "LABEL_1485",
    "1486": "LABEL_1486",
    "1487": "LABEL_1487",
    "1488": "LABEL_1488",
    "1489": "LABEL_1489",
    "1490": "LABEL_1490",
    "1491": "LABEL_1491",
    "1492": "LABEL_1492",
    "1493": "LABEL_1493",
    "1494": "LABEL_1494",
    "1495": "LABEL_1495",
    "1496": "LABEL_1496",
    "1497": "LABEL_1497",
    "1498": "LABEL_1498",
    "1499": "LABEL_1499",
    "1500": "LABEL_1500",
    "1501": "LABEL_1501",
    "1502": "LABEL_1502",
    "1503": "LABEL_1503",
    "1504": "LABEL_1504",
    "1505": "LABEL_1505",
    "1506": "LABEL_1506",
    "1507": "LABEL_1507",
    "1508": "LABEL_1508",
    "1509": "LABEL_1509",
    "1510": "LABEL_1510",
    "1511": "LABEL_1511",
    "1512": "LABEL_1512",
    "1513": "LABEL_1513",
    "1514": "LABEL_1514",
    "1515": "LABEL_1515",
    "1516": "LABEL_1516",
    "1517": "LABEL_1517",
    "1518": "LABEL_1518",
    "1519": "LABEL_1519",
    "1520": "LABEL_1520",
    "1521": "LABEL_1521",
    "1522": "LABEL_1522",
    "1523": "LABEL_1523",
    "1524": "LABEL_1524",
    "1525": "LABEL_1525",
    "1526": "LABEL_1526",
    "1527": "LABEL_1527",
    "1528": "LABEL_1528",
    "1529": "LABEL_1529",
    "1530": "LABEL_1530",
    "1531": "LABEL_1531",
    "1532": "LABEL_1532",
    "1533": "LABEL_1533",
    "1534": "LABEL_1534",
    "1535": "LABEL_1535",
    "1536": "LABEL_1536",
    "1537": "LABEL_1537",
    "1538": "LABEL_1538",
    "1539": "LABEL_1539",
    "1540": "LABEL_1540",
    "1541": "LABEL_1541",
    "1542": "LABEL_1542",
    "1543": "LABEL_1543",
    "1544": "LABEL_1544",
    "1545": "LABEL_1545",
    "1546": "LABEL_1546",
    "1547": "LABEL_1547",
    "1548": "LABEL_1548",
    "1549": "LABEL_1549",
    "1550": "LABEL_1550",
    "1551": "LABEL_1551",
    "1552": "LABEL_1552",
    "1553": "LABEL_1553",
    "1554": "LABEL_1554",
    "1555": "LABEL_1555",
    "1556": "LABEL_1556",
    "1557": "LABEL_1557",
    "1558": "LABEL_1558",
    "1559": "LABEL_1559",
    "1560": "LABEL_1560",
    "1561": "LABEL_1561",
    "1562": "LABEL_1562",
    "1563": "LABEL_1563",
    "1564": "LABEL_1564",
    "1565": "LABEL_1565",
    "1566": "LABEL_1566",
    "1567": "LABEL_1567",
    "1568": "LABEL_1568",
    "1569": "LABEL_1569",
    "1570": "LABEL_1570",
    "1571": "LABEL_1571",
    "1572": "LABEL_1572",
    "1573": "LABEL_1573",
    "1574": "LABEL_1574",
    "1575": "LABEL_1575",
    "1576": "LABEL_1576",
    "1577": "LABEL_1577",
    "1578": "LABEL_1578",
    "1579": "LABEL_1579",
    "1580": "LABEL_1580",
    "1581": "LABEL_1581",
    "1582": "LABEL_1582",
    "1583": "LABEL_1583",
    "1584": "LABEL_1584",
    "1585": "LABEL_1585",
    "1586": "LABEL_1586",
    "1587": "LABEL_1587",
    "1588": "LABEL_1588",
    "1589": "LABEL_1589",
    "1590": "LABEL_1590",
    "1591": "LABEL_1591",
    "1592": "LABEL_1592",
    "1593": "LABEL_1593",
    "1594": "LABEL_1594",
    "1595": "LABEL_1595",
    "1596": "LABEL_1596",
    "1597": "LABEL_1597",
    "1598": "LABEL_1598",
    "1599": "LABEL_1599",
    "1600": "LABEL_1600",
    "1601": "LABEL_1601",
    "1602": "LABEL_1602",
    "1603": "LABEL_1603",
    "1604": "LABEL_1604",
    "1605": "LABEL_1605",
    "1606": "LABEL_1606",
    "1607": "LABEL_1607",
    "1608": "LABEL_1608",
    "1609": "LABEL_1609",
    "1610": "LABEL_1610",
    "1611": "LABEL_1611",
    "1612": "LABEL_1612",
    "1613": "LABEL_1613",
    "1614": "LABEL_1614",
    "1615": "LABEL_1615",
    "1616": "LABEL_1616",
    "1617": "LABEL_1617"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_100": 100,
    "LABEL_1000": 1000,
    "LABEL_1001": 1001,
    "LABEL_1002": 1002,
    "LABEL_1003": 1003,
    "LABEL_1004": 1004,
    "LABEL_1005": 1005,
    "LABEL_1006": 1006,
    "LABEL_1007": 1007,
    "LABEL_1008": 1008,
    "LABEL_1009": 1009,
    "LABEL_101": 101,
    "LABEL_1010": 1010,
    "LABEL_1011": 1011,
    "LABEL_1012": 1012,
    "LABEL_1013": 1013,
    "LABEL_1014": 1014,
    "LABEL_1015": 1015,
    "LABEL_1016": 1016,
    "LABEL_1017": 1017,
    "LABEL_1018": 1018,
    "LABEL_1019": 1019,
    "LABEL_102": 102,
    "LABEL_1020": 1020,
    "LABEL_1021": 1021,
    "LABEL_1022": 1022,
    "LABEL_1023": 1023,
    "LABEL_1024": 1024,
    "LABEL_1025": 1025,
    "LABEL_1026": 1026,
    "LABEL_1027": 1027,
    "LABEL_1028": 1028,
    "LABEL_1029": 1029,
    "LABEL_103": 103,
    "LABEL_1030": 1030,
    "LABEL_1031": 1031,
    "LABEL_1032": 1032,
    "LABEL_1033": 1033,
    "LABEL_1034": 1034,
    "LABEL_1035": 1035,
    "LABEL_1036": 1036,
    "LABEL_1037": 1037,
    "LABEL_1038": 1038,
    "LABEL_1039": 1039,
    "LABEL_104": 104,
    "LABEL_1040": 1040,
    "LABEL_1041": 1041,
    "LABEL_1042": 1042,
    "LABEL_1043": 1043,
    "LABEL_1044": 1044,
    "LABEL_1045": 1045,
    "LABEL_1046": 1046,
    "LABEL_1047": 1047,
    "LABEL_1048": 1048,
    "LABEL_1049": 1049,
    "LABEL_105": 105,
    "LABEL_1050": 1050,
    "LABEL_1051": 1051,
    "LABEL_1052": 1052,
    "LABEL_1053": 1053,
    "LABEL_1054": 1054,
    "LABEL_1055": 1055,
    "LABEL_1056": 1056,
    "LABEL_1057": 1057,
    "LABEL_1058": 1058,
    "LABEL_1059": 1059,
    "LABEL_106": 106,
    "LABEL_1060": 1060,
    "LABEL_1061": 1061,
    "LABEL_1062": 1062,
    "LABEL_1063": 1063,
    "LABEL_1064": 1064,
    "LABEL_1065": 1065,
    "LABEL_1066": 1066,
    "LABEL_1067": 1067,
    "LABEL_1068": 1068,
    "LABEL_1069": 1069,
    "LABEL_107": 107,
    "LABEL_1070": 1070,
    "LABEL_1071": 1071,
    "LABEL_1072": 1072,
    "LABEL_1073": 1073,
    "LABEL_1074": 1074,
    "LABEL_1075": 1075,
    "LABEL_1076": 1076,
    "LABEL_1077": 1077,
    "LABEL_1078": 1078,
    "LABEL_1079": 1079,
    "LABEL_108": 108,
    "LABEL_1080": 1080,
    "LABEL_1081": 1081,
    "LABEL_1082": 1082,
    "LABEL_1083": 1083,
    "LABEL_1084": 1084,
    "LABEL_1085": 1085,
    "LABEL_1086": 1086,
    "LABEL_1087": 1087,
    "LABEL_1088": 1088,
    "LABEL_1089": 1089,
    "LABEL_109": 109,
    "LABEL_1090": 1090,
    "LABEL_1091": 1091,
    "LABEL_1092": 1092,
    "LABEL_1093": 1093,
    "LABEL_1094": 1094,
    "LABEL_1095": 1095,
    "LABEL_1096": 1096,
    "LABEL_1097": 1097,
    "LABEL_1098": 1098,
    "LABEL_1099": 1099,
    "LABEL_11": 11,
    "LABEL_110": 110,
    "LABEL_1100": 1100,
    "LABEL_1101": 1101,
    "LABEL_1102": 1102,
    "LABEL_1103": 1103,
    "LABEL_1104": 1104,
    "LABEL_1105": 1105,
    "LABEL_1106": 1106,
    "LABEL_1107": 1107,
    "LABEL_1108": 1108,
    "LABEL_1109": 1109,
    "LABEL_111": 111,
    "LABEL_1110": 1110,
    "LABEL_1111": 1111,
    "LABEL_1112": 1112,
    "LABEL_1113": 1113,
    "LABEL_1114": 1114,
    "LABEL_1115": 1115,
    "LABEL_1116": 1116,
    "LABEL_1117": 1117,
    "LABEL_1118": 1118,
    "LABEL_1119": 1119,
    "LABEL_112": 112,
    "LABEL_1120": 1120,
    "LABEL_1121": 1121,
    "LABEL_1122": 1122,
    "LABEL_1123": 1123,
    "LABEL_1124": 1124,
    "LABEL_1125": 1125,
    "LABEL_1126": 1126,
    "LABEL_1127": 1127,
    "LABEL_1128": 1128,
    "LABEL_1129": 1129,
    "LABEL_113": 113,
    "LABEL_1130": 1130,
    "LABEL_1131": 1131,
    "LABEL_1132": 1132,
    "LABEL_1133": 1133,
    "LABEL_1134": 1134,
    "LABEL_1135": 1135,
    "LABEL_1136": 1136,
    "LABEL_1137": 1137,
    "LABEL_1138": 1138,
    "LABEL_1139": 1139,
    "LABEL_114": 114,
    "LABEL_1140": 1140,
    "LABEL_1141": 1141,
    "LABEL_1142": 1142,
    "LABEL_1143": 1143,
    "LABEL_1144": 1144,
    "LABEL_1145": 1145,
    "LABEL_1146": 1146,
    "LABEL_1147": 1147,
    "LABEL_1148": 1148,
    "LABEL_1149": 1149,
    "LABEL_115": 115,
    "LABEL_1150": 1150,
    "LABEL_1151": 1151,
    "LABEL_1152": 1152,
    "LABEL_1153": 1153,
    "LABEL_1154": 1154,
    "LABEL_1155": 1155,
    "LABEL_1156": 1156,
    "LABEL_1157": 1157,
    "LABEL_1158": 1158,
    "LABEL_1159": 1159,
    "LABEL_116": 116,
    "LABEL_1160": 1160,
    "LABEL_1161": 1161,
    "LABEL_1162": 1162,
    "LABEL_1163": 1163,
    "LABEL_1164": 1164,
    "LABEL_1165": 1165,
    "LABEL_1166": 1166,
    "LABEL_1167": 1167,
    "LABEL_1168": 1168,
    "LABEL_1169": 1169,
    "LABEL_117": 117,
    "LABEL_1170": 1170,
    "LABEL_1171": 1171,
    "LABEL_1172": 1172,
    "LABEL_1173": 1173,
    "LABEL_1174": 1174,
    "LABEL_1175": 1175,
    "LABEL_1176": 1176,
    "LABEL_1177": 1177,
    "LABEL_1178": 1178,
    "LABEL_1179": 1179,
    "LABEL_118": 118,
    "LABEL_1180": 1180,
    "LABEL_1181": 1181,
    "LABEL_1182": 1182,
    "LABEL_1183": 1183,
    "LABEL_1184": 1184,
    "LABEL_1185": 1185,
    "LABEL_1186": 1186,
    "LABEL_1187": 1187,
    "LABEL_1188": 1188,
    "LABEL_1189": 1189,
    "LABEL_119": 119,
    "LABEL_1190": 1190,
    "LABEL_1191": 1191,
    "LABEL_1192": 1192,
    "LABEL_1193": 1193,
    "LABEL_1194": 1194,
    "LABEL_1195": 1195,
    "LABEL_1196": 1196,
    "LABEL_1197": 1197,
    "LABEL_1198": 1198,
    "LABEL_1199": 1199,
    "LABEL_12": 12,
    "LABEL_120": 120,
    "LABEL_1200": 1200,
    "LABEL_1201": 1201,
    "LABEL_1202": 1202,
    "LABEL_1203": 1203,
    "LABEL_1204": 1204,
    "LABEL_1205": 1205,
    "LABEL_1206": 1206,
    "LABEL_1207": 1207,
    "LABEL_1208": 1208,
    "LABEL_1209": 1209,
    "LABEL_121": 121,
    "LABEL_1210": 1210,
    "LABEL_1211": 1211,
    "LABEL_1212": 1212,
    "LABEL_1213": 1213,
    "LABEL_1214": 1214,
    "LABEL_1215": 1215,
    "LABEL_1216": 1216,
    "LABEL_1217": 1217,
    "LABEL_1218": 1218,
    "LABEL_1219": 1219,
    "LABEL_122": 122,
    "LABEL_1220": 1220,
    "LABEL_1221": 1221,
    "LABEL_1222": 1222,
    "LABEL_1223": 1223,
    "LABEL_1224": 1224,
    "LABEL_1225": 1225,
    "LABEL_1226": 1226,
    "LABEL_1227": 1227,
    "LABEL_1228": 1228,
    "LABEL_1229": 1229,
    "LABEL_123": 123,
    "LABEL_1230": 1230,
    "LABEL_1231": 1231,
    "LABEL_1232": 1232,
    "LABEL_1233": 1233,
    "LABEL_1234": 1234,
    "LABEL_1235": 1235,
    "LABEL_1236": 1236,
    "LABEL_1237": 1237,
    "LABEL_1238": 1238,
    "LABEL_1239": 1239,
    "LABEL_124": 124,
    "LABEL_1240": 1240,
    "LABEL_1241": 1241,
    "LABEL_1242": 1242,
    "LABEL_1243": 1243,
    "LABEL_1244": 1244,
    "LABEL_1245": 1245,
    "LABEL_1246": 1246,
    "LABEL_1247": 1247,
    "LABEL_1248": 1248,
    "LABEL_1249": 1249,
    "LABEL_125": 125,
    "LABEL_1250": 1250,
    "LABEL_1251": 1251,
    "LABEL_1252": 1252,
    "LABEL_1253": 1253,
    "LABEL_1254": 1254,
    "LABEL_1255": 1255,
    "LABEL_1256": 1256,
    "LABEL_1257": 1257,
    "LABEL_1258": 1258,
    "LABEL_1259": 1259,
    "LABEL_126": 126,
    "LABEL_1260": 1260,
    "LABEL_1261": 1261,
    "LABEL_1262": 1262,
    "LABEL_1263": 1263,
    "LABEL_1264": 1264,
    "LABEL_1265": 1265,
    "LABEL_1266": 1266,
    "LABEL_1267": 1267,
    "LABEL_1268": 1268,
    "LABEL_1269": 1269,
    "LABEL_127": 127,
    "LABEL_1270": 1270,
    "LABEL_1271": 1271,
    "LABEL_1272": 1272,
    "LABEL_1273": 1273,
    "LABEL_1274": 1274,
    "LABEL_1275": 1275,
    "LABEL_1276": 1276,
    "LABEL_1277": 1277,
    "LABEL_1278": 1278,
    "LABEL_1279": 1279,
    "LABEL_128": 128,
    "LABEL_1280": 1280,
    "LABEL_1281": 1281,
    "LABEL_1282": 1282,
    "LABEL_1283": 1283,
    "LABEL_1284": 1284,
    "LABEL_1285": 1285,
    "LABEL_1286": 1286,
    "LABEL_1287": 1287,
    "LABEL_1288": 1288,
    "LABEL_1289": 1289,
    "LABEL_129": 129,
    "LABEL_1290": 1290,
    "LABEL_1291": 1291,
    "LABEL_1292": 1292,
    "LABEL_1293": 1293,
    "LABEL_1294": 1294,
    "LABEL_1295": 1295,
    "LABEL_1296": 1296,
    "LABEL_1297": 1297,
    "LABEL_1298": 1298,
    "LABEL_1299": 1299,
    "LABEL_13": 13,
    "LABEL_130": 130,
    "LABEL_1300": 1300,
    "LABEL_1301": 1301,
    "LABEL_1302": 1302,
    "LABEL_1303": 1303,
    "LABEL_1304": 1304,
    "LABEL_1305": 1305,
    "LABEL_1306": 1306,
    "LABEL_1307": 1307,
    "LABEL_1308": 1308,
    "LABEL_1309": 1309,
    "LABEL_131": 131,
    "LABEL_1310": 1310,
    "LABEL_1311": 1311,
    "LABEL_1312": 1312,
    "LABEL_1313": 1313,
    "LABEL_1314": 1314,
    "LABEL_1315": 1315,
    "LABEL_1316": 1316,
    "LABEL_1317": 1317,
    "LABEL_1318": 1318,
    "LABEL_1319": 1319,
    "LABEL_132": 132,
    "LABEL_1320": 1320,
    "LABEL_1321": 1321,
    "LABEL_1322": 1322,
    "LABEL_1323": 1323,
    "LABEL_1324": 1324,
    "LABEL_1325": 1325,
    "LABEL_1326": 1326,
    "LABEL_1327": 1327,
    "LABEL_1328": 1328,
    "LABEL_1329": 1329,
    "LABEL_133": 133,
    "LABEL_1330": 1330,
    "LABEL_1331": 1331,
    "LABEL_1332": 1332,
    "LABEL_1333": 1333,
    "LABEL_1334": 1334,
    "LABEL_1335": 1335,
    "LABEL_1336": 1336,
    "LABEL_1337": 1337,
    "LABEL_1338": 1338,
    "LABEL_1339": 1339,
    "LABEL_134": 134,
    "LABEL_1340": 1340,
    "LABEL_1341": 1341,
    "LABEL_1342": 1342,
    "LABEL_1343": 1343,
    "LABEL_1344": 1344,
    "LABEL_1345": 1345,
    "LABEL_1346": 1346,
    "LABEL_1347": 1347,
    "LABEL_1348": 1348,
    "LABEL_1349": 1349,
    "LABEL_135": 135,
    "LABEL_1350": 1350,
    "LABEL_1351": 1351,
    "LABEL_1352": 1352,
    "LABEL_1353": 1353,
    "LABEL_1354": 1354,
    "LABEL_1355": 1355,
    "LABEL_1356": 1356,
    "LABEL_1357": 1357,
    "LABEL_1358": 1358,
    "LABEL_1359": 1359,
    "LABEL_136": 136,
    "LABEL_1360": 1360,
    "LABEL_1361": 1361,
    "LABEL_1362": 1362,
    "LABEL_1363": 1363,
    "LABEL_1364": 1364,
    "LABEL_1365": 1365,
    "LABEL_1366": 1366,
    "LABEL_1367": 1367,
    "LABEL_1368": 1368,
    "LABEL_1369": 1369,
    "LABEL_137": 137,
    "LABEL_1370": 1370,
    "LABEL_1371": 1371,
    "LABEL_1372": 1372,
    "LABEL_1373": 1373,
    "LABEL_1374": 1374,
    "LABEL_1375": 1375,
    "LABEL_1376": 1376,
    "LABEL_1377": 1377,
    "LABEL_1378": 1378,
    "LABEL_1379": 1379,
    "LABEL_138": 138,
    "LABEL_1380": 1380,
    "LABEL_1381": 1381,
    "LABEL_1382": 1382,
    "LABEL_1383": 1383,
    "LABEL_1384": 1384,
    "LABEL_1385": 1385,
    "LABEL_1386": 1386,
    "LABEL_1387": 1387,
    "LABEL_1388": 1388,
    "LABEL_1389": 1389,
    "LABEL_139": 139,
    "LABEL_1390": 1390,
    "LABEL_1391": 1391,
    "LABEL_1392": 1392,
    "LABEL_1393": 1393,
    "LABEL_1394": 1394,
    "LABEL_1395": 1395,
    "LABEL_1396": 1396,
    "LABEL_1397": 1397,
    "LABEL_1398": 1398,
    "LABEL_1399": 1399,
    "LABEL_14": 14,
    "LABEL_140": 140,
    "LABEL_1400": 1400,
    "LABEL_1401": 1401,
    "LABEL_1402": 1402,
    "LABEL_1403": 1403,
    "LABEL_1404": 1404,
    "LABEL_1405": 1405,
    "LABEL_1406": 1406,
    "LABEL_1407": 1407,
    "LABEL_1408": 1408,
    "LABEL_1409": 1409,
    "LABEL_141": 141,
    "LABEL_1410": 1410,
    "LABEL_1411": 1411,
    "LABEL_1412": 1412,
    "LABEL_1413": 1413,
    "LABEL_1414": 1414,
    "LABEL_1415": 1415,
    "LABEL_1416": 1416,
    "LABEL_1417": 1417,
    "LABEL_1418": 1418,
    "LABEL_1419": 1419,
    "LABEL_142": 142,
    "LABEL_1420": 1420,
    "LABEL_1421": 1421,
    "LABEL_1422": 1422,
    "LABEL_1423": 1423,
    "LABEL_1424": 1424,
    "LABEL_1425": 1425,
    "LABEL_1426": 1426,
    "LABEL_1427": 1427,
    "LABEL_1428": 1428,
    "LABEL_1429": 1429,
    "LABEL_143": 143,
    "LABEL_1430": 1430,
    "LABEL_1431": 1431,
    "LABEL_1432": 1432,
    "LABEL_1433": 1433,
    "LABEL_1434": 1434,
    "LABEL_1435": 1435,
    "LABEL_1436": 1436,
    "LABEL_1437": 1437,
    "LABEL_1438": 1438,
    "LABEL_1439": 1439,
    "LABEL_144": 144,
    "LABEL_1440": 1440,
    "LABEL_1441": 1441,
    "LABEL_1442": 1442,
    "LABEL_1443": 1443,
    "LABEL_1444": 1444,
    "LABEL_1445": 1445,
    "LABEL_1446": 1446,
    "LABEL_1447": 1447,
    "LABEL_1448": 1448,
    "LABEL_1449": 1449,
    "LABEL_145": 145,
    "LABEL_1450": 1450,
    "LABEL_1451": 1451,
    "LABEL_1452": 1452,
    "LABEL_1453": 1453,
    "LABEL_1454": 1454,
    "LABEL_1455": 1455,
    "LABEL_1456": 1456,
    "LABEL_1457": 1457,
    "LABEL_1458": 1458,
    "LABEL_1459": 1459,
    "LABEL_146": 146,
    "LABEL_1460": 1460,
    "LABEL_1461": 1461,
    "LABEL_1462": 1462,
    "LABEL_1463": 1463,
    "LABEL_1464": 1464,
    "LABEL_1465": 1465,
    "LABEL_1466": 1466,
    "LABEL_1467": 1467,
    "LABEL_1468": 1468,
    "LABEL_1469": 1469,
    "LABEL_147": 147,
    "LABEL_1470": 1470,
    "LABEL_1471": 1471,
    "LABEL_1472": 1472,
    "LABEL_1473": 1473,
    "LABEL_1474": 1474,
    "LABEL_1475": 1475,
    "LABEL_1476": 1476,
    "LABEL_1477": 1477,
    "LABEL_1478": 1478,
    "LABEL_1479": 1479,
    "LABEL_148": 148,
    "LABEL_1480": 1480,
    "LABEL_1481": 1481,
    "LABEL_1482": 1482,
    "LABEL_1483": 1483,
    "LABEL_1484": 1484,
    "LABEL_1485": 1485,
    "LABEL_1486": 1486,
    "LABEL_1487": 1487,
    "LABEL_1488": 1488,
    "LABEL_1489": 1489,
    "LABEL_149": 149,
    "LABEL_1490": 1490,
    "LABEL_1491": 1491,
    "LABEL_1492": 1492,
    "LABEL_1493": 1493,
    "LABEL_1494": 1494,
    "LABEL_1495": 1495,
    "LABEL_1496": 1496,
    "LABEL_1497": 1497,
    "LABEL_1498": 1498,
    "LABEL_1499": 1499,
    "LABEL_15": 15,
    "LABEL_150": 150,
    "LABEL_1500": 1500,
    "LABEL_1501": 1501,
    "LABEL_1502": 1502,
    "LABEL_1503": 1503,
    "LABEL_1504": 1504,
    "LABEL_1505": 1505,
    "LABEL_1506": 1506,
    "LABEL_1507": 1507,
    "LABEL_1508": 1508,
    "LABEL_1509": 1509,
    "LABEL_151": 151,
    "LABEL_1510": 1510,
    "LABEL_1511": 1511,
    "LABEL_1512": 1512,
    "LABEL_1513": 1513,
    "LABEL_1514": 1514,
    "LABEL_1515": 1515,
    "LABEL_1516": 1516,
    "LABEL_1517": 1517,
    "LABEL_1518": 1518,
    "LABEL_1519": 1519,
    "LABEL_152": 152,
    "LABEL_1520": 1520,
    "LABEL_1521": 1521,
    "LABEL_1522": 1522,
    "LABEL_1523": 1523,
    "LABEL_1524": 1524,
    "LABEL_1525": 1525,
    "LABEL_1526": 1526,
    "LABEL_1527": 1527,
    "LABEL_1528": 1528,
    "LABEL_1529": 1529,
    "LABEL_153": 153,
    "LABEL_1530": 1530,
    "LABEL_1531": 1531,
    "LABEL_1532": 1532,
    "LABEL_1533": 1533,
    "LABEL_1534": 1534,
    "LABEL_1535": 1535,
    "LABEL_1536": 1536,
    "LABEL_1537": 1537,
    "LABEL_1538": 1538,
    "LABEL_1539": 1539,
    "LABEL_154": 154,
    "LABEL_1540": 1540,
    "LABEL_1541": 1541,
    "LABEL_1542": 1542,
    "LABEL_1543": 1543,
    "LABEL_1544": 1544,
    "LABEL_1545": 1545,
    "LABEL_1546": 1546,
    "LABEL_1547": 1547,
    "LABEL_1548": 1548,
    "LABEL_1549": 1549,
    "LABEL_155": 155,
    "LABEL_1550": 1550,
    "LABEL_1551": 1551,
    "LABEL_1552": 1552,
    "LABEL_1553": 1553,
    "LABEL_1554": 1554,
    "LABEL_1555": 1555,
    "LABEL_1556": 1556,
    "LABEL_1557": 1557,
    "LABEL_1558": 1558,
    "LABEL_1559": 1559,
    "LABEL_156": 156,
    "LABEL_1560": 1560,
    "LABEL_1561": 1561,
    "LABEL_1562": 1562,
    "LABEL_1563": 1563,
    "LABEL_1564": 1564,
    "LABEL_1565": 1565,
    "LABEL_1566": 1566,
    "LABEL_1567": 1567,
    "LABEL_1568": 1568,
    "LABEL_1569": 1569,
    "LABEL_157": 157,
    "LABEL_1570": 1570,
    "LABEL_1571": 1571,
    "LABEL_1572": 1572,
    "LABEL_1573": 1573,
    "LABEL_1574": 1574,
    "LABEL_1575": 1575,
    "LABEL_1576": 1576,
    "LABEL_1577": 1577,
    "LABEL_1578": 1578,
    "LABEL_1579": 1579,
    "LABEL_158": 158,
    "LABEL_1580": 1580,
    "LABEL_1581": 1581,
    "LABEL_1582": 1582,
    "LABEL_1583": 1583,
    "LABEL_1584": 1584,
    "LABEL_1585": 1585,
    "LABEL_1586": 1586,
    "LABEL_1587": 1587,
    "LABEL_1588": 1588,
    "LABEL_1589": 1589,
    "LABEL_159": 159,
    "LABEL_1590": 1590,
    "LABEL_1591": 1591,
    "LABEL_1592": 1592,
    "LABEL_1593": 1593,
    "LABEL_1594": 1594,
    "LABEL_1595": 1595,
    "LABEL_1596": 1596,
    "LABEL_1597": 1597,
    "LABEL_1598": 1598,
    "LABEL_1599": 1599,
    "LABEL_16": 16,
    "LABEL_160": 160,
    "LABEL_1600": 1600,
    "LABEL_1601": 1601,
    "LABEL_1602": 1602,
    "LABEL_1603": 1603,
    "LABEL_1604": 1604,
    "LABEL_1605": 1605,
    "LABEL_1606": 1606,
    "LABEL_1607": 1607,
    "LABEL_1608": 1608,
    "LABEL_1609": 1609,
    "LABEL_161": 161,
    "LABEL_1610": 1610,
    "LABEL_1611": 1611,
    "LABEL_1612": 1612,
    "LABEL_1613": 1613,
    "LABEL_1614": 1614,
    "LABEL_1615": 1615,
    "LABEL_1616": 1616,
    "LABEL_1617": 1617,
    "LABEL_162": 162,
    "LABEL_163": 163,
    "LABEL_164": 164,
    "LABEL_165": 165,
    "LABEL_166": 166,
    "LABEL_167": 167,
    "LABEL_168": 168,
    "LABEL_169": 169,
    "LABEL_17": 17,
    "LABEL_170": 170,
    "LABEL_171": 171,
    "LABEL_172": 172,
    "LABEL_173": 173,
    "LABEL_174": 174,
    "LABEL_175": 175,
    "LABEL_176": 176,
    "LABEL_177": 177,
    "LABEL_178": 178,
    "LABEL_179": 179,
    "LABEL_18": 18,
    "LABEL_180": 180,
    "LABEL_181": 181,
    "LABEL_182": 182,
    "LABEL_183": 183,
    "LABEL_184": 184,
    "LABEL_185": 185,
    "LABEL_186": 186,
    "LABEL_187": 187,
    "LABEL_188": 188,
    "LABEL_189": 189,
    "LABEL_19": 19,
    "LABEL_190": 190,
    "LABEL_191": 191,
    "LABEL_192": 192,
    "LABEL_193": 193,
    "LABEL_194": 194,
    "LABEL_195": 195,
    "LABEL_196": 196,
    "LABEL_197": 197,
    "LABEL_198": 198,
    "LABEL_199": 199,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_200": 200,
    "LABEL_201": 201,
    "LABEL_202": 202,
    "LABEL_203": 203,
    "LABEL_204": 204,
    "LABEL_205": 205,
    "LABEL_206": 206,
    "LABEL_207": 207,
    "LABEL_208": 208,
    "LABEL_209": 209,
    "LABEL_21": 21,
    "LABEL_210": 210,
    "LABEL_211": 211,
    "LABEL_212": 212,
    "LABEL_213": 213,
    "LABEL_214": 214,
    "LABEL_215": 215,
    "LABEL_216": 216,
    "LABEL_217": 217,
    "LABEL_218": 218,
    "LABEL_219": 219,
    "LABEL_22": 22,
    "LABEL_220": 220,
    "LABEL_221": 221,
    "LABEL_222": 222,
    "LABEL_223": 223,
    "LABEL_224": 224,
    "LABEL_225": 225,
    "LABEL_226": 226,
    "LABEL_227": 227,
    "LABEL_228": 228,
    "LABEL_229": 229,
    "LABEL_23": 23,
    "LABEL_230": 230,
    "LABEL_231": 231,
    "LABEL_232": 232,
    "LABEL_233": 233,
    "LABEL_234": 234,
    "LABEL_235": 235,
    "LABEL_236": 236,
    "LABEL_237": 237,
    "LABEL_238": 238,
    "LABEL_239": 239,
    "LABEL_24": 24,
    "LABEL_240": 240,
    "LABEL_241": 241,
    "LABEL_242": 242,
    "LABEL_243": 243,
    "LABEL_244": 244,
    "LABEL_245": 245,
    "LABEL_246": 246,
    "LABEL_247": 247,
    "LABEL_248": 248,
    "LABEL_249": 249,
    "LABEL_25": 25,
    "LABEL_250": 250,
    "LABEL_251": 251,
    "LABEL_252": 252,
    "LABEL_253": 253,
    "LABEL_254": 254,
    "LABEL_255": 255,
    "LABEL_256": 256,
    "LABEL_257": 257,
    "LABEL_258": 258,
    "LABEL_259": 259,
    "LABEL_26": 26,
    "LABEL_260": 260,
    "LABEL_261": 261,
    "LABEL_262": 262,
    "LABEL_263": 263,
    "LABEL_264": 264,
    "LABEL_265": 265,
    "LABEL_266": 266,
    "LABEL_267": 267,
    "LABEL_268": 268,
    "LABEL_269": 269,
    "LABEL_27": 27,
    "LABEL_270": 270,
    "LABEL_271": 271,
    "LABEL_272": 272,
    "LABEL_273": 273,
    "LABEL_274": 274,
    "LABEL_275": 275,
    "LABEL_276": 276,
    "LABEL_277": 277,
    "LABEL_278": 278,
    "LABEL_279": 279,
    "LABEL_28": 28,
    "LABEL_280": 280,
    "LABEL_281": 281,
    "LABEL_282": 282,
    "LABEL_283": 283,
    "LABEL_284": 284,
    "LABEL_285": 285,
    "LABEL_286": 286,
    "LABEL_287": 287,
    "LABEL_288": 288,
    "LABEL_289": 289,
    "LABEL_29": 29,
    "LABEL_290": 290,
    "LABEL_291": 291,
    "LABEL_292": 292,
    "LABEL_293": 293,
    "LABEL_294": 294,
    "LABEL_295": 295,
    "LABEL_296": 296,
    "LABEL_297": 297,
    "LABEL_298": 298,
    "LABEL_299": 299,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_300": 300,
    "LABEL_301": 301,
    "LABEL_302": 302,
    "LABEL_303": 303,
    "LABEL_304": 304,
    "LABEL_305": 305,
    "LABEL_306": 306,
    "LABEL_307": 307,
    "LABEL_308": 308,
    "LABEL_309": 309,
    "LABEL_31": 31,
    "LABEL_310": 310,
    "LABEL_311": 311,
    "LABEL_312": 312,
    "LABEL_313": 313,
    "LABEL_314": 314,
    "LABEL_315": 315,
    "LABEL_316": 316,
    "LABEL_317": 317,
    "LABEL_318": 318,
    "LABEL_319": 319,
    "LABEL_32": 32,
    "LABEL_320": 320,
    "LABEL_321": 321,
    "LABEL_322": 322,
    "LABEL_323": 323,
    "LABEL_324": 324,
    "LABEL_325": 325,
    "LABEL_326": 326,
    "LABEL_327": 327,
    "LABEL_328": 328,
    "LABEL_329": 329,
    "LABEL_33": 33,
    "LABEL_330": 330,
    "LABEL_331": 331,
    "LABEL_332": 332,
    "LABEL_333": 333,
    "LABEL_334": 334,
    "LABEL_335": 335,
    "LABEL_336": 336,
    "LABEL_337": 337,
    "LABEL_338": 338,
    "LABEL_339": 339,
    "LABEL_34": 34,
    "LABEL_340": 340,
    "LABEL_341": 341,
    "LABEL_342": 342,
    "LABEL_343": 343,
    "LABEL_344": 344,
    "LABEL_345": 345,
    "LABEL_346": 346,
    "LABEL_347": 347,
    "LABEL_348": 348,
    "LABEL_349": 349,
    "LABEL_35": 35,
    "LABEL_350": 350,
    "LABEL_351": 351,
    "LABEL_352": 352,
    "LABEL_353": 353,
    "LABEL_354": 354,
    "LABEL_355": 355,
    "LABEL_356": 356,
    "LABEL_357": 357,
    "LABEL_358": 358,
    "LABEL_359": 359,
    "LABEL_36": 36,
    "LABEL_360": 360,
    "LABEL_361": 361,
    "LABEL_362": 362,
    "LABEL_363": 363,
    "LABEL_364": 364,
    "LABEL_365": 365,
    "LABEL_366": 366,
    "LABEL_367": 367,
    "LABEL_368": 368,
    "LABEL_369": 369,
    "LABEL_37": 37,
    "LABEL_370": 370,
    "LABEL_371": 371,
    "LABEL_372": 372,
    "LABEL_373": 373,
    "LABEL_374": 374,
    "LABEL_375": 375,
    "LABEL_376": 376,
    "LABEL_377": 377,
    "LABEL_378": 378,
    "LABEL_379": 379,
    "LABEL_38": 38,
    "LABEL_380": 380,
    "LABEL_381": 381,
    "LABEL_382": 382,
    "LABEL_383": 383,
    "LABEL_384": 384,
    "LABEL_385": 385,
    "LABEL_386": 386,
    "LABEL_387": 387,
    "LABEL_388": 388,
    "LABEL_389": 389,
    "LABEL_39": 39,
    "LABEL_390": 390,
    "LABEL_391": 391,
    "LABEL_392": 392,
    "LABEL_393": 393,
    "LABEL_394": 394,
    "LABEL_395": 395,
    "LABEL_396": 396,
    "LABEL_397": 397,
    "LABEL_398": 398,
    "LABEL_399": 399,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_400": 400,
    "LABEL_401": 401,
    "LABEL_402": 402,
    "LABEL_403": 403,
    "LABEL_404": 404,
    "LABEL_405": 405,
    "LABEL_406": 406,
    "LABEL_407": 407,
    "LABEL_408": 408,
    "LABEL_409": 409,
    "LABEL_41": 41,
    "LABEL_410": 410,
    "LABEL_411": 411,
    "LABEL_412": 412,
    "LABEL_413": 413,
    "LABEL_414": 414,
    "LABEL_415": 415,
    "LABEL_416": 416,
    "LABEL_417": 417,
    "LABEL_418": 418,
    "LABEL_419": 419,
    "LABEL_42": 42,
    "LABEL_420": 420,
    "LABEL_421": 421,
    "LABEL_422": 422,
    "LABEL_423": 423,
    "LABEL_424": 424,
    "LABEL_425": 425,
    "LABEL_426": 426,
    "LABEL_427": 427,
    "LABEL_428": 428,
    "LABEL_429": 429,
    "LABEL_43": 43,
    "LABEL_430": 430,
    "LABEL_431": 431,
    "LABEL_432": 432,
    "LABEL_433": 433,
    "LABEL_434": 434,
    "LABEL_435": 435,
    "LABEL_436": 436,
    "LABEL_437": 437,
    "LABEL_438": 438,
    "LABEL_439": 439,
    "LABEL_44": 44,
    "LABEL_440": 440,
    "LABEL_441": 441,
    "LABEL_442": 442,
    "LABEL_443": 443,
    "LABEL_444": 444,
    "LABEL_445": 445,
    "LABEL_446": 446,
    "LABEL_447": 447,
    "LABEL_448": 448,
    "LABEL_449": 449,
    "LABEL_45": 45,
    "LABEL_450": 450,
    "LABEL_451": 451,
    "LABEL_452": 452,
    "LABEL_453": 453,
    "LABEL_454": 454,
    "LABEL_455": 455,
    "LABEL_456": 456,
    "LABEL_457": 457,
    "LABEL_458": 458,
    "LABEL_459": 459,
    "LABEL_46": 46,
    "LABEL_460": 460,
    "LABEL_461": 461,
    "LABEL_462": 462,
    "LABEL_463": 463,
    "LABEL_464": 464,
    "LABEL_465": 465,
    "LABEL_466": 466,
    "LABEL_467": 467,
    "LABEL_468": 468,
    "LABEL_469": 469,
    "LABEL_47": 47,
    "LABEL_470": 470,
    "LABEL_471": 471,
    "LABEL_472": 472,
    "LABEL_473": 473,
    "LABEL_474": 474,
    "LABEL_475": 475,
    "LABEL_476": 476,
    "LABEL_477": 477,
    "LABEL_478": 478,
    "LABEL_479": 479,
    "LABEL_48": 48,
    "LABEL_480": 480,
    "LABEL_481": 481,
    "LABEL_482": 482,
    "LABEL_483": 483,
    "LABEL_484": 484,
    "LABEL_485": 485,
    "LABEL_486": 486,
    "LABEL_487": 487,
    "LABEL_488": 488,
    "LABEL_489": 489,
    "LABEL_49": 49,
    "LABEL_490": 490,
    "LABEL_491": 491,
    "LABEL_492": 492,
    "LABEL_493": 493,
    "LABEL_494": 494,
    "LABEL_495": 495,
    "LABEL_496": 496,
    "LABEL_497": 497,
    "LABEL_498": 498,
    "LABEL_499": 499,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_500": 500,
    "LABEL_501": 501,
    "LABEL_502": 502,
    "LABEL_503": 503,
    "LABEL_504": 504,
    "LABEL_505": 505,
    "LABEL_506": 506,
    "LABEL_507": 507,
    "LABEL_508": 508,
    "LABEL_509": 509,
    "LABEL_51": 51,
    "LABEL_510": 510,
    "LABEL_511": 511,
    "LABEL_512": 512,
    "LABEL_513": 513,
    "LABEL_514": 514,
    "LABEL_515": 515,
    "LABEL_516": 516,
    "LABEL_517": 517,
    "LABEL_518": 518,
    "LABEL_519": 519,
    "LABEL_52": 52,
    "LABEL_520": 520,
    "LABEL_521": 521,
    "LABEL_522": 522,
    "LABEL_523": 523,
    "LABEL_524": 524,
    "LABEL_525": 525,
    "LABEL_526": 526,
    "LABEL_527": 527,
    "LABEL_528": 528,
    "LABEL_529": 529,
    "LABEL_53": 53,
    "LABEL_530": 530,
    "LABEL_531": 531,
    "LABEL_532": 532,
    "LABEL_533": 533,
    "LABEL_534": 534,
    "LABEL_535": 535,
    "LABEL_536": 536,
    "LABEL_537": 537,
    "LABEL_538": 538,
    "LABEL_539": 539,
    "LABEL_54": 54,
    "LABEL_540": 540,
    "LABEL_541": 541,
    "LABEL_542": 542,
    "LABEL_543": 543,
    "LABEL_544": 544,
    "LABEL_545": 545,
    "LABEL_546": 546,
    "LABEL_547": 547,
    "LABEL_548": 548,
    "LABEL_549": 549,
    "LABEL_55": 55,
    "LABEL_550": 550,
    "LABEL_551": 551,
    "LABEL_552": 552,
    "LABEL_553": 553,
    "LABEL_554": 554,
    "LABEL_555": 555,
    "LABEL_556": 556,
    "LABEL_557": 557,
    "LABEL_558": 558,
    "LABEL_559": 559,
    "LABEL_56": 56,
    "LABEL_560": 560,
    "LABEL_561": 561,
    "LABEL_562": 562,
    "LABEL_563": 563,
    "LABEL_564": 564,
    "LABEL_565": 565,
    "LABEL_566": 566,
    "LABEL_567": 567,
    "LABEL_568": 568,
    "LABEL_569": 569,
    "LABEL_57": 57,
    "LABEL_570": 570,
    "LABEL_571": 571,
    "LABEL_572": 572,
    "LABEL_573": 573,
    "LABEL_574": 574,
    "LABEL_575": 575,
    "LABEL_576": 576,
    "LABEL_577": 577,
    "LABEL_578": 578,
    "LABEL_579": 579,
    "LABEL_58": 58,
    "LABEL_580": 580,
    "LABEL_581": 581,
    "LABEL_582": 582,
    "LABEL_583": 583,
    "LABEL_584": 584,
    "LABEL_585": 585,
    "LABEL_586": 586,
    "LABEL_587": 587,
    "LABEL_588": 588,
    "LABEL_589": 589,
    "LABEL_59": 59,
    "LABEL_590": 590,
    "LABEL_591": 591,
    "LABEL_592": 592,
    "LABEL_593": 593,
    "LABEL_594": 594,
    "LABEL_595": 595,
    "LABEL_596": 596,
    "LABEL_597": 597,
    "LABEL_598": 598,
    "LABEL_599": 599,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_600": 600,
    "LABEL_601": 601,
    "LABEL_602": 602,
    "LABEL_603": 603,
    "LABEL_604": 604,
    "LABEL_605": 605,
    "LABEL_606": 606,
    "LABEL_607": 607,
    "LABEL_608": 608,
    "LABEL_609": 609,
    "LABEL_61": 61,
    "LABEL_610": 610,
    "LABEL_611": 611,
    "LABEL_612": 612,
    "LABEL_613": 613,
    "LABEL_614": 614,
    "LABEL_615": 615,
    "LABEL_616": 616,
    "LABEL_617": 617,
    "LABEL_618": 618,
    "LABEL_619": 619,
    "LABEL_62": 62,
    "LABEL_620": 620,
    "LABEL_621": 621,
    "LABEL_622": 622,
    "LABEL_623": 623,
    "LABEL_624": 624,
    "LABEL_625": 625,
    "LABEL_626": 626,
    "LABEL_627": 627,
    "LABEL_628": 628,
    "LABEL_629": 629,
    "LABEL_63": 63,
    "LABEL_630": 630,
    "LABEL_631": 631,
    "LABEL_632": 632,
    "LABEL_633": 633,
    "LABEL_634": 634,
    "LABEL_635": 635,
    "LABEL_636": 636,
    "LABEL_637": 637,
    "LABEL_638": 638,
    "LABEL_639": 639,
    "LABEL_64": 64,
    "LABEL_640": 640,
    "LABEL_641": 641,
    "LABEL_642": 642,
    "LABEL_643": 643,
    "LABEL_644": 644,
    "LABEL_645": 645,
    "LABEL_646": 646,
    "LABEL_647": 647,
    "LABEL_648": 648,
    "LABEL_649": 649,
    "LABEL_65": 65,
    "LABEL_650": 650,
    "LABEL_651": 651,
    "LABEL_652": 652,
    "LABEL_653": 653,
    "LABEL_654": 654,
    "LABEL_655": 655,
    "LABEL_656": 656,
    "LABEL_657": 657,
    "LABEL_658": 658,
    "LABEL_659": 659,
    "LABEL_66": 66,
    "LABEL_660": 660,
    "LABEL_661": 661,
    "LABEL_662": 662,
    "LABEL_663": 663,
    "LABEL_664": 664,
    "LABEL_665": 665,
    "LABEL_666": 666,
    "LABEL_667": 667,
    "LABEL_668": 668,
    "LABEL_669": 669,
    "LABEL_67": 67,
    "LABEL_670": 670,
    "LABEL_671": 671,
    "LABEL_672": 672,
    "LABEL_673": 673,
    "LABEL_674": 674,
    "LABEL_675": 675,
    "LABEL_676": 676,
    "LABEL_677": 677,
    "LABEL_678": 678,
    "LABEL_679": 679,
    "LABEL_68": 68,
    "LABEL_680": 680,
    "LABEL_681": 681,
    "LABEL_682": 682,
    "LABEL_683": 683,
    "LABEL_684": 684,
    "LABEL_685": 685,
    "LABEL_686": 686,
    "LABEL_687": 687,
    "LABEL_688": 688,
    "LABEL_689": 689,
    "LABEL_69": 69,
    "LABEL_690": 690,
    "LABEL_691": 691,
    "LABEL_692": 692,
    "LABEL_693": 693,
    "LABEL_694": 694,
    "LABEL_695": 695,
    "LABEL_696": 696,
    "LABEL_697": 697,
    "LABEL_698": 698,
    "LABEL_699": 699,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_700": 700,
    "LABEL_701": 701,
    "LABEL_702": 702,
    "LABEL_703": 703,
    "LABEL_704": 704,
    "LABEL_705": 705,
    "LABEL_706": 706,
    "LABEL_707": 707,
    "LABEL_708": 708,
    "LABEL_709": 709,
    "LABEL_71": 71,
    "LABEL_710": 710,
    "LABEL_711": 711,
    "LABEL_712": 712,
    "LABEL_713": 713,
    "LABEL_714": 714,
    "LABEL_715": 715,
    "LABEL_716": 716,
    "LABEL_717": 717,
    "LABEL_718": 718,
    "LABEL_719": 719,
    "LABEL_72": 72,
    "LABEL_720": 720,
    "LABEL_721": 721,
    "LABEL_722": 722,
    "LABEL_723": 723,
    "LABEL_724": 724,
    "LABEL_725": 725,
    "LABEL_726": 726,
    "LABEL_727": 727,
    "LABEL_728": 728,
    "LABEL_729": 729,
    "LABEL_73": 73,
    "LABEL_730": 730,
    "LABEL_731": 731,
    "LABEL_732": 732,
    "LABEL_733": 733,
    "LABEL_734": 734,
    "LABEL_735": 735,
    "LABEL_736": 736,
    "LABEL_737": 737,
    "LABEL_738": 738,
    "LABEL_739": 739,
    "LABEL_74": 74,
    "LABEL_740": 740,
    "LABEL_741": 741,
    "LABEL_742": 742,
    "LABEL_743": 743,
    "LABEL_744": 744,
    "LABEL_745": 745,
    "LABEL_746": 746,
    "LABEL_747": 747,
    "LABEL_748": 748,
    "LABEL_749": 749,
    "LABEL_75": 75,
    "LABEL_750": 750,
    "LABEL_751": 751,
    "LABEL_752": 752,
    "LABEL_753": 753,
    "LABEL_754": 754,
    "LABEL_755": 755,
    "LABEL_756": 756,
    "LABEL_757": 757,
    "LABEL_758": 758,
    "LABEL_759": 759,
    "LABEL_76": 76,
    "LABEL_760": 760,
    "LABEL_761": 761,
    "LABEL_762": 762,
    "LABEL_763": 763,
    "LABEL_764": 764,
    "LABEL_765": 765,
    "LABEL_766": 766,
    "LABEL_767": 767,
    "LABEL_768": 768,
    "LABEL_769": 769,
    "LABEL_77": 77,
    "LABEL_770": 770,
    "LABEL_771": 771,
    "LABEL_772": 772,
    "LABEL_773": 773,
    "LABEL_774": 774,
    "LABEL_775": 775,
    "LABEL_776": 776,
    "LABEL_777": 777,
    "LABEL_778": 778,
    "LABEL_779": 779,
    "LABEL_78": 78,
    "LABEL_780": 780,
    "LABEL_781": 781,
    "LABEL_782": 782,
    "LABEL_783": 783,
    "LABEL_784": 784,
    "LABEL_785": 785,
    "LABEL_786": 786,
    "LABEL_787": 787,
    "LABEL_788": 788,
    "LABEL_789": 789,
    "LABEL_79": 79,
    "LABEL_790": 790,
    "LABEL_791": 791,
    "LABEL_792": 792,
    "LABEL_793": 793,
    "LABEL_794": 794,
    "LABEL_795": 795,
    "LABEL_796": 796,
    "LABEL_797": 797,
    "LABEL_798": 798,
    "LABEL_799": 799,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_800": 800,
    "LABEL_801": 801,
    "LABEL_802": 802,
    "LABEL_803": 803,
    "LABEL_804": 804,
    "LABEL_805": 805,
    "LABEL_806": 806,
    "LABEL_807": 807,
    "LABEL_808": 808,
    "LABEL_809": 809,
    "LABEL_81": 81,
    "LABEL_810": 810,
    "LABEL_811": 811,
    "LABEL_812": 812,
    "LABEL_813": 813,
    "LABEL_814": 814,
    "LABEL_815": 815,
    "LABEL_816": 816,
    "LABEL_817": 817,
    "LABEL_818": 818,
    "LABEL_819": 819,
    "LABEL_82": 82,
    "LABEL_820": 820,
    "LABEL_821": 821,
    "LABEL_822": 822,
    "LABEL_823": 823,
    "LABEL_824": 824,
    "LABEL_825": 825,
    "LABEL_826": 826,
    "LABEL_827": 827,
    "LABEL_828": 828,
    "LABEL_829": 829,
    "LABEL_83": 83,
    "LABEL_830": 830,
    "LABEL_831": 831,
    "LABEL_832": 832,
    "LABEL_833": 833,
    "LABEL_834": 834,
    "LABEL_835": 835,
    "LABEL_836": 836,
    "LABEL_837": 837,
    "LABEL_838": 838,
    "LABEL_839": 839,
    "LABEL_84": 84,
    "LABEL_840": 840,
    "LABEL_841": 841,
    "LABEL_842": 842,
    "LABEL_843": 843,
    "LABEL_844": 844,
    "LABEL_845": 845,
    "LABEL_846": 846,
    "LABEL_847": 847,
    "LABEL_848": 848,
    "LABEL_849": 849,
    "LABEL_85": 85,
    "LABEL_850": 850,
    "LABEL_851": 851,
    "LABEL_852": 852,
    "LABEL_853": 853,
    "LABEL_854": 854,
    "LABEL_855": 855,
    "LABEL_856": 856,
    "LABEL_857": 857,
    "LABEL_858": 858,
    "LABEL_859": 859,
    "LABEL_86": 86,
    "LABEL_860": 860,
    "LABEL_861": 861,
    "LABEL_862": 862,
    "LABEL_863": 863,
    "LABEL_864": 864,
    "LABEL_865": 865,
    "LABEL_866": 866,
    "LABEL_867": 867,
    "LABEL_868": 868,
    "LABEL_869": 869,
    "LABEL_87": 87,
    "LABEL_870": 870,
    "LABEL_871": 871,
    "LABEL_872": 872,
    "LABEL_873": 873,
    "LABEL_874": 874,
    "LABEL_875": 875,
    "LABEL_876": 876,
    "LABEL_877": 877,
    "LABEL_878": 878,
    "LABEL_879": 879,
    "LABEL_88": 88,
    "LABEL_880": 880,
    "LABEL_881": 881,
    "LABEL_882": 882,
    "LABEL_883": 883,
    "LABEL_884": 884,
    "LABEL_885": 885,
    "LABEL_886": 886,
    "LABEL_887": 887,
    "LABEL_888": 888,
    "LABEL_889": 889,
    "LABEL_89": 89,
    "LABEL_890": 890,
    "LABEL_891": 891,
    "LABEL_892": 892,
    "LABEL_893": 893,
    "LABEL_894": 894,
    "LABEL_895": 895,
    "LABEL_896": 896,
    "LABEL_897": 897,
    "LABEL_898": 898,
    "LABEL_899": 899,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_900": 900,
    "LABEL_901": 901,
    "LABEL_902": 902,
    "LABEL_903": 903,
    "LABEL_904": 904,
    "LABEL_905": 905,
    "LABEL_906": 906,
    "LABEL_907": 907,
    "LABEL_908": 908,
    "LABEL_909": 909,
    "LABEL_91": 91,
    "LABEL_910": 910,
    "LABEL_911": 911,
    "LABEL_912": 912,
    "LABEL_913": 913,
    "LABEL_914": 914,
    "LABEL_915": 915,
    "LABEL_916": 916,
    "LABEL_917": 917,
    "LABEL_918": 918,
    "LABEL_919": 919,
    "LABEL_92": 92,
    "LABEL_920": 920,
    "LABEL_921": 921,
    "LABEL_922": 922,
    "LABEL_923": 923,
    "LABEL_924": 924,
    "LABEL_925": 925,
    "LABEL_926": 926,
    "LABEL_927": 927,
    "LABEL_928": 928,
    "LABEL_929": 929,
    "LABEL_93": 93,
    "LABEL_930": 930,
    "LABEL_931": 931,
    "LABEL_932": 932,
    "LABEL_933": 933,
    "LABEL_934": 934,
    "LABEL_935": 935,
    "LABEL_936": 936,
    "LABEL_937": 937,
    "LABEL_938": 938,
    "LABEL_939": 939,
    "LABEL_94": 94,
    "LABEL_940": 940,
    "LABEL_941": 941,
    "LABEL_942": 942,
    "LABEL_943": 943,
    "LABEL_944": 944,
    "LABEL_945": 945,
    "LABEL_946": 946,
    "LABEL_947": 947,
    "LABEL_948": 948,
    "LABEL_949": 949,
    "LABEL_95": 95,
    "LABEL_950": 950,
    "LABEL_951": 951,
    "LABEL_952": 952,
    "LABEL_953": 953,
    "LABEL_954": 954,
    "LABEL_955": 955,
    "LABEL_956": 956,
    "LABEL_957": 957,
    "LABEL_958": 958,
    "LABEL_959": 959,
    "LABEL_96": 96,
    "LABEL_960": 960,
    "LABEL_961": 961,
    "LABEL_962": 962,
    "LABEL_963": 963,
    "LABEL_964": 964,
    "LABEL_965": 965,
    "LABEL_966": 966,
    "LABEL_967": 967,
    "LABEL_968": 968,
    "LABEL_969": 969,
    "LABEL_97": 97,
    "LABEL_970": 970,
    "LABEL_971": 971,
    "LABEL_972": 972,
    "LABEL_973": 973,
    "LABEL_974": 974,
    "LABEL_975": 975,
    "LABEL_976": 976,
    "LABEL_977": 977,
    "LABEL_978": 978,
    "LABEL_979": 979,
    "LABEL_98": 98,
    "LABEL_980": 980,
    "LABEL_981": 981,
    "LABEL_982": 982,
    "LABEL_983": 983,
    "LABEL_984": 984,
    "LABEL_985": 985,
    "LABEL_986": 986,
    "LABEL_987": 987,
    "LABEL_988": 988,
    "LABEL_989": 989,
    "LABEL_99": 99,
    "LABEL_990": 990,
    "LABEL_991": 991,
    "LABEL_992": 992,
    "LABEL_993": 993,
    "LABEL_994": 994,
    "LABEL_995": 995,
    "LABEL_996": 996,
    "LABEL_997": 997,
    "LABEL_998": 998,
    "LABEL_999": 999
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "onnx_export": false,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.35.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

01/22/2024 00:21:10 - INFO - __main__ - setting problem type to single_label_classification
[INFO|configuration_utils.py:717] 2024-01-22 00:21:10,795 >> loading configuration file config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/config.json
[INFO|configuration_utils.py:777] 2024-01-22 00:21:10,796 >> Model config LongformerConfig {
  "_name_or_path": "thunlp/Lawformer",
  "architectures": [
    "LongformerForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "onnx_export": false,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.35.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:21:10,806 >> loading file vocab.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:21:10,807 >> loading file merges.txt from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:21:10,807 >> loading file tokenizer.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:21:10,807 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:21:10,807 >> loading file special_tokens_map.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/special_tokens_map.json
[INFO|tokenization_utils_base.py:2022] 2024-01-22 00:21:10,807 >> loading file tokenizer_config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/tokenizer_config.json
[INFO|configuration_utils.py:717] 2024-01-22 00:21:10,807 >> loading configuration file config.json from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/config.json
[INFO|configuration_utils.py:777] 2024-01-22 00:21:10,808 >> Model config LongformerConfig {
  "_name_or_path": "thunlp/Lawformer",
  "architectures": [
    "LongformerForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "onnx_export": false,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.35.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

[INFO|modeling_utils.py:3121] 2024-01-22 00:21:10,848 >> loading weights file pytorch_model.bin from cache at /home/CE/xiaowang/.cache/huggingface/hub/models--thunlp--Lawformer/snapshots/d2452823634a0c5aff74b894c8b86f5ed346b964/pytorch_model.bin
[INFO|modeling_utils.py:3940] 2024-01-22 00:21:12,862 >> Some weights of the model checkpoint at thunlp/Lawformer were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'longformer.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3952] 2024-01-22 00:21:12,863 >> Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at thunlp/Lawformer and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/22/2024 00:21:12 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.
Running tokenizer on dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e2406588f1c6b09a.arrow
01/22/2024 00:21:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e2406588f1c6b09a.arrow
Running tokenizer on dataset:  25%|██▌       | 1000/4000 [00:08<00:24, 124.00 examples/s]Running tokenizer on dataset:  50%|█████     | 2000/4000 [00:10<00:09, 219.78 examples/s]Running tokenizer on dataset:  75%|███████▌  | 3000/4000 [00:11<00:03, 310.37 examples/s]Running tokenizer on dataset: 100%|██████████| 4000/4000 [00:13<00:00, 401.01 examples/s]Running tokenizer on dataset: 100%|██████████| 4000/4000 [00:13<00:00, 300.16 examples/s]
Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-621e4d75effa8c53.arrow
01/22/2024 00:21:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-621e4d75effa8c53.arrow
Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 672.36 examples/s]Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 649.51 examples/s]
Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-8b62084679707114.arrow
01/22/2024 00:21:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/CE/xiaowang/.cache/huggingface/datasets/csv/default-09e0feb6500006c1/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-8b62084679707114.arrow
Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 703.23 examples/s]Running tokenizer on dataset: 100%|██████████| 500/500 [00:00<00:00, 684.00 examples/s]
01/22/2024 00:21:27 - INFO - __main__ - Sample 2619 of the training set: {'date': '2020-09-22', 'verdict': '湖北省丹江口市人民法院刑 事 判 决 书（2020）鄂0381刑初71号公诉机关丹江口市人民检察院。被告人章林，男，1992年5月29日出生于湖北省丹江口市，汉族，初中文化，无固定职业，住湖北省丹江口市。因涉嫌犯贩卖毒品罪，2019年5月29日被十堰市公安局张湾区分局刑事拘留，同年7月5日经十堰市张湾区人民检察院批准逮捕，同日由十堰市公安局张湾区分局执行逮捕。现羁押于十堰市看守所。辩护人史月丽、高宏焕，湖北嘉略律师事务所律师。被告人盛桂玲，女，1994年6月5日出生于湖北省丹江口市，汉族，小学文化，无固定职业，住湖北省丹江口市，户籍所在地：湖北省丹江口市均。曾因吸毒、容留他人吸毒，2018年12月5日被丹江口市公安局行政拘留二十日，2018年12月11日被丹江口市公安局决定社区戒毒三年；因吸毒，2019年5月29日被十堰市公安局张湾区分局行政拘留十五日。因涉嫌犯贩卖毒品罪，2019年6月12日被十堰市公安局张湾区分局刑事拘留，同年7月5日经十堰市张湾区人民检察院批准逮捕，同日由十堰市公安局张湾区分局执行逮捕。现羁押于十堰市看守所。指定辩护人曾洁，湖北润京律师事务所律师。被告人陈小龙，男，1985年7月29日出生于湖北省丹江口市，汉族，初中文化，无固定职业，住湖北省丹江口市。曾因犯聚众斗殴罪、盗窃罪，2006年12月25日被丹江口市人民法院判处有期徒刑四年，2010年9月10日刑满释放；因吸毒，2018年6月28日被丹江口市公安局行政拘留十日，2018年7月6日被丹江口市公安局决定社区戒毒三年；因吸毒，2019年5月29日被十堰市公安局张湾区分局行政拘留十五日，2019年6月10日被十堰市公安局张湾区分局强制隔离戒毒二年。因涉嫌犯贩卖毒品罪，2019年7月5日经十堰市张湾区人民检察院批准逮捕，同年8月13日由十堰市公安局张湾区分局执行逮捕。现羁押于十堰市看守所。指定辩护人汪晓康，湖北玄岳律师事务所律师。丹江口市人民检察院以丹检二部刑诉〔2020〕37号起诉书指控被告人章林犯贩卖、运输毒品罪、容留他人吸毒罪、被告人盛桂玲、陈小龙犯贩卖毒品罪，于2020年3月31日向本院提起公诉。因冠状肺炎疫情自2020年3月31日至2020年7月10日期间不计入审限。本院于2020年7月30日将本案转为普通程序，依法组成合议庭，公开开庭进行了审理。丹江口市人民检察院指派检察员张玲丽、检察官助理姜盈盈出庭支持公诉，被告人章林及其辩护人史月丽、高宏焕，被告人盛桂玲及其指定辩护人曾洁、被告人陈小龙及其指定辩护人汪晓康到庭参加了诉讼。本案经合议庭评议，本院审判委员会讨论并作出决定，现已审理终结。公诉机关指控，一、被告人章林贩卖、运输毒品、被告人盛桂玲贩卖毒品的事实2019年5月初，被告人章林为购买甲基苯丙胺（冰毒）进行贩卖，与朱某（另案处理）一起驾驶车牌号为鄂Ｃ×××**哈弗越野车至天门市多宝镇以7700元（人民币）的价格购买了33克甲基苯丙胺，后又驾车将该33克甲基苯丙胺运输至丹江口市向他人贩卖。其中章林通过盛桂玲向他人贩卖甲基苯丙胺2.2克，通过王某（另案处理）向他人贩卖甲基苯丙胺1.2克，具体事实如下：（一）被告人章林、被告人盛桂玲共同贩卖甲基苯丙胺2.2克的事实1.2019年5月22日22时许，被告人盛桂玲以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。2.2019年5月22日22时许，被告人盛桂玲以350元的价格将被告人章林处0.35克甲基苯丙胺出售给屈某，并收取150元好处费。3.2019年5月23日20时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.25克甲基苯丙胺出售给刘某。4.2019年5月25日17时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给林某。5.2019年5月25日18时许，被告人盛桂玲在丹江口市大坝一路徐家沟建行附近将被告人章林处0.6克甲基苯丙胺出售给刘某。6.2019年5月26日12时许，被告人盛桂玲在丹江口市徐家沟路口以400元的价格将被告人章林处0.4克甲基苯丙胺出售给林某。7.2019年5月27日15时，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。另查明，2019年5月21日11时许，被告人盛桂玲在丹江口城区一水果批发市场，以320元的价格将其持有的从章林之外的贩毒者处购入的0.2克甲基苯丙胺出售给屈某。（二）被告人章林及王某共同贩卖甲基苯丙胺1.2克的事实1.2019年5月23日17时许，被告人章林以400元的价格向王某出售甲基苯丙胺0.4克，后王某将该0.4克甲基苯丙胺以420元的价格出售给“浩哥”。2.2019年5月24日凌晨4时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王晓洁在丹江口市尚一特宾馆楼电梯门口椅子地垫下收到0.2克毒品后转卖给王波获利。3.2019年5月24日凌晨5时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给“浩哥”。4.2019年5月24日15时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波。5.2019年5月24日18时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波，后王波在丹江口市渡口路附近的铁塔处收到所购毒品。（三）被告人章林其他贩卖毒品的事实1.2019年5月22日16时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。2.2019年5月22日22时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。3.2019年5月23日21时许，被告人章林在丹江口市朝歌KTV附近以200元的价格向盛桂玲出售甲基苯丙胺0.25克，盛桂玲收到毒品后自己吸食。4.2019年5月24日11时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王某在丹江口市特宾馆大厅收到0.2克毒品后自己吸食。5.2019年5月26日12时许，被告人章林在丹江口市明珠宾馆门口以150元的价格向盛桂玲出售甲基苯丙胺0.15克，盛桂玲收到毒品后自己吸食。2019年5月28日，被告人章林被公安机关抓获时，民警从其驾驶的鄂Ｃ×××**车辆内查获甲基苯丙胺12包净重2.338克，甲基苯丙胺片剂（俗称“麻果”）1包净重0.094克、电子秤1个。经十堰市公安司法鉴定中心鉴定，从送检的13包疑似毒品中均检出毒品甲基苯丙胺成分。综上，被告人章林贩卖、运输甲基苯丙胺33克；被告人盛桂玲贩卖甲基苯丙胺2.4克。二、被告人章林容留他人吸毒的事实（一）2019年5月21日，被告人章林在丹江口市尚一特宾馆房间内容留朱某、盛桂玲吸食甲基苯丙胺。（二）2019年5月23日，被告人章林在丹江口市尚一特宾馆房间内容留盛桂玲、王某、朱某吸食甲基苯丙胺。（三）2019年5月26日，被告人章林在其停于丹江口市大坝公园停车场的鄂Ｃ×××**哈弗越野车内容留盛桂玲吸食甲基苯丙胺。三、被告人陈小龙贩卖毒品的事实（一）2019年5月23日17时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，后未向张某提供毒品，也未退还该笔毒资。（二）2019年5月25日18时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，通过盛桂玲从章林处以400元的价格购买了0.4克甲基苯丙胺，在丹江口市火车站锦江之星酒店门口交给张某，陈小龙共从中获利113元。（三）2019年5月28日14时许，被告人陈小龙再次收到张某转账450元购买0.4克甲基苯丙胺的毒资，在领取甲基苯丙胺时，被公安机关抓获。公诉机关认为，被告人章林违反国家毒品管理规定贩卖、运输甲基苯丙胺33克；违反国家毒品管理规定，多次容留他人吸食毒品，其行为触犯了《中华人民共和国刑法》第三百四十七条第一款、第三款、第三百五十四条之规定，犯罪事实清楚，证据确实、充分，应当以贩卖、运输毒品罪、容留他人吸毒罪追究其刑事责任，并数罪并罚。被告人盛桂玲违反国家毒品管理规定，单独并伙同章林多次向他人贩卖甲基苯丙胺2.4克，情节严重，其行为触犯了《中华人民共和国刑法》第三百四十七条第一款、第四款之规定，犯罪事实清楚，证据确实、充分，应当以贩卖毒品罪追究其刑事责任。被告人陈小龙违反国家毒品管理规定，向他人贩卖甲基苯丙胺1.2克，其行为触犯了《中华人民共和国刑法》第三百四十七条第一款、第四款之规定，犯罪事实清楚，证据确实、充分，应当以贩卖毒品罪追究其刑事责任。公诉机关就指控的犯罪事实，当庭提交了相关证据予以证明。被告人章林对起诉书指控的犯罪事实及其罪名均无异议。被告人章林的辩护人史月丽、高宏焕提出：被告人章林的运输和贩卖毒品行为具有牵连关系，应当择一重罪论处，不应数罪并罚；被告人章林系初犯，归案后能认罪、悔罪，犯罪的主观恶性和社会危害性较小。被告人盛桂玲对起诉书指控的犯罪事实及其罪名均无异议。被告人盛桂玲的辩护人曾洁提出：被告人盛桂玲协助公安机关抓获同案人章林，系立功；归案后能如实供述犯罪事实，认罪、悔罪，自愿认罪认罚。被告人陈小龙对起诉书指控的犯罪事实及其罪名均无异议。被告人陈小龙的辩护人汪晓康提出：被告人陈小龙贩卖毒品数量较少，归案后能认罪、悔罪，自愿认罪认罚。经审理查明，一、被告人章林贩卖、运输毒品、被告人盛桂玲贩卖毒品的事实2019年5月初，被告人章林为购买甲基苯丙胺（冰毒）进行贩卖，与朱某（另案处理）一起驾驶车牌号为鄂Ｃ×××**哈弗越野车至天门市多宝镇以7700元（人民币）的价格购买了33克甲基苯丙胺，后又驾车将该33克甲基苯丙胺运输至丹江口市向他人贩卖。被告人章林单独或通过被告人盛桂玲及王某向他人贩卖甲基苯丙胺4.4克。被告人盛桂玲单独或伙同他人贩卖甲基苯丙胺2.4克。案发后，查获的13包毒品中有12包净重共2.338克甲基苯丙胺，1包甲基苯丙胺片剂（俗称“麻果”）净重0.094克。具体事实如下：（一）被告人章林贩卖甲基苯丙胺2.2克、被告人盛桂玲贩卖甲基苯丙胺2.4克的事实1.2019年5月22日22时许，被告人盛桂玲以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。2.2019年5月22日22时许，被告人盛桂玲以350元的价格将被告人章林处0.35克甲基苯丙胺出售给屈某，并收取150元好处费。3.2019年5月23日20时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.25克甲基苯丙胺出售给刘某。4.2019年5月25日17时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给林某。5.2019年5月25日18时许，被告人盛桂玲在丹江口市大坝一路徐家沟建行附近将被告人章林处0.6克甲基苯丙胺出售给刘某。6.2019年5月26日12时许，被告人盛桂玲在丹江口市徐家沟路口以400元的价格将被告人章林处0.4克甲基苯丙胺出售给林某。7.2019年5月27日15时，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。8.2019年5月21日11时许，被告人盛桂玲在丹江口城区一水果批发市场，以320元的价格将其持有0.2克甲基苯丙胺（来源非章林处）出售给屈某。（二）被告人章林及王某共同贩卖甲基苯丙胺1.2克的事实1.2019年5月23日17时许，被告人章林以400元的价格向王某出售甲基苯丙胺0.4克，后王某将该0.4克甲基苯丙胺以420元的价格出售给“浩哥”。2.2019年5月24日凌晨4时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王晓洁在丹江口市尚一特宾馆楼电梯门口椅子地垫下收到0.2克毒品后转卖给王波获利。3.2019年5月24日凌晨5时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给“浩哥”。4.2019年5月24日15时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波。5.2019年5月24日18时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波，后王波在丹江口市渡口路附近的铁塔处收到所购毒品。（三）被告人章林贩卖甲基苯丙胺1克的事实1.2019年5月22日16时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。2.2019年5月22日22时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。3.2019年5月23日21时许，被告人章林在丹江口市朝歌KTV附近以200元的价格向盛桂玲出售甲基苯丙胺0.25克，盛桂玲收到毒品后自己吸食。4.2019年5月24日11时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王某在丹江口市特宾馆大厅收到0.2克毒品后自己吸食。5.2019年5月26日12时许，被告人章林在丹江口市明珠宾馆门口以150元的价格向盛桂玲出售甲基苯丙胺0.15克，盛桂玲收到毒品后自己吸食。2019年5月28日，被告人章林被公安机关抓获时，民警从其驾驶的鄂Ｃ×××**车辆内查获甲基苯丙胺12包净重2.338克，甲基苯丙胺片剂（俗称“麻果”）1包净重0.094克、电子秤1个。经十堰市公安司法鉴定中心鉴定，从送检的13包疑似毒品中均检出毒品甲基苯丙胺成分。二、被告人章林容留他人吸毒的事实（一）2019年5月21日，被告人章林在丹江口市尚一特宾馆房间内容留朱某、盛桂玲吸食甲基苯丙胺。（二）2019年5月23日，被告人章林在丹江口市尚一特宾馆房间内容留盛桂玲、王某、朱某吸食甲基苯丙胺。（三）2019年5月26日，被告人章林在其停于丹江口市大坝公园停车场的鄂Ｃ×××**哈弗越野车内容留盛桂玲吸食甲基苯丙胺。三、被告人陈小龙贩卖甲基苯丙胺1.2克的事实被告人陈小龙自2019年5月23日至28日期间，先后三次向张某贩卖甲基苯丙胺共计1.2克，其中0.4克既遂。具体事实如下：（一）2019年5月23日17时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，后未向张某提供毒品，也未退还该笔毒资。（二）2019年5月25日18时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，通过盛桂玲从章林处以400元的价格购买了0.4克甲基苯丙胺，在丹江口市火车站锦江之星酒店门口交给张某，陈小龙共从中获利113元。（三）2019年5月28日14时许，被告人陈小龙再次收到张某转账450元购买0.4克甲基苯丙胺的毒资，在领取甲基苯丙胺时，被公安机关抓获。案发后，公安机关依法扣押电子秤1个、被告人章林持有的OPPO手机1部、被告人盛桂玲持有的OPPO红色手机1部、被告人陈小龙持有的玫瑰金色手机1部，现场查获被告人章林持有的毒品13小袋（已入库）。上述扣押的3部手机及1个电子秤随案移送至本院。在本案侦查阶段，被告人盛桂玲配合公安机关抓获同案人，三被告人均自愿认罪认罚。上述事实，被告人章林、盛桂玲、陈小龙在开庭审理过程中已无异议，且有手机、电子秤等物证、受案登记表、立案决定书、户籍证明、抓获经过、相关刑事判决书、扣押清单、扣押决定书、入库单、行政处罚决定书、强制隔离戒毒决定书、社区戒毒决定书、罪犯档案资料、毒品称量笔录、认罪认罚具结书、湖北省公安机关上交毒品入库登记单、证人林某、屈某等人的证言、提取、辨认笔录、十堰市公安司法鉴定中心毒品检验鉴定报告、视听资料等证据证实，足以认定。本院认为，被告人章林违反国家毒品管理规定，贩卖甲基苯丙胺33克，并多次容留他人吸食毒品，其行为已构成贩卖毒品罪、容留他人吸毒罪。被告人盛桂玲违反国家毒品管理规定，单独或伙同被告人章林多次向他人贩卖甲基苯丙胺2.4克，情节严重，其行为已构成贩卖毒品罪。被告人陈小龙违反国家毒品管理规定，向他人贩卖甲基苯丙胺1.2克，其行为已构成贩卖毒品罪。公诉机关指控被告人章林犯贩卖毒品罪、容留他人吸毒罪，被告人盛桂玲、陈小龙犯贩卖毒品罪的事实清楚，证据确实、充分，罪名成立，本院予以确认。被告人章林购买运输33克甲基苯丙胺后贩卖了4.4克，贩卖目的明确，针对同一宗毒品既有运输又有贩卖，应当按照贩卖毒品33克定罪，不实行数罪并罚；其多次容留他人吸毒，一人犯数罪，应数罪并罚；其归案后能坦白认罪，有一定的悔罪表现，自愿认罪认罚，被告人章林具有法定和酌定从轻处罚情节。被告人章林的辩护人史月丽、高宏焕提出“被告人章林运输、贩卖毒品，应择一重罪处罚；被告人章林系初犯，归案后能认罪、悔罪”的意见，与庭审查明的事实相符，本院对此予以采纳。被告人盛桂玲多次向他人贩卖甲基苯丙胺2.4克，情节严重；其归案后积极配合办案机关抓捕同案人，系一般立功；能坦白认罪，有一定的悔罪表现，自愿认罪认罚，被告人盛桂玲具有法定和酌定从轻处罚情节。被告人盛桂玲的辩护人曾洁提出“被告人盛桂玲归案后有一般立功表现；能如实供述犯罪事实、悔罪，自愿认罪认罚”的意见，与庭审查明的事实相符，本院对此予以采纳。被告人陈小龙贩卖甲基苯丙胺1.2克，其中0.8克被告人陈小龙已经着手实行犯罪，由于其意志以外的原因而未得逞，是犯罪未遂，可以比照既遂犯从轻处罚；其有前科，酌定从重处罚；但归案后能坦白认罪，有一定的悔罪表现，自愿认罪认罚，被告人陈小龙亦具有法定和酌定从轻处罚情节。被告人陈小龙的辩护人汪晓康提出“被告人陈小龙归案后能认罪、悔罪，自愿认罪认罚”的意见，与庭审查明的事实相符，本院对此予以采纳。依照《中华人民共和国刑法》第三百四十七条第一款、第三款、第四款、第三百五十四条、第二十三条、第二十五条、第六十七条第三款、第六十八条、第六十九条、第五十二条、第五十三条、第六十四条之规定，判决如下：一、被告人章林犯贩卖毒品罪，判处有期徒刑十一年六个月，并处罚金人民币一万元；犯容留他人吸毒罪，判处有期徒刑一年，并处罚金人民币二千元，合并执行有期徒刑十一年六个月（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2019年5月29日起至2030年11月28日止），并处罚金人民币一万二千元（在本判决发生法律效力第二日起三十日内缴纳）；二、被告人盛桂玲犯贩卖毒品罪，判处有期徒刑三年（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2019年6月12日起至2022年6月11日止），并处罚金人民币三千元（在本判决发生法律效力第二日起三十日内缴纳）；三、被告人陈小龙犯贩卖毒品罪，判处有期徒刑一年三个月（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2019年8月13日起至2020年11月12日止），并处罚金人民币二千元（在本判决发生法律效力第二日起三十日内缴纳）；四、扣押的涉案物品电子秤1个、章林持有的OPPO品牌手机1部、盛桂玲持有的OPPO红色手机1部、陈小龙持有的玫瑰金色手机1部，依法予以没收。如不服本判决，可在接到判决书的第二日起十日内，通过本院或者直接向湖北省十堰市中级人民法院提出上诉。书面上诉的，应当提交上诉状正本一份，副本二份。审\u3000判\u3000长\u3000\u3000乔海勤审\u3000判\u3000员\u3000\u3000赵本权人民陪审员\u3000\u3000庹\u3000伟二〇二〇年九月二十二日法官助理赵婕书记员杨逸尘本案适用的相关法律依据：《中华人民共和国刑法》第三百四十七条走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：(一)走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的;(二)走私、贩卖、运输、制造毒品集团的首要分子;(三)武装掩护走私、贩卖、运输、制造毒品的;(四)以暴力抗拒检查、拘留、逮捕，情节严重的;(五)参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金;情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。第三百五十四条容留他人吸食、注射毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金。第二十三条已经着手实行犯罪，由于犯罪分子意志以外的原因而未得逞的，是犯罪未遂。对于未遂犯，可以比照既遂犯从轻或者减轻处罚。第二十五条共同犯罪是指二人以上共同故意犯罪。二人以上共同过失犯罪，不以共同犯罪论处;应当负刑事责任的，按照他们所犯的罪分别处罚。第六十七条犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。被采取强制措施的犯罪嫌疑人、被告人和正在服刑的罪犯，如实供述司法机关还未掌握的本人其他罪行的，以自首论。犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚;因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。第六十八条犯罪分子有揭发他人犯罪行为，查证属实的，或者提供重要线索，从而得以侦破其他案件等立功表现的，可以从轻或者减轻处罚;有重大立功表现的，可以减轻或者免除处罚。第六十九条判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。第五十二条判处罚金，应当根据犯罪情节决定罚金数额。第五十三条罚金在判决指定的期限内一次或者分期缴纳。期满不缴纳的，强制缴纳。对于不能全部缴纳罚金的，人民法院在任何时候发现被执行人有可以执行的财产，应当随时追缴。由于遭遇不能抗拒的灾祸等原因缴纳确实有困难的，经人民法院裁定，可以延期缴纳、酌情减少或者免除。第六十四条犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔;对被害人的合法财产，应当及时返还;违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。', 'defendant_ls': "['章林', '盛桂玲', '陈小龙']", 'accusation': ["'走私、贩卖、运输、制造毒品',", "'容留他人吸毒'"], 'article_content': "[{'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第一款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第三款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第四款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百五十四条', '内容': '【容留他人吸毒罪】容留他人吸食、注射毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十三条第一款', '内容': '【犯罪未遂】已经着手实行犯罪，由于犯罪分子意志以外的原因而未得逞的，是犯罪未遂。对于未遂犯，可以比照既遂犯从轻或者减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十三条第二款', '内容': '【犯罪未遂】已经着手实行犯罪，由于犯罪分子意志以外的原因而未得逞的，是犯罪未遂。对于未遂犯，可以比照既遂犯从轻或者减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十五条第一款', '内容': '【共同犯罪的概念】共同犯罪是指二人以上共同故意犯罪。二人以上共同过失犯罪，不以共同犯罪论处；应当负刑事责任的，按照他们所犯的罪分别处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十五条第二款', '内容': '【共同犯罪的概念】共同犯罪是指二人以上共同故意犯罪。二人以上共同过失犯罪，不以共同犯罪论处；应当负刑事责任的，按照他们所犯的罪分别处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十七条第三款', '内容': '【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。被采取强制措施的犯罪嫌疑人、被告人和正在服刑的罪犯，如实供述司法机关还未掌握的本人其他罪行的，以自首论。犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十八条', '内容': '【立功】犯罪分子有揭发他人犯罪行为，查证属实的，或者提供重要线索，从而得以侦破其他案件等立功表现的，可以从轻或者减轻处罚；有重大立功表现的，可以减轻或者免除处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第一款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第二款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第三款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十二条', '内容': '【罚金数额的裁量】判处罚金，应当根据犯罪情节决定罚金数额。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十三条第一款', '内容': '【罚金的缴纳】罚金在判决指定的期限内一次或者分期缴纳。期满不缴纳的，强制缴纳。对于不能全部缴纳罚金的，人民法院在任何时候发现被执行人有可以执行的财产，应当随时追缴。由于遭遇不能抗拒的灾祸等原因缴纳确实有困难的，经人民法院裁定，可以延期缴纳、酌情减少或者免除。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十三条第二款', '内容': '【罚金的缴纳】罚金在判决指定的期限内一次或者分期缴纳。期满不缴纳的，强制缴纳。对于不能全部缴纳罚金的，人民法院在任何时候发现被执行人有可以执行的财产，应当随时追缴。由于遭遇不能抗拒的灾祸等原因缴纳确实有困难的，经人民法院裁定，可以延期缴纳、酌情减少或者免除。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十四条', '内容': '【犯罪物品的处理】犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔；对被害人的合法财产，应当及时返还；违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。'}]", 'province': '湖北省', 'fact': '一、被告人章林贩卖、运输毒品、被告人盛桂玲贩卖毒品的事实2019年5月初，被告人章林为购买甲基苯丙胺（冰毒）进行贩卖，与朱某（另案处理）一起驾驶车牌号为鄂Ｃ×××**哈弗越野车至天门市多宝镇以7700元（人民币）的价格购买了33克甲基苯丙胺，后又驾车将该33克甲基苯丙胺运输至丹江口市向他人贩卖。被告人章林单独或通过被告人盛桂玲及王某向他人贩卖甲基苯丙胺4.4克。被告人盛桂玲单独或伙同他人贩卖甲基苯丙胺2.4克。案发后，查获的13包毒品中有12包净重共2.338克甲基苯丙胺，1包甲基苯丙胺片剂（俗称“麻果”）净重0.094克。具体事实如下：（一）被告人章林贩卖甲基苯丙胺2.2克、被告人盛桂玲贩卖甲基苯丙胺2.4克的事实1.2019年5月22日22时许，被告人盛桂玲以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。2.2019年5月22日22时许，被告人盛桂玲以350元的价格将被告人章林处0.35克甲基苯丙胺出售给屈某，并收取150元好处费。3.2019年5月23日20时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.25克甲基苯丙胺出售给刘某。4.2019年5月25日17时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给林某。5.2019年5月25日18时许，被告人盛桂玲在丹江口市大坝一路徐家沟建行附近将被告人章林处0.6克甲基苯丙胺出售给刘某。6.2019年5月26日12时许，被告人盛桂玲在丹江口市徐家沟路口以400元的价格将被告人章林处0.4克甲基苯丙胺出售给林某。7.2019年5月27日15时，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。8.2019年5月21日11时许，被告人盛桂玲在丹江口城区一水果批发市场，以320元的价格将其持有0.2克甲基苯丙胺（来源非章林处）出售给屈某。（二）被告人章林及王某共同贩卖甲基苯丙胺1.2克的事实1.2019年5月23日17时许，被告人章林以400元的价格向王某出售甲基苯丙胺0.4克，后王某将该0.4克甲基苯丙胺以420元的价格出售给“浩哥”。2.2019年5月24日凌晨4时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王晓洁在丹江口市尚一特宾馆楼电梯门口椅子地垫下收到0.2克毒品后转卖给王波获利。3.2019年5月24日凌晨5时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给“浩哥”。4.2019年5月24日15时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波。5.2019年5月24日18时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波，后王波在丹江口市渡口路附近的铁塔处收到所购毒品。（三）被告人章林贩卖甲基苯丙胺1克的事实1.2019年5月22日16时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。2.2019年5月22日22时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。3.2019年5月23日21时许，被告人章林在丹江口市朝歌KTV附近以200元的价格向盛桂玲出售甲基苯丙胺0.25克，盛桂玲收到毒品后自己吸食。4.2019年5月24日11时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王某在丹江口市特宾馆大厅收到0.2克毒品后自己吸食。5.2019年5月26日12时许，被告人章林在丹江口市明珠宾馆门口以150元的价格向盛桂玲出售甲基苯丙胺0.15克，盛桂玲收到毒品后自己吸食。2019年5月28日，被告人章林被公安机关抓获时，民警从其驾驶的鄂Ｃ×××**车辆内查获甲基苯丙胺12包净重2.338克，甲基苯丙胺片剂（俗称“麻果”）1包净重0.094克、电子秤1个。经十堰市公安司法鉴定中心鉴定，从送检的13包疑似毒品中均检出毒品甲基苯丙胺成分。二、被告人章林容留他人吸毒的事实（一）2019年5月21日，被告人章林在丹江口市尚一特宾馆房间内容留朱某、盛桂玲吸食甲基苯丙胺。（二）2019年5月23日，被告人章林在丹江口市尚一特宾馆房间内容留盛桂玲、王某、朱某吸食甲基苯丙胺。（三）2019年5月26日，被告人章林在其停于丹江口市大坝公园停车场的鄂Ｃ×××**哈弗越野车内容留盛桂玲吸食甲基苯丙胺。三、被告人陈小龙贩卖甲基苯丙胺1.2克的事实被告人陈小龙自2019年5月23日至28日期间，先后三次向张某贩卖甲基苯丙胺共计1.2克，其中0.4克既遂。具体事实如下：（一）2019年5月23日17时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，后未向张某提供毒品，也未退还该笔毒资。（二）2019年5月25日18时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，通过盛桂玲从章林处以400元的价格购买了0.4克甲基苯丙胺，在丹江口市火车站锦江之星酒店门口交给张某，陈小龙共从中获利113元。（三）2019年5月28日14时许，被告人陈小龙再次收到张某转账450元购买0.4克甲基苯丙胺的毒资，在领取甲基苯丙胺时，被公安机关抓获。案发后，公安机关依法扣押电子秤1个、被告人章林持有的OPPO手机1部、被告人盛桂玲持有的OPPO红色手机1部、被告人陈小龙持有的玫瑰金色手机1部，现场查获被告人章林持有的毒品13小袋（已入库）。上述扣押的3部手机及1个电子秤随案移送至本院。在本案侦查阶段，被告人盛桂玲配合公安机关抓获同案人，三被告人均自愿认罪认罚。上述事实，被告人章林、盛桂玲、陈小龙在开庭审理过程中已无异议，且有手机、电子秤等物证、受案登记表、立案决定书、户籍证明、抓获经过、相关刑事判决书、扣押清单、扣押决定书、入库单、行政处罚决定书、强制隔离戒毒决定书、社区戒毒决定书、罪犯档案资料、毒品称量笔录、认罪认罚具结书、湖北省公安机关上交毒品入库登记单、证人林某、屈某等人的证言、提取、辨认笔录、十堰市公安司法鉴定中心毒品检验鉴定报告、视听资料等证据证实，足以认定', 'label': 1247, 'relevant_article': ['354.0,', '347.0'], 'defendant_judgement': "[{'走私、贩卖、运输、制造毒品': {'有期徒刑': '十一年六个月'}}, {'容留他人吸毒': {'有期徒刑': '一年'}}, {'有期徒刑': '十一年六个月'}]", 'defendant': '[CLS]章林[SEP]一、被告人章林贩卖、运输毒品、被告人盛桂玲贩卖毒品的事实2019年5月初，被告人章林为购买甲基苯丙胺（冰毒）进行贩卖，与朱某（另案处理）一起驾驶车牌号为鄂Ｃ×××**哈弗越野车至天门市多宝镇以7700元（人民币）的价格购买了33克甲基苯丙胺，后又驾车将该33克甲基苯丙胺运输至丹江口市向他人贩卖。被告人章林单独或通过被告人盛桂玲及王某向他人贩卖甲基苯丙胺4.4克。被告人盛桂玲单独或伙同他人贩卖甲基苯丙胺2.4克。案发后，查获的13包毒品中有12包净重共2.338克甲基苯丙胺，1包甲基苯丙胺片剂（俗称“麻果”）净重0.094克。具体事实如下：（一）被告人章林贩卖甲基苯丙胺2.2克、被告人盛桂玲贩卖甲基苯丙胺2.4克的事实1.2019年5月22日22时许，被告人盛桂玲以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。2.2019年5月22日22时许，被告人盛桂玲以350元的价格将被告人章林处0.35克甲基苯丙胺出售给屈某，并收取150元好处费。3.2019年5月23日20时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.25克甲基苯丙胺出售给刘某。4.2019年5月25日17时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给林某。5.2019年5月25日18时许，被告人盛桂玲在丹江口市大坝一路徐家沟建行附近将被告人章林处0.6克甲基苯丙胺出售给刘某。6.2019年5月26日12时许，被告人盛桂玲在丹江口市徐家沟路口以400元的价格将被告人章林处0.4克甲基苯丙胺出售给林某。7.2019年5月27日15时，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。8.2019年5月21日11时许，被告人盛桂玲在丹江口城区一水果批发市场，以320元的价格将其持有0.2克甲基苯丙胺（来源非章林处）出售给屈某。（二）被告人章林及王某共同贩卖甲基苯丙胺1.2克的事实1.2019年5月23日17时许，被告人章林以400元的价格向王某出售甲基苯丙胺0.4克，后王某将该0.4克甲基苯丙胺以420元的价格出售给“浩哥”。2.2019年5月24日凌晨4时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王晓洁在丹江口市尚一特宾馆楼电梯门口椅子地垫下收到0.2克毒品后转卖给王波获利。3.2019年5月24日凌晨5时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给“浩哥”。4.2019年5月24日15时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波。5.2019年5月24日18时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波，后王波在丹江口市渡口路附近的铁塔处收到所购毒品。（三）被告人章林贩卖甲基苯丙胺1克的事实1.2019年5月22日16时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。2.2019年5月22日22时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。3.2019年5月23日21时许，被告人章林在丹江口市朝歌KTV附近以200元的价格向盛桂玲出售甲基苯丙胺0.25克，盛桂玲收到毒品后自己吸食。4.2019年5月24日11时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王某在丹江口市特宾馆大厅收到0.2克毒品后自己吸食。5.2019年5月26日12时许，被告人章林在丹江口市明珠宾馆门口以150元的价格向盛桂玲出售甲基苯丙胺0.15克，盛桂玲收到毒品后自己吸食。2019年5月28日，被告人章林被公安机关抓获时，民警从其驾驶的鄂Ｃ×××**车辆内查获甲基苯丙胺12包净重2.338克，甲基苯丙胺片剂（俗称“麻果”）1包净重0.094克、电子秤1个。经十堰市公安司法鉴定中心鉴定，从送检的13包疑似毒品中均检出毒品甲基苯丙胺成分。二、被告人章林容留他人吸毒的事实（一）2019年5月21日，被告人章林在丹江口市尚一特宾馆房间内容留朱某、盛桂玲吸食甲基苯丙胺。（二）2019年5月23日，被告人章林在丹江口市尚一特宾馆房间内容留盛桂玲、王某、朱某吸食甲基苯丙胺。（三）2019年5月26日，被告人章林在其停于丹江口市大坝公园停车场的鄂Ｃ×××**哈弗越野车内容留盛桂玲吸食甲基苯丙胺。三、被告人陈小龙贩卖甲基苯丙胺1.2克的事实被告人陈小龙自2019年5月23日至28日期间，先后三次向张某贩卖甲基苯丙胺共计1.2克，其中0.4克既遂。具体事实如下：（一）2019年5月23日17时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，后未向张某提供毒品，也未退还该笔毒资。（二）2019年5月25日18时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，通过盛桂玲从章林处以400元的价格购买了0.4克甲基苯丙胺，在丹江口市火车站锦江之星酒店门口交给张某，陈小龙共从中获利113元。（三）2019年5月28日14时许，被告人陈小龙再次收到张某转账450元购买0.4克甲基苯丙胺的毒资，在领取甲基苯丙胺时，被公安机关抓获。案发后，公安机关依法扣押电子秤1个、被告人章林持有的OPPO手机1部、被告人盛桂玲持有的OPPO红色手机1部、被告人陈小龙持有的玫瑰金色手机1部，现场查获被告人章林持有的毒品13小袋（已入库）。上述扣押的3部手机及1个电子秤随案移送至本院。在本案侦查阶段，被告人盛桂玲配合公安机关抓获同案人，三被告人均自愿认罪认罚。上述事实，被告人章林、盛桂玲、陈小龙在开庭审理过程中已无异议，且有手机、电子秤等物证、受案登记表、立案决定书、户籍证明、抓获经过、相关刑事判决书、扣押清单、扣押决定书、入库单、行政处罚决定书、强制隔离戒毒决定书、社区戒毒决定书、罪犯档案资料、毒品称量笔录、认罪认罚具结书、湖北省公安机关上交毒品入库登记单、证人林某、屈某等人的证言、提取、辨认笔录、十堰市公安司法鉴定中心毒品检验鉴定报告、视听资料等证据证实，足以认定', 'imprisonment': 138.0, 'sentence': '[CLS]章林[SEP]一、被告人章林贩卖、运输毒品、被告人盛桂玲贩卖毒品的事实2019年5月初，被告人章林为购买甲基苯丙胺（冰毒）进行贩卖，与朱某（另案处理）一起驾驶车牌号为鄂Ｃ×××**哈弗越野车至天门市多宝镇以7700元（人民币）的价格购买了33克甲基苯丙胺，后又驾车将该33克甲基苯丙胺运输至丹江口市向他人贩卖。被告人章林单独或通过被告人盛桂玲及王某向他人贩卖甲基苯丙胺4.4克。被告人盛桂玲单独或伙同他人贩卖甲基苯丙胺2.4克。案发后，查获的13包毒品中有12包净重共2.338克甲基苯丙胺，1包甲基苯丙胺片剂（俗称“麻果”）净重0.094克。具体事实如下：（一）被告人章林贩卖甲基苯丙胺2.2克、被告人盛桂玲贩卖甲基苯丙胺2.4克的事实1.2019年5月22日22时许，被告人盛桂玲以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。2.2019年5月22日22时许，被告人盛桂玲以350元的价格将被告人章林处0.35克甲基苯丙胺出售给屈某，并收取150元好处费。3.2019年5月23日20时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.25克甲基苯丙胺出售给刘某。4.2019年5月25日17时许，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给林某。5.2019年5月25日18时许，被告人盛桂玲在丹江口市大坝一路徐家沟建行附近将被告人章林处0.6克甲基苯丙胺出售给刘某。6.2019年5月26日12时许，被告人盛桂玲在丹江口市徐家沟路口以400元的价格将被告人章林处0.4克甲基苯丙胺出售给林某。7.2019年5月27日15时，被告人盛桂玲在丹江口市徐家沟路口以200元的价格将被告人章林处0.2克甲基苯丙胺出售给刘某。8.2019年5月21日11时许，被告人盛桂玲在丹江口城区一水果批发市场，以320元的价格将其持有0.2克甲基苯丙胺（来源非章林处）出售给屈某。（二）被告人章林及王某共同贩卖甲基苯丙胺1.2克的事实1.2019年5月23日17时许，被告人章林以400元的价格向王某出售甲基苯丙胺0.4克，后王某将该0.4克甲基苯丙胺以420元的价格出售给“浩哥”。2.2019年5月24日凌晨4时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王晓洁在丹江口市尚一特宾馆楼电梯门口椅子地垫下收到0.2克毒品后转卖给王波获利。3.2019年5月24日凌晨5时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给“浩哥”。4.2019年5月24日15时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波。5.2019年5月24日18时许，被告人章林通过王某以200元的价格出售0.2克甲基苯丙胺给王波，后王波在丹江口市渡口路附近的铁塔处收到所购毒品。（三）被告人章林贩卖甲基苯丙胺1克的事实1.2019年5月22日16时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。2.2019年5月22日22时许，被告人章林在丹江口市特宾馆以200元的价格向盛桂玲出售甲基苯丙胺0.2克，盛桂玲收到毒品后自己吸食。3.2019年5月23日21时许，被告人章林在丹江口市朝歌KTV附近以200元的价格向盛桂玲出售甲基苯丙胺0.25克，盛桂玲收到毒品后自己吸食。4.2019年5月24日11时许，被告人章林以200元的价格向王某出售甲基苯丙胺0.2克，王某在丹江口市特宾馆大厅收到0.2克毒品后自己吸食。5.2019年5月26日12时许，被告人章林在丹江口市明珠宾馆门口以150元的价格向盛桂玲出售甲基苯丙胺0.15克，盛桂玲收到毒品后自己吸食。2019年5月28日，被告人章林被公安机关抓获时，民警从其驾驶的鄂Ｃ×××**车辆内查获甲基苯丙胺12包净重2.338克，甲基苯丙胺片剂（俗称“麻果”）1包净重0.094克、电子秤1个。经十堰市公安司法鉴定中心鉴定，从送检的13包疑似毒品中均检出毒品甲基苯丙胺成分。二、被告人章林容留他人吸毒的事实（一）2019年5月21日，被告人章林在丹江口市尚一特宾馆房间内容留朱某、盛桂玲吸食甲基苯丙胺。（二）2019年5月23日，被告人章林在丹江口市尚一特宾馆房间内容留盛桂玲、王某、朱某吸食甲基苯丙胺。（三）2019年5月26日，被告人章林在其停于丹江口市大坝公园停车场的鄂Ｃ×××**哈弗越野车内容留盛桂玲吸食甲基苯丙胺。三、被告人陈小龙贩卖甲基苯丙胺1.2克的事实被告人陈小龙自2019年5月23日至28日期间，先后三次向张某贩卖甲基苯丙胺共计1.2克，其中0.4克既遂。具体事实如下：（一）2019年5月23日17时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，后未向张某提供毒品，也未退还该笔毒资。（二）2019年5月25日18时许，被告人陈小龙收到张某450元购买0.4克甲基苯丙胺的毒资，通过盛桂玲从章林处以400元的价格购买了0.4克甲基苯丙胺，在丹江口市火车站锦江之星酒店门口交给张某，陈小龙共从中获利113元。（三）2019年5月28日14时许，被告人陈小龙再次收到张某转账450元购买0.4克甲基苯丙胺的毒资，在领取甲基苯丙胺时，被公安机关抓获。案发后，公安机关依法扣押电子秤1个、被告人章林持有的OPPO手机1部、被告人盛桂玲持有的OPPO红色手机1部、被告人陈小龙持有的玫瑰金色手机1部，现场查获被告人章林持有的毒品13小袋（已入库）。上述扣押的3部手机及1个电子秤随案移送至本院。在本案侦查阶段，被告人盛桂玲配合公安机关抓获同案人，三被告人均自愿认罪认罚。上述事实，被告人章林、盛桂玲、陈小龙在开庭审理过程中已无异议，且有手机、电子秤等物证、受案登记表、立案决定书、户籍证明、抓获经过、相关刑事判决书、扣押清单、扣押决定书、入库单、行政处罚决定书、强制隔离戒毒决定书、社区戒毒决定书、罪犯档案资料、毒品称量笔录、认罪认罚具结书、湖北省公安机关上交毒品入库登记单、证人林某、屈某等人的证言、提取、辨认笔录、十堰市公安司法鉴定中心毒品检验鉴定报告、视听资料等证据证实，足以认定', 'input_ids': [101, 101, 4995, 3360, 102, 671, 510, 6158, 1440, 782, 4995, 3360, 6575, 1297, 510, 6817, 6783, 3681, 1501, 510, 6158, 1440, 782, 4670, 3424, 4386, 6575, 1297, 3681, 1501, 4638, 752, 2141, 9160, 2399, 126, 3299, 1159, 8024, 6158, 1440, 782, 4995, 3360, 711, 6579, 743, 4508, 1825, 5738, 688, 5542, 8020, 1102, 3681, 8021, 6822, 6121, 6575, 1297, 8024, 680, 3319, 3378, 8020, 1369, 3428, 1905, 4415, 8021, 671, 6629, 7730, 7724, 6756, 4277, 1384, 711, 6964, 8053, 9569, 9569, 9569, 115, 115, 1506, 2472, 6632, 7029, 6756, 5635, 1921, 7305, 2356, 1914, 2140, 7252, 809, 13215, 8129, 1039, 8020, 782, 3696, 2355, 8021, 4638, 817, 3419, 6579, 743, 749, 8226, 1046, 4508, 1825, 5738, 688, 5542, 8024, 1400, 1348, 7730, 6756, 2199, 6421, 8226, 1046, 4508, 1825, 5738, 688, 5542, 6817, 6783, 5635, 710, 3736, 1366, 2356, 1403, 800, 782, 6575, 1297, 511, 6158, 1440, 782, 4995, 3360, 1296, 4324, 2772, 6858, 6814, 6158, 1440, 782, 4670, 3424, 4386, 1350, 4374, 3378, 1403, 800, 782, 6575, 1297, 4508, 1825, 5738, 688, 5542, 125, 119, 125, 1046, 511, 6158, 1440, 782, 4670, 3424, 4386, 1296, 4324, 2772, 832, 1398, 800, 782, 6575, 1297, 4508, 1825, 5738, 688, 5542, 123, 119, 125, 1046, 511, 3428, 1355, 1400, 8024, 3389, 5815, 4638, 8124, 1259, 3681, 1501, 704, 3300, 8110, 1259, 1112, 7028, 1066, 123, 119, 11929, 1046, 4508, 1825, 5738, 688, 5542, 8024, 122, 1259, 4508, 1825, 5738, 688, 5542, 4275, 1177, 8020, 921, 4917, 100, 7937, 3362, 100, 8021, 1112, 7028, 121, 119, 8141, 8159, 1046, 511, 1072, 860, 752, 2141, 1963, 678, 8038, 8020, 671, 8021, 6158, 1440, 782, 4995, 3360, 6575, 1297, 4508, 1825, 5738, 688, 5542, 123, 119, 123, 1046, 510, 6158, 1440, 782, 4670, 3424, 4386, 6575, 1297, 4508, 1825, 5738, 688, 5542, 123, 119, 125, 1046, 4638, 752, 2141, 122, 119, 9160, 2399, 126, 3299, 8130, 3189, 8130, 3198, 6387, 8024, 6158, 1440, 782, 4670, 3424, 4386, 809, 8185, 1039, 4638, 817, 3419, 2199, 6158, 1440, 782, 4995, 3360, 1905, 121, 119, 123, 1046, 4508, 1825, 5738, 688, 5542, 1139, 1545, 5314, 1155, 3378, 511, 123, 119, 9160, 2399, 126, 3299, 8130, 3189, 8130, 3198, 6387, 8024, 6158, 1440, 782, 4670, 3424, 4386, 809, 8612, 1039, 4638, 817, 3419, 2199, 6158, 1440, 782, 4995, 3360, 1905, 121, 119, 8198, 1046, 4508, 1825, 5738, 688, 5542, 1139, 1545, 5314, 2235, 3378, 8024, 2400, 3119, 1357, 8269, 1039, 1962, 1905, 6589, 511, 124, 119, 9160, 2399, 126, 3299, 8133, 3189, 8113, 3198, 6387, 8024, 6158, 1440, 782, 4670, 3424, 4386, 1762, 710, 3736, 1366, 2356, 2528, 2157, 3765, 6662, 1366, 809, 8185, 1039, 4638, 817, 3419, 2199, 6158, 1440, 782, 4995, 3360, 1905, 121, 119, 8132, 1046, 4508, 1825, 5738, 688, 5542, 1139, 1545, 5314, 1155, 3378, 511, 125, 119, 9160, 2399, 126, 3299, 8132, 3189, 8126, 3198, 6387, 8024, 6158, 1440, 782, 4670, 3424, 4386, 1762, 710, 3736, 1366, 2356, 2528, 2157, 3765, 6662, 1366, 809, 8185, 1039, 4638, 817, 3419, 2199, 6158, 1440, 782, 4995, 3360, 1905, 121, 119, 123, 1046, 4508, 1825, 5738, 688, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
01/22/2024 00:21:27 - INFO - __main__ - Sample 456 of the training set: {'date': '2014-10-29', 'verdict': '江苏省南通市港闸区人民法院刑 事 判 决 书（2014）港刑初字第00118号公诉机关江苏省南通市港闸区人民检察院。被告人李某，无业。曾因卖淫，于2011年3月1日被江苏省南通市公安局崇川分局罚款人民币五百元。因涉嫌犯贩卖毒品罪、容留他人吸毒罪于2014年5月31日被刑事拘留，同年6月24日被逮捕。现羁押于南通市看守所。辩护人殷春花，江苏濠阳律师事务所律师。被告人曹某，无业。曾因犯抢劫罪，于2010年1月25日被江苏省海安县人民法院判处有期徒刑一年六个月，并处罚金人民币一千二百元，2011年1月12日刑满释放。因涉嫌犯贩卖毒品罪、容留他人吸毒罪于2014年5月29日被刑事拘留，同年6月24日被决定取保候审（从2014年6月25日开始起算），2014年10月29日经本院决定，由南通市公安局港闸分局执行逮捕。现羁押于南通市看守所。江苏省南通市港闸区人民检察院以通港检诉刑诉（2014）184号起诉书指控被告人李某、曹某犯贩卖毒品罪、容留他人吸毒罪，于2014年9月17日向本院提起公诉。本院于同日立案受理，依法适用普通程序组成合议庭于2014年9月30日、10月29日公开开庭审理了本案。江苏省南通市港闸区人民检察院指派检察员陈仕出庭支持公诉，被告人李某及其辩护人殷春花、被告人曹某到庭参加诉讼。现已审理终结。江苏省南通市港闸区人民检察院指控：2014年4月至5月间，被告人李某、曹某在南通市港闸区公园二村、新苑华庭等地多次向巫某、姚某贩卖甲基苯丙胺（俗称“冰毒”）共计0.9克；2014年5月28日，公安机关从被告人李某处查获甲基苯丙胺1.26克；2014年4月至5月间，被告人李某、曹某在其位于南通市港闸区公园二村18幢202室的暂住地多次容留谢某、姚某、黄某等人吸食甲基苯丙胺。指控的证据有书证、被告人供述、证人证言、鉴定意见等。公诉机关认为被告人李某、曹某贩卖毒品，应当以贩卖毒品罪追究其刑事责任；被告人李某、曹某多次容留他人吸食毒品，应当以容留他人吸毒罪追究其刑事责任。本案系共同犯罪，在共同犯罪中，被告人李某起主要作用，是主犯，应当按照其参与的全部犯罪处罚；在贩卖毒品的第三、四、五起犯罪事实中，被告人曹某起次要作用，系从犯，应当从轻或者减轻处罚。被告人曹某在有期徒刑刑罚执行完毕五年内再犯应当判处有期徒刑以上刑罚之罪，系累犯，应当从重处罚。被告人李某归案后如实供述自己的罪行，可以从轻处罚。被告人李某、曹某一人犯数罪，应数罪并罚。被告人李某对公诉机关指控的犯罪事实和罪名不持异议，并当庭自愿认罪。辩护人殷春花对公诉机关指控被告人李某犯贩卖毒品罪和容留他人吸毒罪不持异议，指出被告人李某贩卖毒品数量较小，社会危害性不大；其在案发后曾经向办案机关检举他人有吸毒以及贩卖毒品的行为，如查证属实可能构成立功；被告人李某归案后能如实供述自己的罪行，当庭自愿认罪，家庭经济困难，建议对其从轻处罚并适用缓刑。被告人曹某对公诉机关指控其构成贩卖毒品罪不持异议，辩称起诉书指控的第一起贩卖其起初不知道送给巫某的是甲基苯丙胺；对公诉机关指控其容留他人吸毒提出异议，辩称南通市港闸区公园二村18幢202室是被告人李某所租，其仅提供身份证办理了租赁手续，其并没有主动容留谢某等人吸食甲基苯丙胺。经审理查明：一、贩卖毒品犯罪事实被告人李某、曹某系男女朋友关系，2014年4月至5月间，二人在南通市港闸区公园二村、新苑华庭等地多次向巫某、姚某贩卖甲基苯丙胺共计0.8克。分述如下：1、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区新苑华庭小区贩卖给巫某，得款人民币200元。2、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区公园二村小区门前路段贩卖给巫某，得款人民币160元。3、2014年5月的一天，被告人李某乘被告人曹某的电瓶车至巫某位于南通市港闸区新苑华庭10幢的车库，贩卖给巫某甲基苯丙胺0.1克，得款人民币200元。4、2014年4月初的一天，被告人李某乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口，贩卖给姚某甲基苯丙胺0.5克，得款人民币500元。2014年5月28日晚，公安机关从被告人李某身上及其位于南通市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。二、容留他人吸毒犯罪事实2014年4月至5月间，被告人李某在其位于南通市港闸区公园二村18幢202室的暂住地多次容留谢某、姚某、黄某等人吸食甲基苯丙胺。分述如下：1、2014年4月的一天，被告人李某在其暂住地容留谢某吸食甲基苯丙胺。2、2014年4月底的一天，被告人李某在其暂住地容留姚某吸食甲基苯丙胺。3、2014年5月27日晚，被告人李某在其暂住地容留姚某、何某（身份不明）吸食甲基苯丙胺。4、2014年5月28日晚，李某在其暂住地容留黄某吸食甲基苯丙胺。以上事实，有下列由侦查机关依照法定程序收集并经庭审举证、质证的证据予以证实：1、被告人李某的供述及辨认笔某证明2014年4月，巫某两次打电话向其购买甲基苯丙胺，其将甲基苯丙胺准备好交给被告人曹某，由被告人曹某送给巫某，共计贩卖0.2克，得款人民币360元；2014年5月的一天，巫某打电话向其购买甲基苯丙胺，被告人曹某骑电瓶车带着其到巫某租住的车库，其将0.1克甲基苯丙胺交给巫某，得款人民币200元；2014年4月初的一天，姚某打电话向其购买甲基苯丙胺，其乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口贩卖给姚某甲基苯丙胺0.5克，得款人民币500元；南通市港闸区公园二村18幢202室为其与被告人曹某共同承租的房子，被告人曹某办的租房手续，钱是谁出的记不清楚了，可能两个人都出了一部分；2014年4月至5月间，其多次容留谢某、姚某、黄某等人吸食甲基苯丙胺，被告人曹某反对其带谢某、姚某等人到家中吸毒，二人曾经为此吵过架；公安机关从其身上和暂住地查获的甲基苯丙胺为其所有，被告人曹某不知道其在暂住地放有甲基苯丙胺。2、被告人曹某的供述及辨认笔某证明2014年4月，巫某两次打电话向被告人李某购买甲基苯丙胺，其将被告人李某准备好的甲基苯丙胺送给巫某，得款人民币360元；2014年5月的一天，巫某打电话给被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某至巫某租住的车库，被告人李某将甲基苯丙胺交给巫某；2014年4月初的一天，姚某打电话向被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某到南通市港闸区欧凯惠众超市门口，被告人李某将甲基苯丙胺交给姚某；南通市港闸区公园二村18幢202室是被告人李某与房东谈好价钱租的，签订合同的时候，被告人李某不在，其即用自己的身份证签订了合同，租房的钱是被告人李某出的；其知道2014年4月至5月间谢某、姚某、黄某等人在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，其认为这些吸毒的人头脑不太清楚，每次均躲在旁边，曾经因此和被告人李某吵过几次架。3、未到庭证人巫某的证言笔录及辨认笔某证明2014年4至5月间，其向被告人李某、曹某购买四次甲基苯丙胺，其中有两次是被告人曹某送的，还有两次是被告人李某、曹某骑电瓶车送至其车库的。4、未到庭证人姚某的证言笔录及辨认笔某证明2014年4月初的一天，被告人李某和曹某一起骑着电瓶车到唐闸公园旁边的欧凯惠众超市门口，被告人李某给其一包甲基苯丙胺，其给了被告人李某500元；2014年4月底的一天，其在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，不知道被告人曹某当时是否在家；2014年5月27日晚上，其和朋友何伟在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，被告人曹某在家，但被告人曹某没有吸。5、未到庭证人谢某的证言笔录及辨认笔某证明2014年4月份的一天其接到被告人李某的电话到南通市港闸区公园二村18幢202室与被告人李某共同吸食甲基苯丙胺，被告人曹某当时不在家。6、未到庭证人黄某的证言笔录及辨认笔某证明2014年5月28日晚上，其到南通市港闸区公园二村18幢202室被告人李某的家中吸食甲基苯丙胺，被告人李某和曹某先出去了，后其被公安机关当场查获。7、搜查笔录、扣押决定书、称重记录、鉴定意见等，证明公安机关从被告人李某身上及其位于本市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。8、刑事判决书、公安机关行政处罚决定书，证明被告人曹某有前科，被告人李某有劣迹。9、户籍资料，证明被告人李某、曹某具有完全刑事责任能力。10、发破案经过，证明被告人被告人李某、曹某被抓获归案，不具有自首情节。11、情况说明，证明被告人李某的检举揭发行为不能构成立功。本院认为，贩卖毒品罪，是指明知是毒品而故意实施贩卖行为。本罪的主体是一般主体，即达到刑事责任年龄且具有刑事责任能力的自然人均可成为本罪主体；主观方面表现为故意，且是直接故意，即明知是毒品而贩卖；侵犯的客体是国家对毒品的管理制度和人民的生命健康；客观方面表现为有偿转让毒品或者以贩卖为目的而非法收购毒品。被告人曹某在公安机关多次供述其知道起诉书指控的第一起贩卖，被告人李某叫其送给巫某的是甲基苯丙胺，且有被告人李某的供述印证，因此对其辩解意见本院不予采纳。起诉书指控的第四起贩卖，无被告人李某、曹某的供述，仅有证人巫某的证言，不能认定。综上，被告人李某、曹某向他人贩卖甲基苯丙胺合计0.8克，公安机关查获的甲基苯丙胺1.26克应当计入被告人李某贩卖的总数额。被告人李某、曹某明知是毒品而向他人贩卖，其中被告人李某贩卖毒品情节严重，应当以贩卖毒品罪追究两被告人的刑事责任。本案贩卖毒品部分系共同犯罪，在共同犯罪中，被告人李某起主要作用，是主犯，应当按照其参与的全部犯罪处罚；被告人曹某起次要作用，系从犯，应当从轻处罚。容留他人吸毒罪，是指为他人吸食、注射毒品提供场所和方便的行为。本罪侵犯的客体是复杂客体，即国家对毒品的管制制度和人们的身心健康；在客观方面表现为行为人给吸毒者提供吸毒的场所，既可以是行为人主动提供，也可以是在吸毒者的要求或主动前来时被动提供，既可以是有偿提供，也可以是无偿提供。本罪的主体为一般主体，即凡是达到刑事责任年龄具有刑事责任能力的人，均可构成本罪；本罪在主观方面表现为故意，即明知他人吸毒而为其提供场所。本案中，被告人李某为他人吸食毒品提供场所，应当以容留他人吸毒罪追究其刑事责任。谢某等吸毒人员与被告人李某相识，被告人曹某因被告人李某才认识谢某等人，其对谢某等人在二人住处吸食毒品持反对态度，且其没有主动为谢某等人吸毒提供便利条件，相反刻意回避，特别是起诉书指控的第一、二起，谢某、姚某甚至不知道当时被告人曹某是否在家，因此不宜认定被告人曹某构成容留他人吸毒罪。被告人李某、曹某归案后如实供述自己的罪行，可以从轻处罚；被告人曹某有前科，被告人李某有劣迹，酌情从重处罚。被告人李某一人犯两罪，应数罪并罚。据此，依照《中华人民共和国刑法》第三百四十七条第一、四、七款、第三百五十四条、第二十五条第一款、第二十六条第一、四款、第二十七条、第六十七条第三款、第六十九条、第六十四条的规定，判决如下：一、被告人李某犯贩卖毒品罪，判处有期徒刑三年，并处罚金人民币二万元；犯容留他人吸毒罪，判处有期徒刑六个月，并处罚金人民币六千元，决定执行有期徒刑三年三个月，并处罚金人民币二万六千元。（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2014年5月31日起至2017年8月30日止。罚金限于判决发生法律效力的第二日起一个月内缴纳）被告人曹某犯贩卖毒品罪，判处拘役四个月，并处罚金人民币四千元。（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，扣除先前羁押日27日，即自2014年10月29日起至2014年2月1日止。罚金限于判决发生法律效力的第二日起一个月内缴纳）二、追缴被告被告人李某、曹某非法所得人民币1060元，上缴国库。如不服本判决，可在接到判决书的第二日起十日内，通过本院或者直接向江苏省南通市中级人民法院提出上诉。书面上诉的，应当提交上诉状正本一份、副本两份。审\u3000判\u3000长\u3000王\u3000冯代理审判员\u3000黄\u3000玲人民陪审员\u3000沈继成二〇一四年十月二十九日书\u3000记\u3000员\u3000乔旭阳附：《中华人民共和国刑法》第三百四十七条第一、四、七款走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。第三百五十四条容留他人吸食、注射毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金。第二十五条第一款共同犯罪是指二人以上共同故意犯罪。第二十六条第一、四款组织、领导犯罪集团进行犯罪活动的或者在共同犯罪中起主要作用的，是主犯。对于第三款规定以外的主犯，应当按照其所参与的或者组织、指挥的全部犯罪处罚。第二十七条在共同犯罪中起次要或者辅助作用的，是从犯。对于从犯，应当从轻、减轻处罚或者免除处罚。第六十七条第三款犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。第六十九条判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。第六十四条犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔；对被害人的合法财产，应当及时返还；违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。', 'defendant_ls': "['李某', '曹某']", 'accusation': ["'走私、贩卖、运输、制造毒品',", "'容留他人吸毒'"], 'article_content': "[{'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第一款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第四款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百四十七条第七款', '内容': '【走私、贩卖、运输、制造毒品罪】走私、贩卖、运输、制造毒品，无论数量多少，都应当追究刑事责任，予以刑事处罚。走私、贩卖、运输、制造毒品，有下列情形之一的，处十五年有期徒刑、无期徒刑或者死刑，并处没收财产：（一）走私、贩卖、运输、制造鸦片一千克以上、海洛因或者甲基苯丙胺五十克以上或者其他毒品数量大的；（二）走私、贩卖、运输、制造毒品集团的首要分子；（三）武装掩护走私、贩卖、运输、制造毒品的；（四）以暴力抗拒检查、拘留、逮捕，情节严重的；（五）参与有组织的国际贩毒活动的。走私、贩卖、运输、制造鸦片二百克以上不满一千克、海洛因或者甲基苯丙胺十克以上不满五十克或者其他毒品数量较大的，处七年以上有期徒刑，并处罚金。走私、贩卖、运输、制造鸦片不满二百克、海洛因或者甲基苯丙胺不满十克或者其他少量毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金；情节严重的，处三年以上七年以下有期徒刑，并处罚金。单位犯第二款、第三款、第四款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。利用、教唆未成年人走私、贩卖、运输、制造毒品，或者向未成年人出售毒品的，从重处罚。对多次走私、贩卖、运输、制造毒品，未经处理的，毒品数量累计计算。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第三百五十四条', '内容': '【容留他人吸毒罪】容留他人吸食、注射毒品的，处三年以下有期徒刑、拘役或者管制，并处罚金。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十五条第一款', '内容': '【共同犯罪概念】共同犯罪是指二人以上共同故意犯罪。二人以上共同过失犯罪，不以共同犯罪论处；应当负刑事责任的，按照他们所犯的罪分别处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十六条第一款', '内容': '【主犯】组织、领导犯罪集团进行犯罪活动的或者在共同犯罪中起主要作用的，是主犯。三人以上为共同实施犯罪而组成的较为固定的犯罪组织，是犯罪集团。对组织、领导犯罪集团的首要分子，按照集团所犯的全部罪行处罚。对于第三款规定以外的主犯，应当按照其所参与的或者组织、指挥的全部犯罪处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十六条第四款', '内容': '【主犯】组织、领导犯罪集团进行犯罪活动的或者在共同犯罪中起主要作用的，是主犯。三人以上为共同实施犯罪而组成的较为固定的犯罪组织，是犯罪集团。对组织、领导犯罪集团的首要分子，按照集团所犯的全部罪行处罚。对于第三款规定以外的主犯，应当按照其所参与的或者组织、指挥的全部犯罪处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十七条第一款', '内容': '【从犯】在共同犯罪中起次要或者辅助作用的，是从犯。对于从犯，应当从轻、减轻处罚或者免除处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十七条第二款', '内容': '【从犯】在共同犯罪中起次要或者辅助作用的，是从犯。对于从犯，应当从轻、减轻处罚或者免除处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十七条第三款', '内容': '【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。被采取强制措施的犯罪嫌疑人、被告人和正在服刑的罪犯，如实供述司法机关还未掌握的本人其他罪行的，以自首论。犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第一款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第二款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十四条', '内容': '【犯罪物品的处理】犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔；对被害人的合法财产，应当及时返还；违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。'}]", 'province': '江苏省', 'fact': '一、贩卖毒品犯罪事实被告人李某、曹某系男女朋友关系，2014年4月至5月间，二人在南通市港闸区公园二村、新苑华庭等地多次向巫某、姚某贩卖甲基苯丙胺共计0.8克。分述如下：1、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区新苑华庭小区贩卖给巫某，得款人民币200元。2、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区公园二村小区门前路段贩卖给巫某，得款人民币160元。3、2014年5月的一天，被告人李某乘被告人曹某的电瓶车至巫某位于南通市港闸区新苑华庭10幢的车库，贩卖给巫某甲基苯丙胺0.1克，得款人民币200元。4、2014年4月初的一天，被告人李某乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口，贩卖给姚某甲基苯丙胺0.5克，得款人民币500元。2014年5月28日晚，公安机关从被告人李某身上及其位于南通市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。二、容留他人吸毒犯罪事实2014年4月至5月间，被告人李某在其位于南通市港闸区公园二村18幢202室的暂住地多次容留谢某、姚某、黄某等人吸食甲基苯丙胺。分述如下：1、2014年4月的一天，被告人李某在其暂住地容留谢某吸食甲基苯丙胺。2、2014年4月底的一天，被告人李某在其暂住地容留姚某吸食甲基苯丙胺。3、2014年5月27日晚，被告人李某在其暂住地容留姚某、何某（身份不明）吸食甲基苯丙胺。4、2014年5月28日晚，李某在其暂住地容留黄某吸食甲基苯丙胺。以上事实，有下列由侦查机关依照法定程序收集并经庭审举证、质证的证据予以证实：1、被告人李某的供述及辨认笔某证明2014年4月，巫某两次打电话向其购买甲基苯丙胺，其将甲基苯丙胺准备好交给被告人曹某，由被告人曹某送给巫某，共计贩卖0.2克，得款人民币360元；2014年5月的一天，巫某打电话向其购买甲基苯丙胺，被告人曹某骑电瓶车带着其到巫某租住的车库，其将0.1克甲基苯丙胺交给巫某，得款人民币200元；2014年4月初的一天，姚某打电话向其购买甲基苯丙胺，其乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口贩卖给姚某甲基苯丙胺0.5克，得款人民币500元；南通市港闸区公园二村18幢202室为其与被告人曹某共同承租的房子，被告人曹某办的租房手续，钱是谁出的记不清楚了，可能两个人都出了一部分；2014年4月至5月间，其多次容留谢某、姚某、黄某等人吸食甲基苯丙胺，被告人曹某反对其带谢某、姚某等人到家中吸毒，二人曾经为此吵过架；公安机关从其身上和暂住地查获的甲基苯丙胺为其所有，被告人曹某不知道其在暂住地放有甲基苯丙胺。2、被告人曹某的供述及辨认笔某证明2014年4月，巫某两次打电话向被告人李某购买甲基苯丙胺，其将被告人李某准备好的甲基苯丙胺送给巫某，得款人民币360元；2014年5月的一天，巫某打电话给被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某至巫某租住的车库，被告人李某将甲基苯丙胺交给巫某；2014年4月初的一天，姚某打电话向被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某到南通市港闸区欧凯惠众超市门口，被告人李某将甲基苯丙胺交给姚某；南通市港闸区公园二村18幢202室是被告人李某与房东谈好价钱租的，签订合同的时候，被告人李某不在，其即用自己的身份证签订了合同，租房的钱是被告人李某出的；其知道2014年4月至5月间谢某、姚某、黄某等人在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，其认为这些吸毒的人头脑不太清楚，每次均躲在旁边，曾经因此和被告人李某吵过几次架。3、未到庭证人巫某的证言笔录及辨认笔某证明2014年4至5月间，其向被告人李某、曹某购买四次甲基苯丙胺，其中有两次是被告人曹某送的，还有两次是被告人李某、曹某骑电瓶车送至其车库的。4、未到庭证人姚某的证言笔录及辨认笔某证明2014年4月初的一天，被告人李某和曹某一起骑着电瓶车到唐闸公园旁边的欧凯惠众超市门口，被告人李某给其一包甲基苯丙胺，其给了被告人李某500元；2014年4月底的一天，其在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，不知道被告人曹某当时是否在家；2014年5月27日晚上，其和朋友何伟在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，被告人曹某在家，但被告人曹某没有吸。5、未到庭证人谢某的证言笔录及辨认笔某证明2014年4月份的一天其接到被告人李某的电话到南通市港闸区公园二村18幢202室与被告人李某共同吸食甲基苯丙胺，被告人曹某当时不在家。6、未到庭证人黄某的证言笔录及辨认笔某证明2014年5月28日晚上，其到南通市港闸区公园二村18幢202室被告人李某的家中吸食甲基苯丙胺，被告人李某和曹某先出去了，后其被公安机关当场查获。7、搜查笔录、扣押决定书、称重记录、鉴定意见等，证明公安机关从被告人李某身上及其位于本市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。8、刑事判决书、公安机关行政处罚决定书，证明被告人曹某有前科，被告人李某有劣迹。9、户籍资料，证明被告人李某、曹某具有完全刑事责任能力。10、发破案经过，证明被告人被告人李某、曹某被抓获归案，不具有自首情节。11、情况说明，证明被告人李某的检举揭发行为不能构成立功', 'label': 1247, 'relevant_article': ['354.0,', '347.0'], 'defendant_judgement': "[{'走私、贩卖、运输、制造毒品': {'有期徒刑': '三年'}}, {'容留他人吸毒': {'有期徒刑': '六个月'}}, {'有期徒刑': '三年三个月'}]", 'defendant': '[CLS]李某[SEP]一、贩卖毒品犯罪事实被告人李某、曹某系男女朋友关系，2014年4月至5月间，二人在南通市港闸区公园二村、新苑华庭等地多次向巫某、姚某贩卖甲基苯丙胺共计0.8克。分述如下：1、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区新苑华庭小区贩卖给巫某，得款人民币200元。2、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区公园二村小区门前路段贩卖给巫某，得款人民币160元。3、2014年5月的一天，被告人李某乘被告人曹某的电瓶车至巫某位于南通市港闸区新苑华庭10幢的车库，贩卖给巫某甲基苯丙胺0.1克，得款人民币200元。4、2014年4月初的一天，被告人李某乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口，贩卖给姚某甲基苯丙胺0.5克，得款人民币500元。2014年5月28日晚，公安机关从被告人李某身上及其位于南通市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。二、容留他人吸毒犯罪事实2014年4月至5月间，被告人李某在其位于南通市港闸区公园二村18幢202室的暂住地多次容留谢某、姚某、黄某等人吸食甲基苯丙胺。分述如下：1、2014年4月的一天，被告人李某在其暂住地容留谢某吸食甲基苯丙胺。2、2014年4月底的一天，被告人李某在其暂住地容留姚某吸食甲基苯丙胺。3、2014年5月27日晚，被告人李某在其暂住地容留姚某、何某（身份不明）吸食甲基苯丙胺。4、2014年5月28日晚，李某在其暂住地容留黄某吸食甲基苯丙胺。以上事实，有下列由侦查机关依照法定程序收集并经庭审举证、质证的证据予以证实：1、被告人李某的供述及辨认笔某证明2014年4月，巫某两次打电话向其购买甲基苯丙胺，其将甲基苯丙胺准备好交给被告人曹某，由被告人曹某送给巫某，共计贩卖0.2克，得款人民币360元；2014年5月的一天，巫某打电话向其购买甲基苯丙胺，被告人曹某骑电瓶车带着其到巫某租住的车库，其将0.1克甲基苯丙胺交给巫某，得款人民币200元；2014年4月初的一天，姚某打电话向其购买甲基苯丙胺，其乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口贩卖给姚某甲基苯丙胺0.5克，得款人民币500元；南通市港闸区公园二村18幢202室为其与被告人曹某共同承租的房子，被告人曹某办的租房手续，钱是谁出的记不清楚了，可能两个人都出了一部分；2014年4月至5月间，其多次容留谢某、姚某、黄某等人吸食甲基苯丙胺，被告人曹某反对其带谢某、姚某等人到家中吸毒，二人曾经为此吵过架；公安机关从其身上和暂住地查获的甲基苯丙胺为其所有，被告人曹某不知道其在暂住地放有甲基苯丙胺。2、被告人曹某的供述及辨认笔某证明2014年4月，巫某两次打电话向被告人李某购买甲基苯丙胺，其将被告人李某准备好的甲基苯丙胺送给巫某，得款人民币360元；2014年5月的一天，巫某打电话给被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某至巫某租住的车库，被告人李某将甲基苯丙胺交给巫某；2014年4月初的一天，姚某打电话向被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某到南通市港闸区欧凯惠众超市门口，被告人李某将甲基苯丙胺交给姚某；南通市港闸区公园二村18幢202室是被告人李某与房东谈好价钱租的，签订合同的时候，被告人李某不在，其即用自己的身份证签订了合同，租房的钱是被告人李某出的；其知道2014年4月至5月间谢某、姚某、黄某等人在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，其认为这些吸毒的人头脑不太清楚，每次均躲在旁边，曾经因此和被告人李某吵过几次架。3、未到庭证人巫某的证言笔录及辨认笔某证明2014年4至5月间，其向被告人李某、曹某购买四次甲基苯丙胺，其中有两次是被告人曹某送的，还有两次是被告人李某、曹某骑电瓶车送至其车库的。4、未到庭证人姚某的证言笔录及辨认笔某证明2014年4月初的一天，被告人李某和曹某一起骑着电瓶车到唐闸公园旁边的欧凯惠众超市门口，被告人李某给其一包甲基苯丙胺，其给了被告人李某500元；2014年4月底的一天，其在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，不知道被告人曹某当时是否在家；2014年5月27日晚上，其和朋友何伟在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，被告人曹某在家，但被告人曹某没有吸。5、未到庭证人谢某的证言笔录及辨认笔某证明2014年4月份的一天其接到被告人李某的电话到南通市港闸区公园二村18幢202室与被告人李某共同吸食甲基苯丙胺，被告人曹某当时不在家。6、未到庭证人黄某的证言笔录及辨认笔某证明2014年5月28日晚上，其到南通市港闸区公园二村18幢202室被告人李某的家中吸食甲基苯丙胺，被告人李某和曹某先出去了，后其被公安机关当场查获。7、搜查笔录、扣押决定书、称重记录、鉴定意见等，证明公安机关从被告人李某身上及其位于本市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。8、刑事判决书、公安机关行政处罚决定书，证明被告人曹某有前科，被告人李某有劣迹。9、户籍资料，证明被告人李某、曹某具有完全刑事责任能力。10、发破案经过，证明被告人被告人李某、曹某被抓获归案，不具有自首情节。11、情况说明，证明被告人李某的检举揭发行为不能构成立功', 'imprisonment': 39.0, 'sentence': '[CLS]李某[SEP]一、贩卖毒品犯罪事实被告人李某、曹某系男女朋友关系，2014年4月至5月间，二人在南通市港闸区公园二村、新苑华庭等地多次向巫某、姚某贩卖甲基苯丙胺共计0.8克。分述如下：1、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区新苑华庭小区贩卖给巫某，得款人民币200元。2、2014年4月的一天，被告人李某将0.1克甲基苯丙胺交给被告人曹某，由曹某在南通市港闸区公园二村小区门前路段贩卖给巫某，得款人民币160元。3、2014年5月的一天，被告人李某乘被告人曹某的电瓶车至巫某位于南通市港闸区新苑华庭10幢的车库，贩卖给巫某甲基苯丙胺0.1克，得款人民币200元。4、2014年4月初的一天，被告人李某乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口，贩卖给姚某甲基苯丙胺0.5克，得款人民币500元。2014年5月28日晚，公安机关从被告人李某身上及其位于南通市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。二、容留他人吸毒犯罪事实2014年4月至5月间，被告人李某在其位于南通市港闸区公园二村18幢202室的暂住地多次容留谢某、姚某、黄某等人吸食甲基苯丙胺。分述如下：1、2014年4月的一天，被告人李某在其暂住地容留谢某吸食甲基苯丙胺。2、2014年4月底的一天，被告人李某在其暂住地容留姚某吸食甲基苯丙胺。3、2014年5月27日晚，被告人李某在其暂住地容留姚某、何某（身份不明）吸食甲基苯丙胺。4、2014年5月28日晚，李某在其暂住地容留黄某吸食甲基苯丙胺。以上事实，有下列由侦查机关依照法定程序收集并经庭审举证、质证的证据予以证实：1、被告人李某的供述及辨认笔某证明2014年4月，巫某两次打电话向其购买甲基苯丙胺，其将甲基苯丙胺准备好交给被告人曹某，由被告人曹某送给巫某，共计贩卖0.2克，得款人民币360元；2014年5月的一天，巫某打电话向其购买甲基苯丙胺，被告人曹某骑电瓶车带着其到巫某租住的车库，其将0.1克甲基苯丙胺交给巫某，得款人民币200元；2014年4月初的一天，姚某打电话向其购买甲基苯丙胺，其乘被告人曹某的电瓶车至南通市港闸区欧凯惠众超市门口贩卖给姚某甲基苯丙胺0.5克，得款人民币500元；南通市港闸区公园二村18幢202室为其与被告人曹某共同承租的房子，被告人曹某办的租房手续，钱是谁出的记不清楚了，可能两个人都出了一部分；2014年4月至5月间，其多次容留谢某、姚某、黄某等人吸食甲基苯丙胺，被告人曹某反对其带谢某、姚某等人到家中吸毒，二人曾经为此吵过架；公安机关从其身上和暂住地查获的甲基苯丙胺为其所有，被告人曹某不知道其在暂住地放有甲基苯丙胺。2、被告人曹某的供述及辨认笔某证明2014年4月，巫某两次打电话向被告人李某购买甲基苯丙胺，其将被告人李某准备好的甲基苯丙胺送给巫某，得款人民币360元；2014年5月的一天，巫某打电话给被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某至巫某租住的车库，被告人李某将甲基苯丙胺交给巫某；2014年4月初的一天，姚某打电话向被告人李某购买甲基苯丙胺，其骑电瓶车与被告人李某到南通市港闸区欧凯惠众超市门口，被告人李某将甲基苯丙胺交给姚某；南通市港闸区公园二村18幢202室是被告人李某与房东谈好价钱租的，签订合同的时候，被告人李某不在，其即用自己的身份证签订了合同，租房的钱是被告人李某出的；其知道2014年4月至5月间谢某、姚某、黄某等人在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，其认为这些吸毒的人头脑不太清楚，每次均躲在旁边，曾经因此和被告人李某吵过几次架。3、未到庭证人巫某的证言笔录及辨认笔某证明2014年4至5月间，其向被告人李某、曹某购买四次甲基苯丙胺，其中有两次是被告人曹某送的，还有两次是被告人李某、曹某骑电瓶车送至其车库的。4、未到庭证人姚某的证言笔录及辨认笔某证明2014年4月初的一天，被告人李某和曹某一起骑着电瓶车到唐闸公园旁边的欧凯惠众超市门口，被告人李某给其一包甲基苯丙胺，其给了被告人李某500元；2014年4月底的一天，其在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，不知道被告人曹某当时是否在家；2014年5月27日晚上，其和朋友何伟在南通市港闸区公园二村18幢202室吸食甲基苯丙胺，被告人曹某在家，但被告人曹某没有吸。5、未到庭证人谢某的证言笔录及辨认笔某证明2014年4月份的一天其接到被告人李某的电话到南通市港闸区公园二村18幢202室与被告人李某共同吸食甲基苯丙胺，被告人曹某当时不在家。6、未到庭证人黄某的证言笔录及辨认笔某证明2014年5月28日晚上，其到南通市港闸区公园二村18幢202室被告人李某的家中吸食甲基苯丙胺，被告人李某和曹某先出去了，后其被公安机关当场查获。7、搜查笔录、扣押决定书、称重记录、鉴定意见等，证明公安机关从被告人李某身上及其位于本市港闸区公园二村18幢202室的暂住地查获甲基苯丙胺1.26克。8、刑事判决书、公安机关行政处罚决定书，证明被告人曹某有前科，被告人李某有劣迹。9、户籍资料，证明被告人李某、曹某具有完全刑事责任能力。10、发破案经过，证明被告人被告人李某、曹某被抓获归案，不具有自首情节。11、情况说明，证明被告人李某的检举揭发行为不能构成立功', 'input_ids': [101, 101, 3330, 3378, 102, 671, 510, 6575, 1297, 3681, 1501, 4306, 5389, 752, 2141, 6158, 1440, 782, 3330, 3378, 510, 3293, 3378, 5143, 4511, 1957, 3301, 1351, 1068, 5143, 8024, 8127, 2399, 125, 3299, 5635, 126, 3299, 7313, 8024, 753, 782, 1762, 1298, 6858, 2356, 3949, 7316, 1277, 1062, 1736, 753, 3333, 510, 3173, 5723, 1290, 2431, 5023, 1765, 1914, 3613, 1403, 2344, 3378, 510, 2001, 3378, 6575, 1297, 4508, 1825, 5738, 688, 5542, 1066, 6369, 121, 119, 129, 1046, 511, 1146, 6835, 1963, 678, 8038, 122, 510, 8127, 2399, 125, 3299, 4638, 671, 1921, 8024, 6158, 1440, 782, 3330, 3378, 2199, 121, 119, 122, 1046, 4508, 1825, 5738, 688, 5542, 769, 5314, 6158, 1440, 782, 3293, 3378, 8024, 4507, 3293, 3378, 1762, 1298, 6858, 2356, 3949, 7316, 1277, 3173, 5723, 1290, 2431, 2207, 1277, 6575, 1297, 5314, 2344, 3378, 8024, 2533, 3621, 782, 3696, 2355, 8185, 1039, 511, 123, 510, 8127, 2399, 125, 3299, 4638, 671, 1921, 8024, 6158, 1440, 782, 3330, 3378, 2199, 121, 119, 122, 1046, 4508, 1825, 5738, 688, 5542, 769, 5314, 6158, 1440, 782, 3293, 3378, 8024, 4507, 3293, 3378, 1762, 1298, 6858, 2356, 3949, 7316, 1277, 1062, 1736, 753, 3333, 2207, 1277, 7305, 1184, 6662, 3667, 6575, 1297, 5314, 2344, 3378, 8024, 2533, 3621, 782, 3696, 2355, 8522, 1039, 511, 124, 510, 8127, 2399, 126, 3299, 4638, 671, 1921, 8024, 6158, 1440, 782, 3330, 3378, 733, 6158, 1440, 782, 3293, 3378, 4638, 4510, 4486, 6756, 5635, 2344, 3378, 855, 754, 1298, 6858, 2356, 3949, 7316, 1277, 3173, 5723, 1290, 2431, 8108, 2394, 4638, 6756, 2417, 8024, 6575, 1297, 5314, 2344, 3378, 4508, 1825, 5738, 688, 5542, 121, 119, 122, 1046, 8024, 2533, 3621, 782, 3696, 2355, 8185, 1039, 511, 125, 510, 8127, 2399, 125, 3299, 1159, 4638, 671, 1921, 8024, 6158, 1440, 782, 3330, 3378, 733, 6158, 1440, 782, 3293, 3378, 4638, 4510, 4486, 6756, 5635, 1298, 6858, 2356, 3949, 7316, 1277, 3616, 1132, 2669, 830, 6631, 2356, 7305, 1366, 8024, 6575, 1297, 5314, 2001, 3378, 4508, 1825, 5738, 688, 5542, 121, 119, 126, 1046, 8024, 2533, 3621, 782, 3696, 2355, 8195, 1039, 511, 8127, 2399, 126, 3299, 8143, 3189, 3241, 8024, 1062, 2128, 3322, 1068, 794, 6158, 1440, 782, 3330, 3378, 6716, 677, 1350, 1071, 855, 754, 1298, 6858, 2356, 3949, 7316, 1277, 1062, 1736, 753, 3333, 8123, 2394, 9707, 2147, 4638, 3257, 857, 1765, 3389, 5815, 4508, 1825, 5738, 688, 5542, 122, 119, 8153, 1046, 511, 753, 510, 2159, 4522, 800, 782, 1429, 3681, 4306, 5389, 752, 2141, 8127, 2399, 125, 3299, 5635, 126, 3299, 7313, 8024, 6158, 1440, 782, 3330, 3378, 1762, 1071, 855, 754, 1298, 6858, 2356, 3949, 7316, 1277, 1062, 1736, 753, 3333, 8123, 2394, 9707, 2147, 4638, 3257, 857, 1765, 1914, 3613, 2159, 4522, 6468, 3378, 510, 2001, 3378, 510, 7942, 3378, 5023, 782, 1429, 7608, 4508, 1825, 5738, 688, 5542, 511, 1146, 6835, 1963, 678, 8038, 122, 510, 8127, 2399, 125, 3299, 4638, 671, 1921, 8024, 6158, 1440, 782, 3330, 3378, 1762, 1071, 3257, 857, 1765, 2159, 4522, 6468, 3378, 1429, 7608, 4508, 1825, 5738, 688, 5542, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
01/22/2024 00:21:27 - INFO - __main__ - Sample 102 of the training set: {'date': '2020-12-11', 'verdict': '云南省建水县人民法院刑 事 判 决 书（2020）云2524刑初116号公诉机关云南省建水县人民检察院。被告人张亮，男，1998年4月6日生，汉族，小学文化，云南省建水县人，农民，住建水县。2019年3月19日因涉嫌犯抢劫罪被建水县公安局刑事拘留，4月25日被逮捕。2019年11月12日被本院以抢劫罪判处有期徒刑十年零六个月，并处罚金人民币1000元。因涉嫌犯聚众斗殴罪、非法持有枪支罪于2020年1月20日被建水县公安局从云南省建水县监狱解回看守所侦查。现羁押于建水县看守所。被告人张敏，男，1996年9月4日生，汉族，初中文化，云南省建水县人，农民，住建水县。2018年8月14日因涉嫌犯故意伤害罪被建水县公安局刑事拘留，2018年9月17日被逮捕。2019年3月7日被本院以故意伤害罪判处有期徒刑三年。因涉嫌犯聚众斗殴罪、非法持有枪支罪于2019年10月31日被建水县公安局从云南省建水县监狱解回看守所侦查。现羁押于建水县看守所。建水县人民检察院以建检一部刑诉（2020）56号起诉书指控被告人张敏、张亮犯聚众斗殴罪、非法持有枪支罪，于2020年3月12日向本院提起公诉。因全国新型冠状病毒疫情防控期间无法开展相关工作，报经红河州中级人民法院批准，延长审理期限3个月。本院依法组成合议庭，公开开庭审理了本案。建水县人民检察院指派检察员李艳芳等二人出庭支持公诉。被告人张敏、张亮到庭参加了诉讼。现已审理终结。建水县人民检察院指控：被告人张敏、张亮持械聚众斗殴，致二人轻伤；非法持有以火药为动力的枪支一支，其行为触犯了《中华人民共和国刑法》第二百九十二条、第一百二十八条第一款、第二十五条第一款、第六十九条、第七十条之规定，犯罪事实清楚，证据确实、充分，应当以聚众斗殴罪、非法持有枪支罪追究其刑事责任。被告人张敏、张亮到案后，如实供述其主要犯罪事实，根据《中华人民共和国刑法》第六十七条第三款之规定，系坦白，可以从轻处罚。建议法庭以聚众斗殴罪判处被告人张敏有期徒刑三年至四年，以非法持有枪支罪判处被告人张敏有期徒刑六个月至一年零六个月，与前罪故意伤害罪数罪并罚；建议法庭以聚众斗殴罪、非法持有枪支罪判处被告人张亮有期徒刑五年至七年，与前罪抢劫罪数罪并罚。被告人张亮对公诉机关指控的事实、罪名及量刑建议无异议，且签字具结；被告人张敏对公诉机关指控的事实及罪名无异议。经审理查明：一、聚众斗殴的事实。1.2018年5月27日晚，被告人张亮、张敏与毛某、潘某、唐某（三人另案处理）、谭某、卢某（二人已判处）等人在建水县临安镇“蜕变ＫＴＶ”内唱歌。同时余某、白某、李某1、程某、王某、邬建严（均已判处）等人在“顶尖ＫＴＶ”唱歌，张亮与余某联系后，张亮、潘某到“顶尖ＫＴＶ”找余某玩，后三人又返回“蜕变ＫＴＶ”。余某在“蜕变ＫＴＶ”门口上楼梯的过程中与被害人黄某1相遇，黄某1被余某踩到脚，双方发生争吵，后黄某1打电话给张某1等人到“蜕变ＫＴＶ”与其汇合。被告人张亮见状便邀约毛某、唐某、谭某、卢某从包房出来与其在“蜕变ＫＴＶ”门口楼梯底汇合。同时余某打电话邀约白某、李某1等人到“蜕变ＫＴＶ”与其汇合。2018年5月27日23时48分许，张某1等人到达“蜕变ＫＴＶ”门口与黄某1汇合，张亮在看到张某1等人来到后，意识到双方可能会发生打架，又打电话邀约张敏从包房出来参与打架。23时51分许，白某、李某1、程某、王某、邬建严等6人来到“蜕变ＫＴＶ”门口，6人下车后随即与黄某1、张某1等人发生争吵，因李某1认识黄某1一方的人，遂从中劝阻，但劝阻未成。在争吵过程中张某1先动手打了白某的头，随后双方发生互殴，白某、王某、李某1等人殴打黄某1、张某1，张亮、张敏、谭某、卢某见状便参与斗殴，白某、张敏在打架过程中使用跳刀伤害黄某1、张某1，因有警车经过双方停止斗殴。经鉴定，黄某1的伤情为轻伤一级，张某1的伤情为轻伤二级。2.2017年10月31日，建水县城“鑫都汇ＫＴＶ”开业，朱某、彭某（均已判处）为了争夺该ＫＴＶ看场的权利，朱某一方中的余某打电话邀约被告人张亮，被告人张亮又邀约谭某、毛某、潘某等人到“达鑫都汇ＫＴＶ”。张亮、谭某等人在现场领取钢管、三尖叉等工具准备斗殴。彭某一方的人员驾驶车辆赶到“鑫都汇ＫＴＶ”门口后，张亮等人手持钢管冲出去，对彭某一方及车辆进行追逐、打砸，后张亮等人到“鑫都汇ＫＴＶ”内躲避一段时间后逃离现场。经建水县发展和改革局价格认证中心认定，现场被砸车辆的损失共计人民币20459元。二、非法持有枪支的事实2016年至2017年的一天，被告人张亮从李永康（已判处）处获得一支疑似枪支，并将该疑似枪支藏匿于被告人张敏家中，被告人张敏将该疑似枪支放在自家卧室的床下面。2018年被告人张敏因其他犯罪行为被抓捕后，该疑似枪支又被张亮转移到谭某家中。2019年11月4日，建水县公安局民警在谭某的带领下到建水县西庄镇马厂村227号谭某家中查获该疑似枪支。经红河州公安司法鉴定中心鉴定，该疑似枪支系以火药为动力发射弹丸的民用非制式枪支。上述事实，被告人张亮、张敏在开庭审理过程中亦无异议，且有庭审记录、公诉机关提供并经法庭当庭举证、质证和认证的户口证明，受案登记表、立案决定书，（2019）云2524刑初9号刑事判决书、（2019）云2524刑初305号刑事判决书、将服刑人员提至看守所羁押函、云南省监狱管理局关于罪犯解回侦查的告知函，行政处罚决定书、行政处罚告知笔录，伤情证明、手术记录、红公（司）鉴（痕迹）字［2019］219号鉴定书、（建）公（司）鉴（法损）字［2019］65号鉴定书、（建）公（司）鉴（法损）字［2019］66号鉴定书、建价认字（2017）352号价格认定结论书、建价认字（2017）353号价格认定结论书、建价认字（2018）89号价格认定结论书、价格认定标的物明细表、鉴定意见通知书，被害人张某1、黄某1的陈述，证人余某、谭某、卢某、毛某、程某、白某、付某、王某、李某1、潘某、唐某、朱某、许某、李某2、李某3、吴某、彭某、孙某、胡某1、胡某2、胡某3、张某2、李某4、黄某2、张某3的证言，辨认笔录及照片、辨认监控视频截图笔录及照片、辨认枪支笔录及照片、现场辨认笔录及照片，现场勘验笔录及照片、现场图，光碟1张，被告人张敏、张亮的供述与辩解等证据证实，足以认定。本院认为，被告人张亮、张敏持械聚众斗殴，致二人轻伤；被告人张亮、张敏违反枪支管理规定，非法持有以火药为动力发射弹丸的民用非制式枪支1支，其行为已构成聚众斗殴罪、非法持有枪支罪。公诉机关指控被告人张亮、张敏的事实及罪名成立。被告人张亮、张敏在判决宣告后，刑罚执行完毕前发现判决宣告前还有其他罪行没有判决，依法应当数罪并罚。被告人张亮、张敏在聚众斗殴的共同犯罪中起次要作用，系从犯，依法应当从轻处罚。被告人张亮、张敏到案后如实供述自己的犯罪事实，系坦白，依法可以从轻处罚。被告人张亮自愿认罪认罚，依法可以从宽处罚。公诉机关的量刑建议合法，本院予以采纳。据此，根据本案的事实、性质、情节及对社会的危害程度，依照《中华人民共和国刑法》第二百九十二条第一款第（四）项、第一百二十八条第一款、第二十五条第一款、第二十七条、第六十七条第三款、第六十九条、第七十条、第五十二条、第五十三条第一款、第六十四条之规定，判决如下：一、被告人张亮犯聚众斗殴罪，判处有期徒刑四年零六个月；犯非法持有枪支罪，判处有期徒刑一年。前罪犯抢劫罪，判处有期徒刑十年零六个月，并处罚金人民币1000元，总和刑期十六年，并处罚金人民币1000元，决定执行有期徒刑十五年，并处罚金人民币1000元（刑期从判决执行之日起计算。判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2019年3月19日起至2034年3月18日止；罚金限期于判决生效后十日内缴纳）。二、被告人张敏犯聚众斗殴罪，判处有期徒刑四年；犯非法持有枪支罪，判处有期徒刑一年，前罪犯故意伤害罪，判处有期徒刑三年，总和刑期八年，决定执行有期徒刑七年（刑期从判决执行之日起计算，判决执行以前先行羁押的，羁押一日折抵刑期一日，即自2018年8月14日起至2025年8月13日止）。三、随案移送的光碟1张，作为证据随案保存。如不服本判决，可在接到判决书的第二日起十日内，通过本院或直接向云南省红河哈尼族彝族自治州中级人民法院提出上诉。书面上诉的，应交上诉状正本一份，副本二份。审\u3000判\u3000长\u3000\u3000王贵鑫人民陪审员\u3000\u3000潘兴国人民陪审员\u3000\u3000孔繁贵二〇二〇年十二月十一日书\u3000记\u3000员\u3000\u3000蒋\u3000骁', 'defendant_ls': "['张亮', '张敏']", 'accusation': ["'聚众斗殴',", "'非法持有、私藏枪支、弹药'"], 'article_content': "[{'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二百九十二条第一款第四项', '内容': '【聚众斗殴罪】聚众斗殴的，对首要分子和其他积极参加的，处三年以下有期徒刑、拘役或者管制；有下列情形之一的，对首要分子和其他积极参加的，处三年以上十年以下有期徒刑：（一）多次聚众斗殴的；（二）聚众斗殴人数多，规模大，社会影响恶劣的；（三）在公共场所或者交通要道聚众斗殴，造成社会秩序严重混乱的；（四）持械聚众斗殴的。【故意伤害罪】【故意杀人罪】聚众斗殴，致人重伤、死亡的，依照本法第二百三十四条、第二百三十二条的规定定罪处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第一百二十八条第一款', '内容': '【非法持有、私藏枪支、弹药罪】违反枪支管理规定，非法持有、私藏枪支、弹药的，处三年以下有期徒刑、拘役或者管制；情节严重的，处三年以上七年以下有期徒刑。【非法出租、出借枪支罪】依法配备公务用枪的人员，非法出租、出借枪支的，依照前款的规定处罚。【非法出租、出借枪支罪】依法配置枪支的人员，非法出租、出借枪支，造成严重后果的，依照第一款的规定处罚。单位犯第二款、第三款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照第一款的规定处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十五条第一款', '内容': '【共同犯罪的概念】共同犯罪是指二人以上共同故意犯罪。二人以上共同过失犯罪，不以共同犯罪论处；应当负刑事责任的，按照他们所犯的罪分别处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十七条第一款', '内容': '【从犯】在共同犯罪中起次要或者辅助作用的，是从犯。对于从犯，应当从轻、减轻处罚或者免除处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第二十七条第二款', '内容': '【从犯】在共同犯罪中起次要或者辅助作用的，是从犯。对于从犯，应当从轻、减轻处罚或者免除处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十七条第三款', '内容': '【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。被采取强制措施的犯罪嫌疑人、被告人和正在服刑的罪犯，如实供述司法机关还未掌握的本人其他罪行的，以自首论。犯罪嫌疑人虽不具有前两款规定的自首情节，但是如实供述自己罪行的，可以从轻处罚；因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第一款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第二款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十九条第三款', '内容': '【数罪并罚的一般原则】判决宣告以前一人犯数罪的，除判处死刑和无期徒刑的以外，应当在总和刑期以下、数刑中最高刑期以上，酌情决定执行的刑期，但是管制最高不能超过三年，拘役最高不能超过一年，有期徒刑总和刑期不满三十五年的，最高不能超过二十年，总和刑期在三十五年以上的，最高不能超过二十五年。数罪中有判处有期徒刑和拘役的，执行有期徒刑。数罪中有判处有期徒刑和管制，或者拘役和管制的，有期徒刑、拘役执行完毕后，管制仍须执行。数罪中有判处附加刑的，附加刑仍须执行，其中附加刑种类相同的，合并执行，种类不同的，分别执行。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第七十条', '内容': '【判决宣告后发现漏罪的并罚】判决宣告以后，刑罚执行完毕以前，发现被判刑的犯罪分子在判决宣告以前还有其他罪没有判决的，应当对新发现的罪作出判决，把前后两个判决所判处的刑罚，依照本法第六十九条的规定，决定执行的刑罚。已经执行的刑期，应当计算在新判决决定的刑期以内。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十二条', '内容': '【罚金数额的裁量】判处罚金，应当根据犯罪情节决定罚金数额。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第五十三条第一款', '内容': '【罚金的缴纳】罚金在判决指定的期限内一次或者分期缴纳。期满不缴纳的，强制缴纳。对于不能全部缴纳罚金的，人民法院在任何时候发现被执行人有可以执行的财产，应当随时追缴。由于遭遇不能抗拒的灾祸等原因缴纳确实有困难的，经人民法院裁定，可以延期缴纳、酌情减少或者免除。'}, {'法律名称': '《中华人民共和国刑法（1997年）》', '法律条文': '第六十四条', '内容': '【犯罪物品的处理】犯罪分子违法所得的一切财物，应当予以追缴或者责令退赔；对被害人的合法财产，应当及时返还；违禁品和供犯罪所用的本人财物，应当予以没收。没收的财物和罚金，一律上缴国库，不得挪用和自行处理。'}]", 'province': '云南省', 'fact': '一、聚众斗殴的事实。1.2018年5月27日晚，被告人张亮、张敏与毛某、潘某、唐某（三人另案处理）、谭某、卢某（二人已判处）等人在建水县临安镇“蜕变ＫＴＶ”内唱歌。同时余某、白某、李某1、程某、王某、邬建严（均已判处）等人在“顶尖ＫＴＶ”唱歌，张亮与余某联系后，张亮、潘某到“顶尖ＫＴＶ”找余某玩，后三人又返回“蜕变ＫＴＶ”。余某在“蜕变ＫＴＶ”门口上楼梯的过程中与被害人黄某1相遇，黄某1被余某踩到脚，双方发生争吵，后黄某1打电话给张某1等人到“蜕变ＫＴＶ”与其汇合。被告人张亮见状便邀约毛某、唐某、谭某、卢某从包房出来与其在“蜕变ＫＴＶ”门口楼梯底汇合。同时余某打电话邀约白某、李某1等人到“蜕变ＫＴＶ”与其汇合。2018年5月27日23时48分许，张某1等人到达“蜕变ＫＴＶ”门口与黄某1汇合，张亮在看到张某1等人来到后，意识到双方可能会发生打架，又打电话邀约张敏从包房出来参与打架。23时51分许，白某、李某1、程某、王某、邬建严等6人来到“蜕变ＫＴＶ”门口，6人下车后随即与黄某1、张某1等人发生争吵，因李某1认识黄某1一方的人，遂从中劝阻，但劝阻未成。在争吵过程中张某1先动手打了白某的头，随后双方发生互殴，白某、王某、李某1等人殴打黄某1、张某1，张亮、张敏、谭某、卢某见状便参与斗殴，白某、张敏在打架过程中使用跳刀伤害黄某1、张某1，因有警车经过双方停止斗殴。经鉴定，黄某1的伤情为轻伤一级，张某1的伤情为轻伤二级。2.2017年10月31日，建水县城“鑫都汇ＫＴＶ”开业，朱某、彭某（均已判处）为了争夺该ＫＴＶ看场的权利，朱某一方中的余某打电话邀约被告人张亮，被告人张亮又邀约谭某、毛某、潘某等人到“达鑫都汇ＫＴＶ”。张亮、谭某等人在现场领取钢管、三尖叉等工具准备斗殴。彭某一方的人员驾驶车辆赶到“鑫都汇ＫＴＶ”门口后，张亮等人手持钢管冲出去，对彭某一方及车辆进行追逐、打砸，后张亮等人到“鑫都汇ＫＴＶ”内躲避一段时间后逃离现场。经建水县发展和改革局价格认证中心认定，现场被砸车辆的损失共计人民币20459元。二、非法持有枪支的事实2016年至2017年的一天，被告人张亮从李永康（已判处）处获得一支疑似枪支，并将该疑似枪支藏匿于被告人张敏家中，被告人张敏将该疑似枪支放在自家卧室的床下面。2018年被告人张敏因其他犯罪行为被抓捕后，该疑似枪支又被张亮转移到谭某家中。2019年11月4日，建水县公安局民警在谭某的带领下到建水县西庄镇马厂村227号谭某家中查获该疑似枪支。经红河州公安司法鉴定中心鉴定，该疑似枪支系以火药为动力发射弹丸的民用非制式枪支。上述事实，被告人张亮、张敏在开庭审理过程中亦无异议，且有庭审记录、公诉机关提供并经法庭当庭举证、质证和认证的户口证明，受案登记表、立案决定书，（2019）云2524刑初9号刑事判决书、（2019）云2524刑初305号刑事判决书、将服刑人员提至看守所羁押函、云南省监狱管理局关于罪犯解回侦查的告知函，行政处罚决定书、行政处罚告知笔录，伤情证明、手术记录、红公（司）鉴（痕迹）字［2019］219号鉴定书、（建）公（司）鉴（法损）字［2019］65号鉴定书、（建）公（司）鉴（法损）字［2019］66号鉴定书、建价认字（2017）352号价格认定结论书、建价认字（2017）353号价格认定结论书、建价认字（2018）89号价格认定结论书、价格认定标的物明细表、鉴定意见通知书，被害人张某1、黄某1的陈述，证人余某、谭某、卢某、毛某、程某、白某、付某、王某、李某1、潘某、唐某、朱某、许某、李某2、李某3、吴某、彭某、孙某、胡某1、胡某2、胡某3、张某2、李某4、黄某2、张某3的证言，辨认笔录及照片、辨认监控视频截图笔录及照片、辨认枪支笔录及照片、现场辨认笔录及照片，现场勘验笔录及照片、现场图，光碟1张，被告人张敏、张亮的供述与辩解等证据证实，足以认定', 'label': 1042, 'relevant_article': ['292.0,', '128.0'], 'defendant_judgement': "[{'聚众斗殴': {'有期徒刑': '四年零六个月'}}, {'非法持有、私藏枪支、弹药': {'有期徒刑': '一年'}}, {'有期徒刑': '十五年'}]", 'defendant': '[CLS]张亮[SEP]一、聚众斗殴的事实。1.2018年5月27日晚，被告人张亮、张敏与毛某、潘某、唐某（三人另案处理）、谭某、卢某（二人已判处）等人在建水县临安镇“蜕变ＫＴＶ”内唱歌。同时余某、白某、李某1、程某、王某、邬建严（均已判处）等人在“顶尖ＫＴＶ”唱歌，张亮与余某联系后，张亮、潘某到“顶尖ＫＴＶ”找余某玩，后三人又返回“蜕变ＫＴＶ”。余某在“蜕变ＫＴＶ”门口上楼梯的过程中与被害人黄某1相遇，黄某1被余某踩到脚，双方发生争吵，后黄某1打电话给张某1等人到“蜕变ＫＴＶ”与其汇合。被告人张亮见状便邀约毛某、唐某、谭某、卢某从包房出来与其在“蜕变ＫＴＶ”门口楼梯底汇合。同时余某打电话邀约白某、李某1等人到“蜕变ＫＴＶ”与其汇合。2018年5月27日23时48分许，张某1等人到达“蜕变ＫＴＶ”门口与黄某1汇合，张亮在看到张某1等人来到后，意识到双方可能会发生打架，又打电话邀约张敏从包房出来参与打架。23时51分许，白某、李某1、程某、王某、邬建严等6人来到“蜕变ＫＴＶ”门口，6人下车后随即与黄某1、张某1等人发生争吵，因李某1认识黄某1一方的人，遂从中劝阻，但劝阻未成。在争吵过程中张某1先动手打了白某的头，随后双方发生互殴，白某、王某、李某1等人殴打黄某1、张某1，张亮、张敏、谭某、卢某见状便参与斗殴，白某、张敏在打架过程中使用跳刀伤害黄某1、张某1，因有警车经过双方停止斗殴。经鉴定，黄某1的伤情为轻伤一级，张某1的伤情为轻伤二级。2.2017年10月31日，建水县城“鑫都汇ＫＴＶ”开业，朱某、彭某（均已判处）为了争夺该ＫＴＶ看场的权利，朱某一方中的余某打电话邀约被告人张亮，被告人张亮又邀约谭某、毛某、潘某等人到“达鑫都汇ＫＴＶ”。张亮、谭某等人在现场领取钢管、三尖叉等工具准备斗殴。彭某一方的人员驾驶车辆赶到“鑫都汇ＫＴＶ”门口后，张亮等人手持钢管冲出去，对彭某一方及车辆进行追逐、打砸，后张亮等人到“鑫都汇ＫＴＶ”内躲避一段时间后逃离现场。经建水县发展和改革局价格认证中心认定，现场被砸车辆的损失共计人民币20459元。二、非法持有枪支的事实2016年至2017年的一天，被告人张亮从李永康（已判处）处获得一支疑似枪支，并将该疑似枪支藏匿于被告人张敏家中，被告人张敏将该疑似枪支放在自家卧室的床下面。2018年被告人张敏因其他犯罪行为被抓捕后，该疑似枪支又被张亮转移到谭某家中。2019年11月4日，建水县公安局民警在谭某的带领下到建水县西庄镇马厂村227号谭某家中查获该疑似枪支。经红河州公安司法鉴定中心鉴定，该疑似枪支系以火药为动力发射弹丸的民用非制式枪支。上述事实，被告人张亮、张敏在开庭审理过程中亦无异议，且有庭审记录、公诉机关提供并经法庭当庭举证、质证和认证的户口证明，受案登记表、立案决定书，（2019）云2524刑初9号刑事判决书、（2019）云2524刑初305号刑事判决书、将服刑人员提至看守所羁押函、云南省监狱管理局关于罪犯解回侦查的告知函，行政处罚决定书、行政处罚告知笔录，伤情证明、手术记录、红公（司）鉴（痕迹）字［2019］219号鉴定书、（建）公（司）鉴（法损）字［2019］65号鉴定书、（建）公（司）鉴（法损）字［2019］66号鉴定书、建价认字（2017）352号价格认定结论书、建价认字（2017）353号价格认定结论书、建价认字（2018）89号价格认定结论书、价格认定标的物明细表、鉴定意见通知书，被害人张某1、黄某1的陈述，证人余某、谭某、卢某、毛某、程某、白某、付某、王某、李某1、潘某、唐某、朱某、许某、李某2、李某3、吴某、彭某、孙某、胡某1、胡某2、胡某3、张某2、李某4、黄某2、张某3的证言，辨认笔录及照片、辨认监控视频截图笔录及照片、辨认枪支笔录及照片、现场辨认笔录及照片，现场勘验笔录及照片、现场图，光碟1张，被告人张敏、张亮的供述与辩解等证据证实，足以认定', 'imprisonment': 180.0, 'sentence': '[CLS]张亮[SEP]一、聚众斗殴的事实。1.2018年5月27日晚，被告人张亮、张敏与毛某、潘某、唐某（三人另案处理）、谭某、卢某（二人已判处）等人在建水县临安镇“蜕变ＫＴＶ”内唱歌。同时余某、白某、李某1、程某、王某、邬建严（均已判处）等人在“顶尖ＫＴＶ”唱歌，张亮与余某联系后，张亮、潘某到“顶尖ＫＴＶ”找余某玩，后三人又返回“蜕变ＫＴＶ”。余某在“蜕变ＫＴＶ”门口上楼梯的过程中与被害人黄某1相遇，黄某1被余某踩到脚，双方发生争吵，后黄某1打电话给张某1等人到“蜕变ＫＴＶ”与其汇合。被告人张亮见状便邀约毛某、唐某、谭某、卢某从包房出来与其在“蜕变ＫＴＶ”门口楼梯底汇合。同时余某打电话邀约白某、李某1等人到“蜕变ＫＴＶ”与其汇合。2018年5月27日23时48分许，张某1等人到达“蜕变ＫＴＶ”门口与黄某1汇合，张亮在看到张某1等人来到后，意识到双方可能会发生打架，又打电话邀约张敏从包房出来参与打架。23时51分许，白某、李某1、程某、王某、邬建严等6人来到“蜕变ＫＴＶ”门口，6人下车后随即与黄某1、张某1等人发生争吵，因李某1认识黄某1一方的人，遂从中劝阻，但劝阻未成。在争吵过程中张某1先动手打了白某的头，随后双方发生互殴，白某、王某、李某1等人殴打黄某1、张某1，张亮、张敏、谭某、卢某见状便参与斗殴，白某、张敏在打架过程中使用跳刀伤害黄某1、张某1，因有警车经过双方停止斗殴。经鉴定，黄某1的伤情为轻伤一级，张某1的伤情为轻伤二级。2.2017年10月31日，建水县城“鑫都汇ＫＴＶ”开业，朱某、彭某（均已判处）为了争夺该ＫＴＶ看场的权利，朱某一方中的余某打电话邀约被告人张亮，被告人张亮又邀约谭某、毛某、潘某等人到“达鑫都汇ＫＴＶ”。张亮、谭某等人在现场领取钢管、三尖叉等工具准备斗殴。彭某一方的人员驾驶车辆赶到“鑫都汇ＫＴＶ”门口后，张亮等人手持钢管冲出去，对彭某一方及车辆进行追逐、打砸，后张亮等人到“鑫都汇ＫＴＶ”内躲避一段时间后逃离现场。经建水县发展和改革局价格认证中心认定，现场被砸车辆的损失共计人民币20459元。二、非法持有枪支的事实2016年至2017年的一天，被告人张亮从李永康（已判处）处获得一支疑似枪支，并将该疑似枪支藏匿于被告人张敏家中，被告人张敏将该疑似枪支放在自家卧室的床下面。2018年被告人张敏因其他犯罪行为被抓捕后，该疑似枪支又被张亮转移到谭某家中。2019年11月4日，建水县公安局民警在谭某的带领下到建水县西庄镇马厂村227号谭某家中查获该疑似枪支。经红河州公安司法鉴定中心鉴定，该疑似枪支系以火药为动力发射弹丸的民用非制式枪支。上述事实，被告人张亮、张敏在开庭审理过程中亦无异议，且有庭审记录、公诉机关提供并经法庭当庭举证、质证和认证的户口证明，受案登记表、立案决定书，（2019）云2524刑初9号刑事判决书、（2019）云2524刑初305号刑事判决书、将服刑人员提至看守所羁押函、云南省监狱管理局关于罪犯解回侦查的告知函，行政处罚决定书、行政处罚告知笔录，伤情证明、手术记录、红公（司）鉴（痕迹）字［2019］219号鉴定书、（建）公（司）鉴（法损）字［2019］65号鉴定书、（建）公（司）鉴（法损）字［2019］66号鉴定书、建价认字（2017）352号价格认定结论书、建价认字（2017）353号价格认定结论书、建价认字（2018）89号价格认定结论书、价格认定标的物明细表、鉴定意见通知书，被害人张某1、黄某1的陈述，证人余某、谭某、卢某、毛某、程某、白某、付某、王某、李某1、潘某、唐某、朱某、许某、李某2、李某3、吴某、彭某、孙某、胡某1、胡某2、胡某3、张某2、李某4、黄某2、张某3的证言，辨认笔录及照片、辨认监控视频截图笔录及照片、辨认枪支笔录及照片、现场辨认笔录及照片，现场勘验笔录及照片、现场图，光碟1张，被告人张敏、张亮的供述与辩解等证据证实，足以认定', 'input_ids': [101, 101, 2476, 778, 102, 671, 510, 5471, 830, 3159, 3666, 4638, 752, 2141, 511, 122, 119, 8271, 2399, 126, 3299, 8149, 3189, 3241, 8024, 6158, 1440, 782, 2476, 778, 510, 2476, 3130, 680, 3688, 3378, 510, 4050, 3378, 510, 1538, 3378, 8020, 676, 782, 1369, 3428, 1905, 4415, 8021, 510, 6478, 3378, 510, 1306, 3378, 8020, 753, 782, 2347, 1161, 1905, 8021, 5023, 782, 1762, 2456, 3717, 1344, 707, 2128, 7252, 100, 6053, 1359, 8061, 11766, 12420, 100, 1079, 1548, 3625, 511, 1398, 3198, 865, 3378, 510, 4635, 3378, 510, 3330, 3378, 122, 510, 4923, 3378, 510, 4374, 3378, 510, 6933, 2456, 698, 8020, 1772, 2347, 1161, 1905, 8021, 5023, 782, 1762, 100, 7553, 2211, 8061, 11766, 12420, 100, 1548, 3625, 8024, 2476, 778, 680, 865, 3378, 5468, 5143, 1400, 8024, 2476, 778, 510, 4050, 3378, 1168, 100, 7553, 2211, 8061, 11766, 12420, 100, 2823, 865, 3378, 4381, 8024, 1400, 676, 782, 1348, 6819, 1726, 100, 6053, 1359, 8061, 11766, 12420, 100, 511, 865, 3378, 1762, 100, 6053, 1359, 8061, 11766, 12420, 100, 7305, 1366, 677, 3517, 3461, 4638, 6814, 4923, 704, 680, 6158, 2154, 782, 7942, 3378, 122, 4685, 6878, 8024, 7942, 3378, 122, 6158, 865, 3378, 6678, 1168, 5558, 8024, 1352, 3175, 1355, 4495, 751, 1427, 8024, 1400, 7942, 3378, 122, 2802, 4510, 6413, 5314, 2476, 3378, 122, 5023, 782, 1168, 100, 6053, 1359, 8061, 11766, 12420, 100, 680, 1071, 3726, 1394, 511, 6158, 1440, 782, 2476, 778, 6224, 4307, 912, 6913, 5276, 3688, 3378, 510, 1538, 3378, 510, 6478, 3378, 510, 1306, 3378, 794, 1259, 2791, 1139, 3341, 680, 1071, 1762, 100, 6053, 1359, 8061, 11766, 12420, 100, 7305, 1366, 3517, 3461, 2419, 3726, 1394, 511, 1398, 3198, 865, 3378, 2802, 4510, 6413, 6913, 5276, 4635, 3378, 510, 3330, 3378, 122, 5023, 782, 1168, 100, 6053, 1359, 8061, 11766, 12420, 100, 680, 1071, 3726, 1394, 511, 8271, 2399, 126, 3299, 8149, 3189, 8133, 3198, 8214, 1146, 6387, 8024, 2476, 3378, 122, 5023, 782, 1168, 6809, 100, 6053, 1359, 8061, 11766, 12420, 100, 7305, 1366, 680, 7942, 3378, 122, 3726, 1394, 8024, 2476, 778, 1762, 4692, 1168, 2476, 3378, 122, 5023, 782, 3341, 1168, 1400, 8024, 2692, 6399, 1168, 1352, 3175, 1377, 5543, 833, 1355, 4495, 2802, 3373, 8024, 1348, 2802, 4510, 6413, 6913, 5276, 2476, 3130, 794, 1259, 2791, 1139, 3341, 1346, 680, 2802, 3373, 511, 8133, 3198, 8246, 1146, 6387, 8024, 4635, 3378, 510, 3330, 3378, 122, 510, 4923, 3378, 510, 4374, 3378, 510, 6933, 2456, 698, 5023, 127, 782, 3341, 1168, 100, 6053, 1359, 8061, 11766, 12420, 100, 7305, 1366, 8024, 127, 782, 678, 6756, 1400, 7390, 1315, 680, 7942, 3378, 122, 510, 2476, 3378, 122, 5023, 782, 1355, 4495, 751, 1427, 8024, 1728, 3330, 3378, 122, 6371, 6399, 7942, 3378, 122, 671, 3175, 4638, 782, 8024, 6876, 794, 704, 1214, 7349, 8024, 852, 1214, 7349, 3313, 2768, 511, 1762, 751, 1427, 6814, 4923, 704, 2476, 3378, 122, 1044, 1220, 2797, 2802, 749, 4635, 3378, 4638, 1928, 8024, 7390, 1400, 1352, 3175, 1355, 4495, 757, 3666, 8024, 4635, 3378, 510, 4374, 3378, 510, 3330, 3378, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
01/22/2024 00:21:27 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:738] 2024-01-22 00:21:33,975 >> The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1723] 2024-01-22 00:21:33,986 >> ***** Running training *****
[INFO|trainer.py:1724] 2024-01-22 00:21:33,986 >>   Num examples = 4,000
[INFO|trainer.py:1725] 2024-01-22 00:21:33,987 >>   Num Epochs = 200
[INFO|trainer.py:1726] 2024-01-22 00:21:33,987 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1729] 2024-01-22 00:21:33,987 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1730] 2024-01-22 00:21:33,987 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1731] 2024-01-22 00:21:33,987 >>   Total optimization steps = 25,000
[INFO|trainer.py:1732] 2024-01-22 00:21:33,988 >>   Number of trainable parameters = 127,527,250
[INFO|integration_utils.py:718] 2024-01-22 00:21:33,989 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: loss4wang. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /local/xiaowang/LJP Task/train/bert_roberta_lawformer/wandb/run-20240122_002135-gv2w0i3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lawformer_data4_t2_bs32_lr7e-5
wandb: ⭐️ View project at https://wandb.ai/loss4wang/LJP_baselines_task2
wandb: 🚀 View run at https://wandb.ai/loss4wang/LJP_baselines_task2/runs/gv2w0i3e
  0%|          | 0/25000 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-01-22 00:21:40,227 >> You're using a LongformerTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[INFO|modeling_longformer.py:1920] 2024-01-22 00:21:40,268 >> Initializing global attention on CLS token...
  0%|          | 1/25000 [00:05<39:52:21,  5.74s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:45,976 >> Initializing global attention on CLS token...
  0%|          | 2/25000 [00:07<22:42:24,  3.27s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:47,517 >> Initializing global attention on CLS token...
  0%|          | 3/25000 [00:08<17:11:00,  2.47s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:49,046 >> Initializing global attention on CLS token...
  0%|          | 4/25000 [00:10<14:45:04,  2.12s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:50,633 >> Initializing global attention on CLS token...
  0%|          | 5/25000 [00:11<13:16:57,  1.91s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:52,171 >> Initializing global attention on CLS token...
  0%|          | 6/25000 [00:13<12:21:18,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:53,692 >> Initializing global attention on CLS token...
  0%|          | 7/25000 [00:14<11:48:28,  1.70s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:55,232 >> Initializing global attention on CLS token...
  0%|          | 8/25000 [00:16<11:24:58,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:56,754 >> Initializing global attention on CLS token...
  0%|          | 9/25000 [00:18<11:12:02,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:58,299 >> Initializing global attention on CLS token...
  0%|          | 10/25000 [00:19<11:00:17,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:21:59,823 >> Initializing global attention on CLS token...
  0%|          | 11/25000 [00:21<10:57:59,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:01,390 >> Initializing global attention on CLS token...
  0%|          | 12/25000 [00:22<10:53:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:02,930 >> Initializing global attention on CLS token...
  0%|          | 13/25000 [00:24<10:50:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:04,476 >> Initializing global attention on CLS token...
  0%|          | 14/25000 [00:25<10:48:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:06,023 >> Initializing global attention on CLS token...
  0%|          | 15/25000 [00:27<10:44:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:07,552 >> Initializing global attention on CLS token...
  0%|          | 16/25000 [00:28<10:42:03,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:09,077 >> Initializing global attention on CLS token...
  0%|          | 17/25000 [00:30<10:40:06,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:10,608 >> Initializing global attention on CLS token...
  0%|          | 18/25000 [00:31<10:51:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:12,232 >> Initializing global attention on CLS token...
  0%|          | 19/25000 [00:33<10:47:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:13,766 >> Initializing global attention on CLS token...
  0%|          | 20/25000 [00:35<10:44:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:15,298 >> Initializing global attention on CLS token...
  0%|          | 21/25000 [00:36<10:43:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:16,835 >> Initializing global attention on CLS token...
  0%|          | 22/25000 [00:38<10:41:03,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:18,364 >> Initializing global attention on CLS token...
  0%|          | 23/25000 [00:39<10:40:18,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:19,897 >> Initializing global attention on CLS token...
  0%|          | 24/25000 [00:41<10:39:04,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:21,425 >> Initializing global attention on CLS token...
  0%|          | 25/25000 [00:42<10:48:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:23,036 >> Initializing global attention on CLS token...
  0%|          | 26/25000 [00:44<10:44:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:24,565 >> Initializing global attention on CLS token...
  0%|          | 27/25000 [00:45<10:42:34,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:26,096 >> Initializing global attention on CLS token...
  0%|          | 28/25000 [00:47<10:41:25,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:27,632 >> Initializing global attention on CLS token...
  0%|          | 29/25000 [00:48<10:40:58,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:29,178 >> Initializing global attention on CLS token...
  0%|          | 30/25000 [00:50<10:42:05,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:30,719 >> Initializing global attention on CLS token...
  0%|          | 31/25000 [00:52<10:40:35,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:32,249 >> Initializing global attention on CLS token...
  0%|          | 32/25000 [00:53<10:42:52,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:33,810 >> Initializing global attention on CLS token...
  0%|          | 33/25000 [00:55<10:46:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:35,385 >> Initializing global attention on CLS token...
  0%|          | 34/25000 [00:56<10:43:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:36,915 >> Initializing global attention on CLS token...
  0%|          | 35/25000 [00:58<10:43:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:38,462 >> Initializing global attention on CLS token...
  0%|          | 36/25000 [00:59<10:42:27,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:39,998 >> Initializing global attention on CLS token...
  0%|          | 37/25000 [01:01<10:45:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:41,570 >> Initializing global attention on CLS token...
  0%|          | 38/25000 [01:02<10:43:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:43,169 >> Initializing global attention on CLS token...
  0%|          | 39/25000 [01:04<10:50:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:44,711 >> Initializing global attention on CLS token...
  0%|          | 40/25000 [01:06<10:53:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:46,295 >> Initializing global attention on CLS token...
  0%|          | 41/25000 [01:07<10:48:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:47,829 >> Initializing global attention on CLS token...
  0%|          | 42/25000 [01:09<10:47:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:49,380 >> Initializing global attention on CLS token...
  0%|          | 43/25000 [01:10<10:49:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:50,950 >> Initializing global attention on CLS token...
  0%|          | 44/25000 [01:12<10:52:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:52,540 >> Initializing global attention on CLS token...
  0%|          | 45/25000 [01:13<10:56:12,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:54,133 >> Initializing global attention on CLS token...
  0%|          | 46/25000 [01:15<10:50:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:55,668 >> Initializing global attention on CLS token...
  0%|          | 47/25000 [01:16<10:47:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:57,213 >> Initializing global attention on CLS token...
  0%|          | 48/25000 [01:18<10:46:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:22:58,759 >> Initializing global attention on CLS token...
  0%|          | 49/25000 [01:20<10:46:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:00,318 >> Initializing global attention on CLS token...
  0%|          | 50/25000 [01:21<10:45:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:01,860 >> Initializing global attention on CLS token...
  0%|          | 51/25000 [01:23<10:43:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:03,405 >> Initializing global attention on CLS token...
  0%|          | 52/25000 [01:24<10:51:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:05,005 >> Initializing global attention on CLS token...
  0%|          | 53/25000 [01:26<10:47:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:06,545 >> Initializing global attention on CLS token...
  0%|          | 54/25000 [01:27<10:49:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:08,115 >> Initializing global attention on CLS token...
  0%|          | 55/25000 [01:29<10:49:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:09,676 >> Initializing global attention on CLS token...
  0%|          | 56/25000 [01:30<10:48:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:11,235 >> Initializing global attention on CLS token...
  0%|          | 57/25000 [01:32<10:46:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:12,775 >> Initializing global attention on CLS token...
  0%|          | 58/25000 [01:34<10:44:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:14,313 >> Initializing global attention on CLS token...
  0%|          | 59/25000 [01:35<10:45:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:15,876 >> Initializing global attention on CLS token...
  0%|          | 60/25000 [01:37<10:46:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:17,435 >> Initializing global attention on CLS token...
  0%|          | 61/25000 [01:38<10:44:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:18,975 >> Initializing global attention on CLS token...
  0%|          | 62/25000 [01:40<10:42:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:20,513 >> Initializing global attention on CLS token...
  0%|          | 63/25000 [01:41<10:43:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:22,066 >> Initializing global attention on CLS token...
  0%|          | 64/25000 [01:43<10:42:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:23,606 >> Initializing global attention on CLS token...
  0%|          | 65/25000 [01:44<10:42:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:25,149 >> Initializing global attention on CLS token...
  0%|          | 66/25000 [01:46<10:45:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:26,723 >> Initializing global attention on CLS token...
  0%|          | 67/25000 [01:48<10:44:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:28,267 >> Initializing global attention on CLS token...
  0%|          | 68/25000 [01:49<10:43:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:29,812 >> Initializing global attention on CLS token...
  0%|          | 69/25000 [01:51<10:42:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:31,354 >> Initializing global attention on CLS token...
  0%|          | 70/25000 [01:52<10:43:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:32,904 >> Initializing global attention on CLS token...
  0%|          | 71/25000 [01:54<10:42:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:34,447 >> Initializing global attention on CLS token...
  0%|          | 72/25000 [01:55<10:41:41,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:35,987 >> Initializing global attention on CLS token...
  0%|          | 73/25000 [01:57<10:42:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:37,559 >> Initializing global attention on CLS token...
  0%|          | 74/25000 [01:58<10:44:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:39,099 >> Initializing global attention on CLS token...
  0%|          | 75/25000 [02:00<10:43:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:40,641 >> Initializing global attention on CLS token...
  0%|          | 76/25000 [02:01<10:42:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:42,183 >> Initializing global attention on CLS token...
  0%|          | 77/25000 [02:03<10:43:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:43,735 >> Initializing global attention on CLS token...
  0%|          | 78/25000 [02:05<10:42:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:45,275 >> Initializing global attention on CLS token...
  0%|          | 79/25000 [02:06<10:43:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:46,837 >> Initializing global attention on CLS token...
  0%|          | 80/25000 [02:08<10:42:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:48,377 >> Initializing global attention on CLS token...
  0%|          | 81/25000 [02:09<10:46:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:49,955 >> Initializing global attention on CLS token...
  0%|          | 82/25000 [02:11<10:44:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:51,495 >> Initializing global attention on CLS token...
  0%|          | 83/25000 [02:12<10:42:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:53,035 >> Initializing global attention on CLS token...
  0%|          | 84/25000 [02:14<10:51:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:54,653 >> Initializing global attention on CLS token...
  0%|          | 85/25000 [02:15<10:48:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:56,198 >> Initializing global attention on CLS token...
  0%|          | 86/25000 [02:17<10:47:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:57,755 >> Initializing global attention on CLS token...
  0%|          | 87/25000 [02:19<10:47:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:23:59,313 >> Initializing global attention on CLS token...
  0%|          | 88/25000 [02:20<10:45:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:00,858 >> Initializing global attention on CLS token...
  0%|          | 89/25000 [02:22<10:43:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:02,399 >> Initializing global attention on CLS token...
  0%|          | 90/25000 [02:23<10:42:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:03,939 >> Initializing global attention on CLS token...
  0%|          | 91/25000 [02:25<10:44:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:05,503 >> Initializing global attention on CLS token...
  0%|          | 92/25000 [02:26<10:49:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:07,095 >> Initializing global attention on CLS token...
  0%|          | 93/25000 [02:28<10:48:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:08,653 >> Initializing global attention on CLS token...
  0%|          | 94/25000 [02:29<10:45:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:10,199 >> Initializing global attention on CLS token...
  0%|          | 95/25000 [02:31<10:47:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:11,762 >> Initializing global attention on CLS token...
  0%|          | 96/25000 [02:33<10:45:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:13,304 >> Initializing global attention on CLS token...
  0%|          | 97/25000 [02:34<10:43:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:14,848 >> Initializing global attention on CLS token...
  0%|          | 98/25000 [02:36<10:43:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:16,393 >> Initializing global attention on CLS token...
  0%|          | 99/25000 [02:37<10:46:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:17,970 >> Initializing global attention on CLS token...
  0%|          | 100/25000 [02:39<10:46:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:19,531 >> Initializing global attention on CLS token...
  0%|          | 101/25000 [02:40<10:46:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:21,088 >> Initializing global attention on CLS token...
  0%|          | 102/25000 [02:42<10:47:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:22,655 >> Initializing global attention on CLS token...
  0%|          | 103/25000 [02:43<10:45:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:24,202 >> Initializing global attention on CLS token...
  0%|          | 104/25000 [02:45<10:44:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:25,747 >> Initializing global attention on CLS token...
  0%|          | 105/25000 [02:47<10:44:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:27,310 >> Initializing global attention on CLS token...
  0%|          | 106/25000 [02:48<10:45:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:28,864 >> Initializing global attention on CLS token...
  0%|          | 107/25000 [02:50<10:44:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:30,409 >> Initializing global attention on CLS token...
  0%|          | 108/25000 [02:51<10:43:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:31,952 >> Initializing global attention on CLS token...
  0%|          | 109/25000 [02:53<10:42:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:33,499 >> Initializing global attention on CLS token...
  0%|          | 110/25000 [02:54<10:41:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:35,043 >> Initializing global attention on CLS token...
  0%|          | 111/25000 [02:56<10:40:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:36,583 >> Initializing global attention on CLS token...
  0%|          | 112/25000 [02:57<10:41:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:38,133 >> Initializing global attention on CLS token...
  0%|          | 113/25000 [02:59<10:41:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:39,676 >> Initializing global attention on CLS token...
  0%|          | 114/25000 [03:00<10:40:34,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:41,218 >> Initializing global attention on CLS token...
  0%|          | 115/25000 [03:02<10:40:25,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:42,761 >> Initializing global attention on CLS token...
  0%|          | 116/25000 [03:04<10:42:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:44,320 >> Initializing global attention on CLS token...
  0%|          | 117/25000 [03:05<10:41:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:45,863 >> Initializing global attention on CLS token...
  0%|          | 118/25000 [03:07<10:41:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:47,407 >> Initializing global attention on CLS token...
  0%|          | 119/25000 [03:08<10:45:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:48,992 >> Initializing global attention on CLS token...
  0%|          | 120/25000 [03:10<10:44:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:50,540 >> Initializing global attention on CLS token...
  0%|          | 121/25000 [03:11<10:43:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:52,084 >> Initializing global attention on CLS token...
  0%|          | 122/25000 [03:13<10:42:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:53,631 >> Initializing global attention on CLS token...
  0%|          | 123/25000 [03:14<10:46:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:55,212 >> Initializing global attention on CLS token...
  0%|          | 124/25000 [03:16<10:46:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:24:56,752 >> Initializing global attention on CLS token...
  0%|          | 125/25000 [03:18<10:44:01,  1.55s/it]                                                        0%|          | 125/25000 [03:18<10:44:01,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:24:58,290 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:24:58,296 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:24:58,296 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:24:58,296 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,308 >> Initializing global attention on CLS token...
{'loss': 6.435, 'learning_rate': 6.965e-05, 'epoch': 1.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,427 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,547 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,665 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.59it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,785 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:58,903 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,024 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,143 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  9.00it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,262 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.86it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,381 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  7.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,561 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  7.88it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,682 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,802 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:24:59,921 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:06,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,042 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,163 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,282 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,401 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,522 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,641 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,761 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:00,880 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,000 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,122 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,242 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,361 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,480 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,599 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,718 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,837 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:01,957 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,077 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,196 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,315 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,437 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,556 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,675 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,794 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:02,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:02,914 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,033 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,151 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,270 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,389 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,509 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,627 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,746 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,866 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:03,984 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,104 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,224 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,346 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,466 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,588 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,707 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,827 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:01,  7.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:04,993 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  7.74it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,111 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,230 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,348 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,468 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,587 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,703 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:05,818 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  0%|          | 125/25000 [03:25<10:44:01,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.36it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:25:05,906 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-125
[INFO|configuration_utils.py:461] 2024-01-22 00:25:05,914 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-125/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:25:06,494 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:25:06,495 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:25:06,496 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-125/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:25:07,627 >> Initializing global attention on CLS token...
  1%|          | 126/25000 [03:29<30:10:58,  4.37s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:09,290 >> Initializing global attention on CLS token...
  1%|          | 127/25000 [03:30<24:32:18,  3.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:10,893 >> Initializing global attention on CLS token...
  1%|          | 128/25000 [03:32<20:22:36,  2.95s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:12,438 >> Initializing global attention on CLS token...
  1%|          | 129/25000 [03:33<17:28:51,  2.53s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:13,990 >> Initializing global attention on CLS token...
  1%|          | 130/25000 [03:35<15:26:27,  2.24s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:15,536 >> Initializing global attention on CLS token...
  1%|          | 131/25000 [03:36<14:01:21,  2.03s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:17,087 >> Initializing global attention on CLS token...
  1%|          | 132/25000 [03:38<13:00:49,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:18,631 >> Initializing global attention on CLS token...
  1%|          | 133/25000 [03:39<12:21:01,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:20,193 >> Initializing global attention on CLS token...
  1%|          | 134/25000 [03:41<11:53:21,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:21,757 >> Initializing global attention on CLS token...
  1%|          | 135/25000 [03:43<11:32:59,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:23,317 >> Initializing global attention on CLS token...
  1%|          | 136/25000 [03:44<11:16:58,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:24,861 >> Initializing global attention on CLS token...
  1%|          | 137/25000 [03:46<11:05:34,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:26,404 >> Initializing global attention on CLS token...
  1%|          | 138/25000 [03:47<10:59:17,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:27,961 >> Initializing global attention on CLS token...
  1%|          | 139/25000 [03:49<10:54:45,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:29,519 >> Initializing global attention on CLS token...
  1%|          | 140/25000 [03:50<10:51:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:31,073 >> Initializing global attention on CLS token...
  1%|          | 141/25000 [03:52<10:50:44,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:32,634 >> Initializing global attention on CLS token...
  1%|          | 142/25000 [03:53<10:47:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:34,180 >> Initializing global attention on CLS token...
  1%|          | 143/25000 [03:55<10:44:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:35,717 >> Initializing global attention on CLS token...
  1%|          | 144/25000 [03:57<10:42:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:37,255 >> Initializing global attention on CLS token...
  1%|          | 145/25000 [03:58<10:43:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:38,814 >> Initializing global attention on CLS token...
  1%|          | 146/25000 [04:00<10:41:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:40,353 >> Initializing global attention on CLS token...
  1%|          | 147/25000 [04:01<10:40:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:41,889 >> Initializing global attention on CLS token...
  1%|          | 148/25000 [04:03<10:41:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:43,447 >> Initializing global attention on CLS token...
  1%|          | 149/25000 [04:04<10:40:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:44,991 >> Initializing global attention on CLS token...
  1%|          | 150/25000 [04:06<10:40:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:46,539 >> Initializing global attention on CLS token...
  1%|          | 151/25000 [04:07<10:42:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:48,103 >> Initializing global attention on CLS token...
  1%|          | 152/25000 [04:09<10:43:00,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:49,656 >> Initializing global attention on CLS token...
  1%|          | 153/25000 [04:10<10:41:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:51,197 >> Initializing global attention on CLS token...
  1%|          | 154/25000 [04:12<10:40:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:52,737 >> Initializing global attention on CLS token...
  1%|          | 155/25000 [04:14<10:40:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:54,283 >> Initializing global attention on CLS token...
  1%|          | 156/25000 [04:15<10:41:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:55,844 >> Initializing global attention on CLS token...
  1%|          | 157/25000 [04:17<10:41:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:57,390 >> Initializing global attention on CLS token...
  1%|          | 158/25000 [04:18<10:41:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:25:58,941 >> Initializing global attention on CLS token...
  1%|          | 159/25000 [04:20<10:43:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:00,502 >> Initializing global attention on CLS token...
  1%|          | 160/25000 [04:21<10:41:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:02,047 >> Initializing global attention on CLS token...
  1%|          | 161/25000 [04:23<10:44:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:03,616 >> Initializing global attention on CLS token...
  1%|          | 162/25000 [04:24<10:43:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:05,164 >> Initializing global attention on CLS token...
  1%|          | 163/25000 [04:26<10:41:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:06,707 >> Initializing global attention on CLS token...
  1%|          | 164/25000 [04:28<10:41:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:08,253 >> Initializing global attention on CLS token...
  1%|          | 165/25000 [04:29<10:40:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:09,798 >> Initializing global attention on CLS token...
  1%|          | 166/25000 [04:31<10:41:28,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:11,352 >> Initializing global attention on CLS token...
  1%|          | 167/25000 [04:32<10:42:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:12,912 >> Initializing global attention on CLS token...
  1%|          | 168/25000 [04:34<10:42:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:14,465 >> Initializing global attention on CLS token...
  1%|          | 169/25000 [04:35<10:44:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:16,034 >> Initializing global attention on CLS token...
  1%|          | 170/25000 [04:37<10:42:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:17,575 >> Initializing global attention on CLS token...
  1%|          | 171/25000 [04:38<10:41:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:19,119 >> Initializing global attention on CLS token...
  1%|          | 172/25000 [04:40<10:40:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:20,661 >> Initializing global attention on CLS token...
  1%|          | 173/25000 [04:42<10:45:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:22,250 >> Initializing global attention on CLS token...
  1%|          | 174/25000 [04:43<10:43:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:23,794 >> Initializing global attention on CLS token...
  1%|          | 175/25000 [04:45<10:42:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:25,345 >> Initializing global attention on CLS token...
  1%|          | 176/25000 [04:46<10:47:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:26,932 >> Initializing global attention on CLS token...
  1%|          | 177/25000 [04:48<10:44:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:28,477 >> Initializing global attention on CLS token...
  1%|          | 178/25000 [04:49<10:43:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:30,027 >> Initializing global attention on CLS token...
  1%|          | 179/25000 [04:51<10:42:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:31,577 >> Initializing global attention on CLS token...
  1%|          | 180/25000 [04:52<10:46:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:33,160 >> Initializing global attention on CLS token...
  1%|          | 181/25000 [04:54<10:44:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:34,704 >> Initializing global attention on CLS token...
  1%|          | 182/25000 [04:56<10:42:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:36,248 >> Initializing global attention on CLS token...
  1%|          | 183/25000 [04:57<10:41:32,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:37,794 >> Initializing global attention on CLS token...
  1%|          | 184/25000 [04:59<10:42:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:39,356 >> Initializing global attention on CLS token...
  1%|          | 185/25000 [05:00<10:43:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:40,917 >> Initializing global attention on CLS token...
  1%|          | 186/25000 [05:02<10:41:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:42,460 >> Initializing global attention on CLS token...
  1%|          | 187/25000 [05:03<10:40:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:44,008 >> Initializing global attention on CLS token...
  1%|          | 188/25000 [05:05<10:43:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:45,580 >> Initializing global attention on CLS token...
  1%|          | 189/25000 [05:06<10:43:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:47,129 >> Initializing global attention on CLS token...
  1%|          | 190/25000 [05:08<10:42:00,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:48,674 >> Initializing global attention on CLS token...
  1%|          | 191/25000 [05:09<10:41:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:50,223 >> Initializing global attention on CLS token...
  1%|          | 192/25000 [05:11<10:42:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:51,786 >> Initializing global attention on CLS token...
  1%|          | 193/25000 [05:13<10:41:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:53,330 >> Initializing global attention on CLS token...
  1%|          | 194/25000 [05:14<10:41:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:54,877 >> Initializing global attention on CLS token...
  1%|          | 195/25000 [05:16<10:42:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:56,443 >> Initializing global attention on CLS token...
  1%|          | 196/25000 [05:17<10:41:28,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:57,987 >> Initializing global attention on CLS token...
  1%|          | 197/25000 [05:19<10:41:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:26:59,566 >> Initializing global attention on CLS token...
  1%|          | 198/25000 [05:20<10:44:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:01,117 >> Initializing global attention on CLS token...
  1%|          | 199/25000 [05:22<10:45:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:02,684 >> Initializing global attention on CLS token...
  1%|          | 200/25000 [05:23<10:43:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:04,231 >> Initializing global attention on CLS token...
  1%|          | 201/25000 [05:25<10:42:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:05,785 >> Initializing global attention on CLS token...
  1%|          | 202/25000 [05:27<10:42:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:07,359 >> Initializing global attention on CLS token...
  1%|          | 203/25000 [05:28<10:57:09,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:09,006 >> Initializing global attention on CLS token...
  1%|          | 204/25000 [05:30<10:54:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:10,577 >> Initializing global attention on CLS token...
  1%|          | 205/25000 [05:31<10:53:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:12,152 >> Initializing global attention on CLS token...
  1%|          | 206/25000 [05:33<10:50:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:13,707 >> Initializing global attention on CLS token...
  1%|          | 207/25000 [05:35<10:54:02,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:15,312 >> Initializing global attention on CLS token...
  1%|          | 208/25000 [05:36<10:50:18,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:16,865 >> Initializing global attention on CLS token...
  1%|          | 209/25000 [05:38<10:47:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:18,414 >> Initializing global attention on CLS token...
  1%|          | 210/25000 [05:39<10:46:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:19,974 >> Initializing global attention on CLS token...
  1%|          | 211/25000 [05:41<10:46:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:21,537 >> Initializing global attention on CLS token...
  1%|          | 212/25000 [05:42<10:45:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:23,092 >> Initializing global attention on CLS token...
  1%|          | 213/25000 [05:44<10:43:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:24,640 >> Initializing global attention on CLS token...
  1%|          | 214/25000 [05:45<10:42:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:26,189 >> Initializing global attention on CLS token...
  1%|          | 215/25000 [05:47<10:51:36,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:27,819 >> Initializing global attention on CLS token...
  1%|          | 216/25000 [05:49<10:47:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:29,367 >> Initializing global attention on CLS token...
  1%|          | 217/25000 [05:50<10:45:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:30,916 >> Initializing global attention on CLS token...
  1%|          | 218/25000 [05:52<10:44:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:32,467 >> Initializing global attention on CLS token...
  1%|          | 219/25000 [05:53<10:42:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:34,018 >> Initializing global attention on CLS token...
  1%|          | 220/25000 [05:55<10:44:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:35,584 >> Initializing global attention on CLS token...
  1%|          | 221/25000 [05:56<10:43:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:37,138 >> Initializing global attention on CLS token...
  1%|          | 222/25000 [05:58<10:43:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:38,697 >> Initializing global attention on CLS token...
  1%|          | 223/25000 [06:00<10:42:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:40,243 >> Initializing global attention on CLS token...
  1%|          | 224/25000 [06:01<10:43:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:41,805 >> Initializing global attention on CLS token...
  1%|          | 225/25000 [06:03<10:44:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:43,374 >> Initializing global attention on CLS token...
  1%|          | 226/25000 [06:04<10:45:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:44,941 >> Initializing global attention on CLS token...
  1%|          | 227/25000 [06:06<10:43:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:46,489 >> Initializing global attention on CLS token...
  1%|          | 228/25000 [06:07<10:41:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:48,035 >> Initializing global attention on CLS token...
  1%|          | 229/25000 [06:09<10:41:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:49,582 >> Initializing global attention on CLS token...
  1%|          | 230/25000 [06:10<10:41:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:51,137 >> Initializing global attention on CLS token...
  1%|          | 231/25000 [06:12<10:41:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:52,696 >> Initializing global attention on CLS token...
  1%|          | 232/25000 [06:14<10:42:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:54,259 >> Initializing global attention on CLS token...
  1%|          | 233/25000 [06:15<10:43:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:55,824 >> Initializing global attention on CLS token...
  1%|          | 234/25000 [06:17<10:42:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:57,374 >> Initializing global attention on CLS token...
  1%|          | 235/25000 [06:18<10:42:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:27:58,929 >> Initializing global attention on CLS token...
  1%|          | 236/25000 [06:20<10:41:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:00,476 >> Initializing global attention on CLS token...
  1%|          | 237/25000 [06:21<10:40:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:02,026 >> Initializing global attention on CLS token...
  1%|          | 238/25000 [06:23<10:39:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:03,572 >> Initializing global attention on CLS token...
  1%|          | 239/25000 [06:24<10:43:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:05,160 >> Initializing global attention on CLS token...
  1%|          | 240/25000 [06:26<10:48:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:06,750 >> Initializing global attention on CLS token...
  1%|          | 241/25000 [06:28<10:44:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:08,291 >> Initializing global attention on CLS token...
  1%|          | 242/25000 [06:29<10:42:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:09,841 >> Initializing global attention on CLS token...
  1%|          | 243/25000 [06:31<10:41:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:11,391 >> Initializing global attention on CLS token...
  1%|          | 244/25000 [06:32<10:48:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:12,997 >> Initializing global attention on CLS token...
  1%|          | 245/25000 [06:34<10:44:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:14,543 >> Initializing global attention on CLS token...
  1%|          | 246/25000 [06:35<10:43:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:16,096 >> Initializing global attention on CLS token...
  1%|          | 247/25000 [06:37<10:44:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:17,665 >> Initializing global attention on CLS token...
  1%|          | 248/25000 [06:38<10:43:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:19,222 >> Initializing global attention on CLS token...
  1%|          | 249/25000 [06:40<11:23:51,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:21,085 >> Initializing global attention on CLS token...
  1%|          | 250/25000 [06:42<11:07:34,  1.62s/it]                                                        1%|          | 250/25000 [06:42<11:07:34,  1.62s/it][INFO|trainer.py:738] 2024-01-22 00:28:22,612 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:28:22,625 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:28:22,625 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:28:22,625 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:22,637 >> Initializing global attention on CLS token...
{'eval_loss': 6.101380825042725, 'eval_accuracy': 0.15, 'eval_macro_f1': 0.014465890718894953, 'eval_macro_precision': 0.010579862699226379, 'eval_macro_recall': 0.03396358543417367, 'eval_micro_f1': 0.15, 'eval_micro_precision': 0.15, 'eval_micro_recall': 0.15, 'eval_combined_score': 0.09414419126461357, 'eval_runtime': 7.6076, 'eval_samples_per_second': 65.724, 'eval_steps_per_second': 8.281, 'epoch': 1.0}
{'loss': 5.4321, 'learning_rate': 6.929999999999999e-05, 'epoch': 2.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:22,759 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:04, 13.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:22,904 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,024 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05,  9.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,144 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,263 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,384 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,505 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,626 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,746 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.53it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,866 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:23,986 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,106 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,229 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,350 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,470 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,591 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,711 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,830 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:24,950 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,072 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,194 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,327 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,466 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  7.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,587 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,707 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,827 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:25,947 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,068 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,187 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,308 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,427 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,547 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,666 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,788 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:26,908 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,026 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,148 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,267 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,388 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,516 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,637 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,760 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:27,878 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,001 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,125 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  6.62it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,344 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:02,  7.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,462 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:02,  7.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,583 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  7.67it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,702 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  7.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,823 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  7.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:28,942 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,066 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,186 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,306 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,428 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,548 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,668 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,788 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:29,907 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:30,029 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:30,146 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:30,259 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  1%|          | 250/25000 [06:50<11:07:34,  1.62s/it]
100%|██████████| 63/63 [00:07<00:00,  8.39it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:28:30,346 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-250
[INFO|configuration_utils.py:461] 2024-01-22 00:28:30,355 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-250/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:28:30,899 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:28:30,900 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:28:30,901 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-250/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:28:31,988 >> Initializing global attention on CLS token...
  1%|          | 251/25000 [06:53<30:16:27,  4.40s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:33,534 >> Initializing global attention on CLS token...
  1%|          | 252/25000 [06:54<24:24:01,  3.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:35,091 >> Initializing global attention on CLS token...
  1%|          | 253/25000 [06:56<20:21:31,  2.96s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:36,680 >> Initializing global attention on CLS token...
  1%|          | 254/25000 [06:57<17:26:08,  2.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:38,226 >> Initializing global attention on CLS token...
  1%|          | 255/25000 [06:59<15:23:09,  2.24s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:39,765 >> Initializing global attention on CLS token...
  1%|          | 256/25000 [07:01<13:56:26,  2.03s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:41,303 >> Initializing global attention on CLS token...
  1%|          | 257/25000 [07:02<12:56:06,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:42,844 >> Initializing global attention on CLS token...
  1%|          | 258/25000 [07:04<12:15:17,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:44,398 >> Initializing global attention on CLS token...
  1%|          | 259/25000 [07:05<11:47:16,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:45,956 >> Initializing global attention on CLS token...
  1%|          | 260/25000 [07:07<11:27:29,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:47,511 >> Initializing global attention on CLS token...
  1%|          | 261/25000 [07:08<11:16:02,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:49,085 >> Initializing global attention on CLS token...
  1%|          | 262/25000 [07:10<11:04:28,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:50,633 >> Initializing global attention on CLS token...
  1%|          | 263/25000 [07:11<10:57:53,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:52,193 >> Initializing global attention on CLS token...
  1%|          | 264/25000 [07:13<10:51:54,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:53,739 >> Initializing global attention on CLS token...
  1%|          | 265/25000 [07:15<10:47:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:55,287 >> Initializing global attention on CLS token...
  1%|          | 266/25000 [07:16<10:45:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:56,841 >> Initializing global attention on CLS token...
  1%|          | 267/25000 [07:18<10:42:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:58,383 >> Initializing global attention on CLS token...
  1%|          | 268/25000 [07:19<10:41:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:28:59,933 >> Initializing global attention on CLS token...
  1%|          | 269/25000 [07:21<10:41:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:01,490 >> Initializing global attention on CLS token...
  1%|          | 270/25000 [07:22<10:43:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:03,065 >> Initializing global attention on CLS token...
  1%|          | 271/25000 [07:24<10:42:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:04,612 >> Initializing global attention on CLS token...
  1%|          | 272/25000 [07:25<10:41:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:06,166 >> Initializing global attention on CLS token...
  1%|          | 273/25000 [07:27<10:43:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:07,741 >> Initializing global attention on CLS token...
  1%|          | 274/25000 [07:29<10:42:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:09,293 >> Initializing global attention on CLS token...
  1%|          | 275/25000 [07:30<10:41:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:10,847 >> Initializing global attention on CLS token...
  1%|          | 276/25000 [07:32<10:41:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:12,404 >> Initializing global attention on CLS token...
  1%|          | 277/25000 [07:33<10:42:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:13,967 >> Initializing global attention on CLS token...
  1%|          | 278/25000 [07:35<10:40:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:15,513 >> Initializing global attention on CLS token...
  1%|          | 279/25000 [07:36<10:39:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:17,061 >> Initializing global attention on CLS token...
  1%|          | 280/25000 [07:38<10:42:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:18,633 >> Initializing global attention on CLS token...
  1%|          | 281/25000 [07:39<10:40:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:20,177 >> Initializing global attention on CLS token...
  1%|          | 282/25000 [07:41<10:39:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:21,723 >> Initializing global attention on CLS token...
  1%|          | 283/25000 [07:43<10:40:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:23,296 >> Initializing global attention on CLS token...
  1%|          | 284/25000 [07:44<10:44:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:24,875 >> Initializing global attention on CLS token...
  1%|          | 285/25000 [07:46<10:42:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:26,420 >> Initializing global attention on CLS token...
  1%|          | 286/25000 [07:47<10:40:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:27,967 >> Initializing global attention on CLS token...
  1%|          | 287/25000 [07:49<10:40:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:29,517 >> Initializing global attention on CLS token...
  1%|          | 288/25000 [07:50<10:38:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:31,063 >> Initializing global attention on CLS token...
  1%|          | 289/25000 [07:52<10:38:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:32,608 >> Initializing global attention on CLS token...
  1%|          | 290/25000 [07:53<10:37:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:34,155 >> Initializing global attention on CLS token...
  1%|          | 291/25000 [07:55<10:37:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:35,703 >> Initializing global attention on CLS token...
  1%|          | 292/25000 [07:57<10:38:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:37,254 >> Initializing global attention on CLS token...
  1%|          | 293/25000 [07:58<10:38:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:38,809 >> Initializing global attention on CLS token...
  1%|          | 294/25000 [08:00<10:39:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:40,365 >> Initializing global attention on CLS token...
  1%|          | 295/25000 [08:01<10:43:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:41,951 >> Initializing global attention on CLS token...
  1%|          | 296/25000 [08:03<10:41:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:43,497 >> Initializing global attention on CLS token...
  1%|          | 297/25000 [08:04<10:40:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:45,045 >> Initializing global attention on CLS token...
  1%|          | 298/25000 [08:06<10:39:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:46,599 >> Initializing global attention on CLS token...
  1%|          | 299/25000 [08:07<10:40:00,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:48,153 >> Initializing global attention on CLS token...
  1%|          | 300/25000 [08:09<10:40:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:49,708 >> Initializing global attention on CLS token...
  1%|          | 301/25000 [08:11<10:38:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:51,252 >> Initializing global attention on CLS token...
  1%|          | 302/25000 [08:12<10:40:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:52,816 >> Initializing global attention on CLS token...
  1%|          | 303/25000 [08:14<10:39:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:54,362 >> Initializing global attention on CLS token...
  1%|          | 304/25000 [08:15<10:38:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:55,909 >> Initializing global attention on CLS token...
  1%|          | 305/25000 [08:17<10:46:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:57,524 >> Initializing global attention on CLS token...
  1%|          | 306/25000 [08:18<10:43:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:29:59,072 >> Initializing global attention on CLS token...
  1%|          | 307/25000 [08:20<10:42:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:00,626 >> Initializing global attention on CLS token...
  1%|          | 308/25000 [08:21<10:40:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:02,170 >> Initializing global attention on CLS token...
  1%|          | 309/25000 [08:23<10:41:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:03,737 >> Initializing global attention on CLS token...
  1%|          | 310/25000 [08:25<10:39:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:05,283 >> Initializing global attention on CLS token...
  1%|          | 311/25000 [08:26<10:39:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:06,832 >> Initializing global attention on CLS token...
  1%|          | 312/25000 [08:28<10:38:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:08,381 >> Initializing global attention on CLS token...
  1%|▏         | 313/25000 [08:29<10:41:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:09,960 >> Initializing global attention on CLS token...
  1%|▏         | 314/25000 [08:31<10:40:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:11,508 >> Initializing global attention on CLS token...
  1%|▏         | 315/25000 [08:32<10:40:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:13,065 >> Initializing global attention on CLS token...
  1%|▏         | 316/25000 [08:34<10:41:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:14,628 >> Initializing global attention on CLS token...
  1%|▏         | 317/25000 [08:35<10:40:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:16,185 >> Initializing global attention on CLS token...
  1%|▏         | 318/25000 [08:37<10:41:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:17,772 >> Initializing global attention on CLS token...
  1%|▏         | 319/25000 [08:39<10:45:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:19,341 >> Initializing global attention on CLS token...
  1%|▏         | 320/25000 [08:40<10:47:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:20,927 >> Initializing global attention on CLS token...
  1%|▏         | 321/25000 [08:42<10:47:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:22,500 >> Initializing global attention on CLS token...
  1%|▏         | 322/25000 [08:43<10:45:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:24,059 >> Initializing global attention on CLS token...
  1%|▏         | 323/25000 [08:45<10:46:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:25,638 >> Initializing global attention on CLS token...
  1%|▏         | 324/25000 [08:46<10:46:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:27,206 >> Initializing global attention on CLS token...
  1%|▏         | 325/25000 [08:48<10:43:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:28,753 >> Initializing global attention on CLS token...
  1%|▏         | 326/25000 [08:50<10:42:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:30,309 >> Initializing global attention on CLS token...
  1%|▏         | 327/25000 [08:51<10:40:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:31,857 >> Initializing global attention on CLS token...
  1%|▏         | 328/25000 [08:53<10:39:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:33,404 >> Initializing global attention on CLS token...
  1%|▏         | 329/25000 [08:54<10:38:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:34,953 >> Initializing global attention on CLS token...
  1%|▏         | 330/25000 [08:56<10:38:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:36,505 >> Initializing global attention on CLS token...
  1%|▏         | 331/25000 [08:57<10:44:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:38,105 >> Initializing global attention on CLS token...
  1%|▏         | 332/25000 [08:59<10:42:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:39,658 >> Initializing global attention on CLS token...
  1%|▏         | 333/25000 [09:00<10:41:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:41,215 >> Initializing global attention on CLS token...
  1%|▏         | 334/25000 [09:02<10:40:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:42,762 >> Initializing global attention on CLS token...
  1%|▏         | 335/25000 [09:04<10:40:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:44,327 >> Initializing global attention on CLS token...
  1%|▏         | 336/25000 [09:05<10:39:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:45,874 >> Initializing global attention on CLS token...
  1%|▏         | 337/25000 [09:07<10:38:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:47,420 >> Initializing global attention on CLS token...
  1%|▏         | 338/25000 [09:08<10:40:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:48,995 >> Initializing global attention on CLS token...
  1%|▏         | 339/25000 [09:10<10:39:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:50,552 >> Initializing global attention on CLS token...
  1%|▏         | 340/25000 [09:11<10:39:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:52,106 >> Initializing global attention on CLS token...
  1%|▏         | 341/25000 [09:13<10:41:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:53,672 >> Initializing global attention on CLS token...
  1%|▏         | 342/25000 [09:14<10:39:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:55,227 >> Initializing global attention on CLS token...
  1%|▏         | 343/25000 [09:16<10:39:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:56,776 >> Initializing global attention on CLS token...
  1%|▏         | 344/25000 [09:18<10:38:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:58,322 >> Initializing global attention on CLS token...
  1%|▏         | 345/25000 [09:19<10:42:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:30:59,907 >> Initializing global attention on CLS token...
  1%|▏         | 346/25000 [09:21<10:40:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:01,452 >> Initializing global attention on CLS token...
  1%|▏         | 347/25000 [09:22<10:38:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:02,997 >> Initializing global attention on CLS token...
  1%|▏         | 348/25000 [09:24<10:41:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:04,573 >> Initializing global attention on CLS token...
  1%|▏         | 349/25000 [09:25<10:39:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:06,123 >> Initializing global attention on CLS token...
  1%|▏         | 350/25000 [09:27<10:41:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:07,691 >> Initializing global attention on CLS token...
  1%|▏         | 351/25000 [09:29<10:39:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:09,240 >> Initializing global attention on CLS token...
  1%|▏         | 352/25000 [09:30<10:43:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:10,830 >> Initializing global attention on CLS token...
  1%|▏         | 353/25000 [09:32<10:43:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:12,395 >> Initializing global attention on CLS token...
  1%|▏         | 354/25000 [09:33<10:41:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:13,947 >> Initializing global attention on CLS token...
  1%|▏         | 355/25000 [09:35<10:41:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:15,505 >> Initializing global attention on CLS token...
  1%|▏         | 356/25000 [09:36<10:39:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:17,051 >> Initializing global attention on CLS token...
  1%|▏         | 357/25000 [09:38<10:38:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:18,605 >> Initializing global attention on CLS token...
  1%|▏         | 358/25000 [09:39<10:37:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:20,151 >> Initializing global attention on CLS token...
  1%|▏         | 359/25000 [09:41<10:37:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:21,701 >> Initializing global attention on CLS token...
  1%|▏         | 360/25000 [09:43<10:36:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:23,249 >> Initializing global attention on CLS token...
  1%|▏         | 361/25000 [09:44<10:36:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:24,794 >> Initializing global attention on CLS token...
  1%|▏         | 362/25000 [09:46<10:35:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:26,341 >> Initializing global attention on CLS token...
  1%|▏         | 363/25000 [09:47<10:35:28,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:27,886 >> Initializing global attention on CLS token...
  1%|▏         | 364/25000 [09:49<10:35:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:29,432 >> Initializing global attention on CLS token...
  1%|▏         | 365/25000 [09:50<10:34:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:30,976 >> Initializing global attention on CLS token...
  1%|▏         | 366/25000 [09:52<10:35:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:32,527 >> Initializing global attention on CLS token...
  1%|▏         | 367/25000 [09:53<10:35:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:34,090 >> Initializing global attention on CLS token...
  1%|▏         | 368/25000 [09:55<10:37:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:35,637 >> Initializing global attention on CLS token...
  1%|▏         | 369/25000 [09:56<10:36:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:37,187 >> Initializing global attention on CLS token...
  1%|▏         | 370/25000 [09:58<10:36:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:38,734 >> Initializing global attention on CLS token...
  1%|▏         | 371/25000 [10:00<10:37:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:40,296 >> Initializing global attention on CLS token...
  1%|▏         | 372/25000 [10:01<10:36:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:41,844 >> Initializing global attention on CLS token...
  1%|▏         | 373/25000 [10:03<10:40:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:43,424 >> Initializing global attention on CLS token...
  1%|▏         | 374/25000 [10:04<10:39:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:44,954 >> Initializing global attention on CLS token...
  2%|▏         | 375/25000 [10:06<10:35:57,  1.55s/it]                                                        2%|▏         | 375/25000 [10:06<10:35:57,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:31:46,487 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:31:46,491 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:31:46,491 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:31:46,491 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:46,503 >> Initializing global attention on CLS token...
{'eval_loss': 5.663393974304199, 'eval_accuracy': 0.184, 'eval_macro_f1': 0.02444430916748041, 'eval_macro_precision': 0.01736390553756274, 'eval_macro_recall': 0.05837535014005602, 'eval_micro_f1': 0.184, 'eval_micro_precision': 0.184, 'eval_micro_recall': 0.184, 'eval_combined_score': 0.11945479497787129, 'eval_runtime': 7.7191, 'eval_samples_per_second': 64.774, 'eval_steps_per_second': 8.162, 'epoch': 2.0}
{'loss': 4.8727, 'learning_rate': 6.895e-05, 'epoch': 3.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:46,622 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 15.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:46,753 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:46,872 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:46,991 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,111 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,232 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  9.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,352 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,472 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,592 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.59it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,712 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,835 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:47,972 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,092 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,212 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,333 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,456 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,576 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,696 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,816 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:48,936 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,060 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,181 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,300 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,420 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,539 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,659 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,779 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:49,899 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,020 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,140 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,259 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,379 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,501 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,620 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,740 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,859 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:50,987 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,106 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,227 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,347 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,466 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,585 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,704 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,827 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:51,946 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,065 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,186 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,305 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,424 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,547 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,666 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,785 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:52,905 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,025 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,144 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  7.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,311 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  7.62it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,431 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  7.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,550 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  7.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,670 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,790 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:53,907 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:54,021 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  2%|▏         | 375/25000 [10:13<10:35:57,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.29it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:31:54,105 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-375
[INFO|configuration_utils.py:461] 2024-01-22 00:31:54,113 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-375/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:31:54,695 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-375/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:31:54,696 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-375/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:31:54,697 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-375/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:31:55,832 >> Initializing global attention on CLS token...
  2%|▏         | 376/25000 [10:17<29:43:28,  4.35s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:57,373 >> Initializing global attention on CLS token...
  2%|▏         | 377/25000 [10:18<24:02:37,  3.52s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:31:58,953 >> Initializing global attention on CLS token...
  2%|▏         | 378/25000 [10:20<20:00:02,  2.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:00,498 >> Initializing global attention on CLS token...
  2%|▏         | 379/25000 [10:21<17:10:13,  2.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:02,044 >> Initializing global attention on CLS token...
  2%|▏         | 380/25000 [10:23<15:11:15,  2.22s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:03,585 >> Initializing global attention on CLS token...
  2%|▏         | 381/25000 [10:24<13:50:08,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:05,155 >> Initializing global attention on CLS token...
  2%|▏         | 382/25000 [10:26<12:53:07,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:06,724 >> Initializing global attention on CLS token...
  2%|▏         | 383/25000 [10:28<12:13:33,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:08,273 >> Initializing global attention on CLS token...
  2%|▏         | 384/25000 [10:29<11:44:01,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:09,820 >> Initializing global attention on CLS token...
  2%|▏         | 385/25000 [10:31<11:23:01,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:11,367 >> Initializing global attention on CLS token...
  2%|▏         | 386/25000 [10:32<11:08:20,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:12,913 >> Initializing global attention on CLS token...
  2%|▏         | 387/25000 [10:34<10:57:53,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:14,458 >> Initializing global attention on CLS token...
  2%|▏         | 388/25000 [10:35<10:51:52,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:16,013 >> Initializing global attention on CLS token...
  2%|▏         | 389/25000 [10:37<10:47:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:17,563 >> Initializing global attention on CLS token...
  2%|▏         | 390/25000 [10:38<10:44:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:19,120 >> Initializing global attention on CLS token...
  2%|▏         | 391/25000 [10:40<10:42:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:20,673 >> Initializing global attention on CLS token...
  2%|▏         | 392/25000 [10:41<10:39:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:22,219 >> Initializing global attention on CLS token...
  2%|▏         | 393/25000 [10:43<10:38:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:23,771 >> Initializing global attention on CLS token...
  2%|▏         | 394/25000 [10:45<10:40:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:25,341 >> Initializing global attention on CLS token...
  2%|▏         | 395/25000 [10:46<10:39:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:26,893 >> Initializing global attention on CLS token...
  2%|▏         | 396/25000 [10:48<10:37:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:28,438 >> Initializing global attention on CLS token...
  2%|▏         | 397/25000 [10:49<10:36:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:29,985 >> Initializing global attention on CLS token...
  2%|▏         | 398/25000 [10:51<10:40:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:31,567 >> Initializing global attention on CLS token...
  2%|▏         | 399/25000 [10:52<10:38:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:33,117 >> Initializing global attention on CLS token...
  2%|▏         | 400/25000 [10:54<10:38:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:34,683 >> Initializing global attention on CLS token...
  2%|▏         | 401/25000 [10:55<10:38:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:36,231 >> Initializing global attention on CLS token...
  2%|▏         | 402/25000 [10:57<10:37:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:37,782 >> Initializing global attention on CLS token...
  2%|▏         | 403/25000 [10:59<10:36:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:39,330 >> Initializing global attention on CLS token...
  2%|▏         | 404/25000 [11:00<10:35:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:40,876 >> Initializing global attention on CLS token...
  2%|▏         | 405/25000 [11:02<10:42:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:42,479 >> Initializing global attention on CLS token...
  2%|▏         | 406/25000 [11:03<10:40:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:44,081 >> Initializing global attention on CLS token...
  2%|▏         | 407/25000 [11:05<10:44:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:45,625 >> Initializing global attention on CLS token...
  2%|▏         | 408/25000 [11:06<10:40:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:47,168 >> Initializing global attention on CLS token...
  2%|▏         | 409/25000 [11:08<10:38:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:48,717 >> Initializing global attention on CLS token...
  2%|▏         | 410/25000 [11:10<10:36:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:50,260 >> Initializing global attention on CLS token...
  2%|▏         | 411/25000 [11:11<10:35:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:51,804 >> Initializing global attention on CLS token...
  2%|▏         | 412/25000 [11:13<10:46:40,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:53,443 >> Initializing global attention on CLS token...
  2%|▏         | 413/25000 [11:14<10:42:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:54,987 >> Initializing global attention on CLS token...
  2%|▏         | 414/25000 [11:16<10:39:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:56,535 >> Initializing global attention on CLS token...
  2%|▏         | 415/25000 [11:17<10:37:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:58,078 >> Initializing global attention on CLS token...
  2%|▏         | 416/25000 [11:19<10:36:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:32:59,622 >> Initializing global attention on CLS token...
  2%|▏         | 417/25000 [11:20<10:34:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:01,160 >> Initializing global attention on CLS token...
  2%|▏         | 418/25000 [11:22<10:33:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:02,708 >> Initializing global attention on CLS token...
  2%|▏         | 419/25000 [11:24<10:41:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:04,333 >> Initializing global attention on CLS token...
  2%|▏         | 420/25000 [11:25<10:41:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:05,881 >> Initializing global attention on CLS token...
  2%|▏         | 421/25000 [11:27<10:39:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:07,428 >> Initializing global attention on CLS token...
  2%|▏         | 422/25000 [11:28<10:37:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:08,977 >> Initializing global attention on CLS token...
  2%|▏         | 423/25000 [11:30<10:36:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:10,526 >> Initializing global attention on CLS token...
  2%|▏         | 424/25000 [11:31<10:35:28,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:12,071 >> Initializing global attention on CLS token...
  2%|▏         | 425/25000 [11:33<10:34:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:13,616 >> Initializing global attention on CLS token...
  2%|▏         | 426/25000 [11:35<10:45:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:15,252 >> Initializing global attention on CLS token...
  2%|▏         | 427/25000 [11:36<10:42:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:16,802 >> Initializing global attention on CLS token...
  2%|▏         | 428/25000 [11:38<10:40:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:18,354 >> Initializing global attention on CLS token...
  2%|▏         | 429/25000 [11:39<10:38:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:19,906 >> Initializing global attention on CLS token...
  2%|▏         | 430/25000 [11:41<10:38:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:21,461 >> Initializing global attention on CLS token...
  2%|▏         | 431/25000 [11:42<10:37:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:23,013 >> Initializing global attention on CLS token...
  2%|▏         | 432/25000 [11:44<10:36:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:24,566 >> Initializing global attention on CLS token...
  2%|▏         | 433/25000 [11:45<10:39:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:26,138 >> Initializing global attention on CLS token...
  2%|▏         | 434/25000 [11:47<10:37:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:27,690 >> Initializing global attention on CLS token...
  2%|▏         | 435/25000 [11:49<10:37:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:29,242 >> Initializing global attention on CLS token...
  2%|▏         | 436/25000 [11:50<10:35:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:30,790 >> Initializing global attention on CLS token...
  2%|▏         | 437/25000 [11:52<10:37:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:32,353 >> Initializing global attention on CLS token...
  2%|▏         | 438/25000 [11:53<10:36:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:33,900 >> Initializing global attention on CLS token...
  2%|▏         | 439/25000 [11:55<10:35:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:35,447 >> Initializing global attention on CLS token...
  2%|▏         | 440/25000 [11:56<10:48:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:37,105 >> Initializing global attention on CLS token...
  2%|▏         | 441/25000 [11:58<10:43:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:38,667 >> Initializing global attention on CLS token...
  2%|▏         | 442/25000 [11:59<10:43:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:40,224 >> Initializing global attention on CLS token...
  2%|▏         | 443/25000 [12:01<10:39:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:41,763 >> Initializing global attention on CLS token...
  2%|▏         | 444/25000 [12:03<10:37:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:43,313 >> Initializing global attention on CLS token...
  2%|▏         | 445/25000 [12:04<10:36:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:44,858 >> Initializing global attention on CLS token...
  2%|▏         | 446/25000 [12:06<10:34:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:46,403 >> Initializing global attention on CLS token...
  2%|▏         | 447/25000 [12:07<10:42:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:48,017 >> Initializing global attention on CLS token...
  2%|▏         | 448/25000 [12:09<10:40:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:49,581 >> Initializing global attention on CLS token...
  2%|▏         | 449/25000 [12:10<10:39:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:51,127 >> Initializing global attention on CLS token...
  2%|▏         | 450/25000 [12:12<10:38:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:52,684 >> Initializing global attention on CLS token...
  2%|▏         | 451/25000 [12:14<10:37:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:54,236 >> Initializing global attention on CLS token...
  2%|▏         | 452/25000 [12:15<10:36:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:55,789 >> Initializing global attention on CLS token...
  2%|▏         | 453/25000 [12:17<10:36:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:57,341 >> Initializing global attention on CLS token...
  2%|▏         | 454/25000 [12:18<10:42:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:33:58,955 >> Initializing global attention on CLS token...
  2%|▏         | 455/25000 [12:20<10:41:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:00,510 >> Initializing global attention on CLS token...
  2%|▏         | 456/25000 [12:21<10:38:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:02,057 >> Initializing global attention on CLS token...
  2%|▏         | 457/25000 [12:23<10:36:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:03,603 >> Initializing global attention on CLS token...
  2%|▏         | 458/25000 [12:24<10:39:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:05,182 >> Initializing global attention on CLS token...
  2%|▏         | 459/25000 [12:26<10:37:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:06,732 >> Initializing global attention on CLS token...
  2%|▏         | 460/25000 [12:28<10:36:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:08,277 >> Initializing global attention on CLS token...
  2%|▏         | 461/25000 [12:29<10:35:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:09,827 >> Initializing global attention on CLS token...
  2%|▏         | 462/25000 [12:31<10:36:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:11,387 >> Initializing global attention on CLS token...
  2%|▏         | 463/25000 [12:32<10:34:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:12,933 >> Initializing global attention on CLS token...
  2%|▏         | 464/25000 [12:34<10:34:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:14,482 >> Initializing global attention on CLS token...
  2%|▏         | 465/25000 [12:35<10:40:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:16,079 >> Initializing global attention on CLS token...
  2%|▏         | 466/25000 [12:37<10:38:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:17,631 >> Initializing global attention on CLS token...
  2%|▏         | 467/25000 [12:38<10:36:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:19,180 >> Initializing global attention on CLS token...
  2%|▏         | 468/25000 [12:40<10:39:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:20,765 >> Initializing global attention on CLS token...
  2%|▏         | 469/25000 [12:42<10:40:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:22,330 >> Initializing global attention on CLS token...
  2%|▏         | 470/25000 [12:43<10:39:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:23,890 >> Initializing global attention on CLS token...
  2%|▏         | 471/25000 [12:45<10:37:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:25,437 >> Initializing global attention on CLS token...
  2%|▏         | 472/25000 [12:46<10:36:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:26,992 >> Initializing global attention on CLS token...
  2%|▏         | 473/25000 [12:48<10:35:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:28,537 >> Initializing global attention on CLS token...
  2%|▏         | 474/25000 [12:49<10:34:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:30,090 >> Initializing global attention on CLS token...
  2%|▏         | 475/25000 [12:51<10:34:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:31,750 >> Initializing global attention on CLS token...
  2%|▏         | 476/25000 [12:53<10:47:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:33,295 >> Initializing global attention on CLS token...
  2%|▏         | 477/25000 [12:54<10:42:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:34,840 >> Initializing global attention on CLS token...
  2%|▏         | 478/25000 [12:56<10:39:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:36,386 >> Initializing global attention on CLS token...
  2%|▏         | 479/25000 [12:57<10:38:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:37,940 >> Initializing global attention on CLS token...
  2%|▏         | 480/25000 [12:59<10:36:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:39,487 >> Initializing global attention on CLS token...
  2%|▏         | 481/25000 [13:00<10:34:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:41,033 >> Initializing global attention on CLS token...
  2%|▏         | 482/25000 [13:02<10:33:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:42,633 >> Initializing global attention on CLS token...
  2%|▏         | 483/25000 [13:03<10:39:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:44,179 >> Initializing global attention on CLS token...
  2%|▏         | 484/25000 [13:05<10:38:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:45,734 >> Initializing global attention on CLS token...
  2%|▏         | 485/25000 [13:07<10:36:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:47,279 >> Initializing global attention on CLS token...
  2%|▏         | 486/25000 [13:08<10:37:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:48,845 >> Initializing global attention on CLS token...
  2%|▏         | 487/25000 [13:10<10:35:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:50,392 >> Initializing global attention on CLS token...
  2%|▏         | 488/25000 [13:11<10:34:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:51,939 >> Initializing global attention on CLS token...
  2%|▏         | 489/25000 [13:13<10:33:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:53,548 >> Initializing global attention on CLS token...
  2%|▏         | 490/25000 [13:14<10:41:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:55,099 >> Initializing global attention on CLS token...
  2%|▏         | 491/25000 [13:16<10:38:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:56,645 >> Initializing global attention on CLS token...
  2%|▏         | 492/25000 [13:17<10:36:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:58,192 >> Initializing global attention on CLS token...
  2%|▏         | 493/25000 [13:19<10:36:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:34:59,748 >> Initializing global attention on CLS token...
  2%|▏         | 494/25000 [13:21<10:34:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:01,291 >> Initializing global attention on CLS token...
  2%|▏         | 495/25000 [13:22<10:33:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:02,837 >> Initializing global attention on CLS token...
  2%|▏         | 496/25000 [13:24<10:32:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:04,425 >> Initializing global attention on CLS token...
  2%|▏         | 497/25000 [13:25<10:42:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:06,009 >> Initializing global attention on CLS token...
  2%|▏         | 498/25000 [13:27<10:39:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:07,578 >> Initializing global attention on CLS token...
  2%|▏         | 499/25000 [13:28<10:40:06,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:09,110 >> Initializing global attention on CLS token...
  2%|▏         | 500/25000 [13:30<10:35:51,  1.56s/it]                                                        2%|▏         | 500/25000 [13:30<10:35:51,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:35:10,642 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:35:10,645 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:35:10,645 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:35:10,645 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:10,657 >> Initializing global attention on CLS token...
{'eval_loss': 5.4254536628723145, 'eval_accuracy': 0.186, 'eval_macro_f1': 0.026997470333498814, 'eval_macro_precision': 0.020260818850224783, 'eval_macro_recall': 0.06132514230181868, 'eval_micro_f1': 0.186, 'eval_micro_precision': 0.186, 'eval_micro_recall': 0.186, 'eval_combined_score': 0.12179763306936318, 'eval_runtime': 7.613, 'eval_samples_per_second': 65.677, 'eval_steps_per_second': 8.275, 'epoch': 3.0}
{'loss': 4.4372, 'learning_rate': 6.859999999999999e-05, 'epoch': 4.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:10,778 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:10,899 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,017 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,136 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,256 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,376 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,495 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,613 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.86it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,733 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.66it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,857 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.57it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:11,976 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:05,  8.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,097 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,216 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,335 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.44it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,454 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,580 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,701 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,820 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:12,940 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,060 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,179 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,300 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,421 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,542 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,662 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,782 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:13,908 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,027 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,146 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,266 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,385 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,504 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,623 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,743 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,865 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:14,984 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,105 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,224 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:03,  7.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,404 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:03,  7.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,523 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  7.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,642 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  7.96it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,761 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,880 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:15,999 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,118 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,238 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,358 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,478 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,597 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,716 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,836 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:16,956 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,074 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,195 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,313 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,432 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,552 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,671 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,790 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:17,910 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:18,028 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:18,142 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  2%|▏         | 500/25000 [13:38<10:35:51,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.40it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:35:18,227 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-500
[INFO|configuration_utils.py:461] 2024-01-22 00:35:18,235 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-500/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:35:18,821 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:35:18,823 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:35:18,823 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-500/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:35:19,924 >> Initializing global attention on CLS token...
  2%|▏         | 501/25000 [13:41<29:32:09,  4.34s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:21,497 >> Initializing global attention on CLS token...
  2%|▏         | 502/25000 [13:42<23:49:51,  3.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:23,042 >> Initializing global attention on CLS token...
  2%|▏         | 503/25000 [13:44<19:50:22,  2.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:24,590 >> Initializing global attention on CLS token...
  2%|▏         | 504/25000 [13:45<17:02:29,  2.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:26,135 >> Initializing global attention on CLS token...
  2%|▏         | 505/25000 [13:47<15:07:11,  2.22s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:27,699 >> Initializing global attention on CLS token...
  2%|▏         | 506/25000 [13:49<13:43:56,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:29,241 >> Initializing global attention on CLS token...
  2%|▏         | 507/25000 [13:50<12:46:01,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:30,787 >> Initializing global attention on CLS token...
  2%|▏         | 508/25000 [13:52<12:06:29,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:32,343 >> Initializing global attention on CLS token...
  2%|▏         | 509/25000 [13:53<11:37:39,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:33,883 >> Initializing global attention on CLS token...
  2%|▏         | 510/25000 [13:55<11:16:31,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:35,420 >> Initializing global attention on CLS token...
  2%|▏         | 511/25000 [13:56<11:02:23,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:36,965 >> Initializing global attention on CLS token...
  2%|▏         | 512/25000 [13:58<11:07:48,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:38,632 >> Initializing global attention on CLS token...
  2%|▏         | 513/25000 [13:59<10:57:08,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:40,182 >> Initializing global attention on CLS token...
  2%|▏         | 514/25000 [14:01<10:48:54,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:41,725 >> Initializing global attention on CLS token...
  2%|▏         | 515/25000 [14:03<10:44:48,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:43,282 >> Initializing global attention on CLS token...
  2%|▏         | 516/25000 [14:04<10:40:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:44,827 >> Initializing global attention on CLS token...
  2%|▏         | 517/25000 [14:06<10:38:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:46,379 >> Initializing global attention on CLS token...
  2%|▏         | 518/25000 [14:07<10:36:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:47,930 >> Initializing global attention on CLS token...
  2%|▏         | 519/25000 [14:09<10:36:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:49,489 >> Initializing global attention on CLS token...
  2%|▏         | 520/25000 [14:10<10:34:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:51,031 >> Initializing global attention on CLS token...
  2%|▏         | 521/25000 [14:12<10:38:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:52,624 >> Initializing global attention on CLS token...
  2%|▏         | 522/25000 [14:13<10:38:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:54,188 >> Initializing global attention on CLS token...
  2%|▏         | 523/25000 [14:15<10:36:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:55,740 >> Initializing global attention on CLS token...
  2%|▏         | 524/25000 [14:17<10:35:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:57,286 >> Initializing global attention on CLS token...
  2%|▏         | 525/25000 [14:18<10:33:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:35:58,831 >> Initializing global attention on CLS token...
  2%|▏         | 526/25000 [14:20<10:35:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:00,401 >> Initializing global attention on CLS token...
  2%|▏         | 527/25000 [14:21<10:33:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:01,945 >> Initializing global attention on CLS token...
  2%|▏         | 528/25000 [14:23<10:32:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:03,492 >> Initializing global attention on CLS token...
  2%|▏         | 529/25000 [14:24<10:32:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:05,080 >> Initializing global attention on CLS token...
  2%|▏         | 530/25000 [14:26<10:41:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:06,666 >> Initializing global attention on CLS token...
  2%|▏         | 531/25000 [14:27<10:38:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:08,210 >> Initializing global attention on CLS token...
  2%|▏         | 532/25000 [14:29<10:35:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:09,757 >> Initializing global attention on CLS token...
  2%|▏         | 533/25000 [14:31<10:35:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:11,311 >> Initializing global attention on CLS token...
  2%|▏         | 534/25000 [14:32<10:34:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:12,861 >> Initializing global attention on CLS token...
  2%|▏         | 535/25000 [14:34<10:32:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:14,406 >> Initializing global attention on CLS token...
  2%|▏         | 536/25000 [14:35<10:32:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:15,956 >> Initializing global attention on CLS token...
  2%|▏         | 537/25000 [14:37<10:47:25,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:17,627 >> Initializing global attention on CLS token...
  2%|▏         | 538/25000 [14:38<10:41:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:19,170 >> Initializing global attention on CLS token...
  2%|▏         | 539/25000 [14:40<10:38:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:20,718 >> Initializing global attention on CLS token...
  2%|▏         | 540/25000 [14:42<10:45:33,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:22,341 >> Initializing global attention on CLS token...
  2%|▏         | 541/25000 [14:43<10:40:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:23,884 >> Initializing global attention on CLS token...
  2%|▏         | 542/25000 [14:45<10:37:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:25,429 >> Initializing global attention on CLS token...
  2%|▏         | 543/25000 [14:46<10:40:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:27,016 >> Initializing global attention on CLS token...
  2%|▏         | 544/25000 [14:48<10:36:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:28,560 >> Initializing global attention on CLS token...
  2%|▏         | 545/25000 [14:49<10:35:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:30,132 >> Initializing global attention on CLS token...
  2%|▏         | 546/25000 [14:51<10:37:59,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:31,693 >> Initializing global attention on CLS token...
  2%|▏         | 547/25000 [14:53<10:36:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:33,249 >> Initializing global attention on CLS token...
  2%|▏         | 548/25000 [14:54<10:34:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:34,797 >> Initializing global attention on CLS token...
  2%|▏         | 549/25000 [14:56<10:33:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:36,346 >> Initializing global attention on CLS token...
  2%|▏         | 550/25000 [14:57<10:33:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:37,897 >> Initializing global attention on CLS token...
  2%|▏         | 551/25000 [14:59<10:33:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:39,446 >> Initializing global attention on CLS token...
  2%|▏         | 552/25000 [15:00<10:31:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:40,989 >> Initializing global attention on CLS token...
  2%|▏         | 553/25000 [15:02<10:30:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:42,528 >> Initializing global attention on CLS token...
  2%|▏         | 554/25000 [15:03<10:30:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:44,079 >> Initializing global attention on CLS token...
  2%|▏         | 555/25000 [15:05<10:29:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:45,624 >> Initializing global attention on CLS token...
  2%|▏         | 556/25000 [15:06<10:29:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:47,169 >> Initializing global attention on CLS token...
  2%|▏         | 557/25000 [15:08<10:30:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:48,724 >> Initializing global attention on CLS token...
  2%|▏         | 558/25000 [15:10<10:33:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:50,293 >> Initializing global attention on CLS token...
  2%|▏         | 559/25000 [15:11<10:33:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:51,850 >> Initializing global attention on CLS token...
  2%|▏         | 560/25000 [15:13<10:32:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:53,396 >> Initializing global attention on CLS token...
  2%|▏         | 561/25000 [15:14<10:31:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:54,945 >> Initializing global attention on CLS token...
  2%|▏         | 562/25000 [15:16<10:33:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:56,512 >> Initializing global attention on CLS token...
  2%|▏         | 563/25000 [15:17<10:32:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:58,057 >> Initializing global attention on CLS token...
  2%|▏         | 564/25000 [15:19<10:31:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:36:59,604 >> Initializing global attention on CLS token...
  2%|▏         | 565/25000 [15:20<10:31:32,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:01,154 >> Initializing global attention on CLS token...
  2%|▏         | 566/25000 [15:22<10:31:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:02,700 >> Initializing global attention on CLS token...
  2%|▏         | 567/25000 [15:24<10:30:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:04,246 >> Initializing global attention on CLS token...
  2%|▏         | 568/25000 [15:25<10:29:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:05,789 >> Initializing global attention on CLS token...
  2%|▏         | 569/25000 [15:27<10:33:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:07,369 >> Initializing global attention on CLS token...
  2%|▏         | 570/25000 [15:28<10:32:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:08,915 >> Initializing global attention on CLS token...
  2%|▏         | 571/25000 [15:30<10:31:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:10,462 >> Initializing global attention on CLS token...
  2%|▏         | 572/25000 [15:31<10:34:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:12,034 >> Initializing global attention on CLS token...
  2%|▏         | 573/25000 [15:33<10:32:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:13,577 >> Initializing global attention on CLS token...
  2%|▏         | 574/25000 [15:34<10:31:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:15,124 >> Initializing global attention on CLS token...
  2%|▏         | 575/25000 [15:36<10:31:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:16,672 >> Initializing global attention on CLS token...
  2%|▏         | 576/25000 [15:38<10:39:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:18,292 >> Initializing global attention on CLS token...
  2%|▏         | 577/25000 [15:39<10:37:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:19,846 >> Initializing global attention on CLS token...
  2%|▏         | 578/25000 [15:41<10:35:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:21,398 >> Initializing global attention on CLS token...
  2%|▏         | 579/25000 [15:42<10:35:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:22,958 >> Initializing global attention on CLS token...
  2%|▏         | 580/25000 [15:44<10:33:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:24,500 >> Initializing global attention on CLS token...
  2%|▏         | 581/25000 [15:45<10:33:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:26,056 >> Initializing global attention on CLS token...
  2%|▏         | 582/25000 [15:47<10:32:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:27,606 >> Initializing global attention on CLS token...
  2%|▏         | 583/25000 [15:48<10:31:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:29,157 >> Initializing global attention on CLS token...
  2%|▏         | 584/25000 [15:50<10:31:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:30,705 >> Initializing global attention on CLS token...
  2%|▏         | 585/25000 [15:52<10:30:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:32,252 >> Initializing global attention on CLS token...
  2%|▏         | 586/25000 [15:53<10:30:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:33,801 >> Initializing global attention on CLS token...
  2%|▏         | 587/25000 [15:55<10:29:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:35,346 >> Initializing global attention on CLS token...
  2%|▏         | 588/25000 [15:56<10:29:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:36,891 >> Initializing global attention on CLS token...
  2%|▏         | 589/25000 [15:58<10:29:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:38,436 >> Initializing global attention on CLS token...
  2%|▏         | 590/25000 [15:59<10:31:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:40,002 >> Initializing global attention on CLS token...
  2%|▏         | 591/25000 [16:01<10:31:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:41,555 >> Initializing global attention on CLS token...
  2%|▏         | 592/25000 [16:02<10:40:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:43,180 >> Initializing global attention on CLS token...
  2%|▏         | 593/25000 [16:04<10:41:55,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:44,763 >> Initializing global attention on CLS token...
  2%|▏         | 594/25000 [16:06<10:39:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:46,321 >> Initializing global attention on CLS token...
  2%|▏         | 595/25000 [16:07<10:39:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:47,897 >> Initializing global attention on CLS token...
  2%|▏         | 596/25000 [16:09<10:38:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:49,456 >> Initializing global attention on CLS token...
  2%|▏         | 597/25000 [16:10<10:38:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:51,026 >> Initializing global attention on CLS token...
  2%|▏         | 598/25000 [16:12<10:37:13,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:52,588 >> Initializing global attention on CLS token...
  2%|▏         | 599/25000 [16:13<10:35:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:54,140 >> Initializing global attention on CLS token...
  2%|▏         | 600/25000 [16:15<10:34:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:55,694 >> Initializing global attention on CLS token...
  2%|▏         | 601/25000 [16:17<10:42:15,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:57,319 >> Initializing global attention on CLS token...
  2%|▏         | 602/25000 [16:18<10:44:25,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:37:58,917 >> Initializing global attention on CLS token...
  2%|▏         | 603/25000 [16:20<10:40:29,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:00,469 >> Initializing global attention on CLS token...
  2%|▏         | 604/25000 [16:21<10:37:17,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:02,018 >> Initializing global attention on CLS token...
  2%|▏         | 605/25000 [16:23<10:34:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:03,564 >> Initializing global attention on CLS token...
  2%|▏         | 606/25000 [16:24<10:34:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:05,121 >> Initializing global attention on CLS token...
  2%|▏         | 607/25000 [16:26<10:32:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:06,668 >> Initializing global attention on CLS token...
  2%|▏         | 608/25000 [16:27<10:33:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:08,231 >> Initializing global attention on CLS token...
  2%|▏         | 609/25000 [16:29<10:31:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:09,777 >> Initializing global attention on CLS token...
  2%|▏         | 610/25000 [16:31<10:31:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:11,332 >> Initializing global attention on CLS token...
  2%|▏         | 611/25000 [16:32<10:30:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:12,878 >> Initializing global attention on CLS token...
  2%|▏         | 612/25000 [16:34<10:30:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:14,429 >> Initializing global attention on CLS token...
  2%|▏         | 613/25000 [16:35<10:31:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:15,989 >> Initializing global attention on CLS token...
  2%|▏         | 614/25000 [16:37<10:30:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:17,535 >> Initializing global attention on CLS token...
  2%|▏         | 615/25000 [16:38<10:30:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:19,082 >> Initializing global attention on CLS token...
  2%|▏         | 616/25000 [16:40<10:29:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:20,630 >> Initializing global attention on CLS token...
  2%|▏         | 617/25000 [16:41<10:35:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:22,227 >> Initializing global attention on CLS token...
  2%|▏         | 618/25000 [16:43<10:43:34,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:23,854 >> Initializing global attention on CLS token...
  2%|▏         | 619/25000 [16:45<10:39:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:25,403 >> Initializing global attention on CLS token...
  2%|▏         | 620/25000 [16:46<10:36:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:26,955 >> Initializing global attention on CLS token...
  2%|▏         | 621/25000 [16:48<10:34:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:28,506 >> Initializing global attention on CLS token...
  2%|▏         | 622/25000 [16:49<10:33:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:30,063 >> Initializing global attention on CLS token...
  2%|▏         | 623/25000 [16:51<10:32:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:31,612 >> Initializing global attention on CLS token...
  2%|▏         | 624/25000 [16:52<10:31:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:33,138 >> Initializing global attention on CLS token...
  2%|▎         | 625/25000 [16:54<10:28:07,  1.55s/it]                                                        2%|▎         | 625/25000 [16:54<10:28:07,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:38:34,669 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:38:34,671 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:38:34,672 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:38:34,672 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:34,684 >> Initializing global attention on CLS token...
{'eval_loss': 5.27437686920166, 'eval_accuracy': 0.2, 'eval_macro_f1': 0.03486504674098733, 'eval_macro_precision': 0.029733081166957016, 'eval_macro_recall': 0.07129515599343185, 'eval_micro_f1': 0.20000000000000004, 'eval_micro_precision': 0.2, 'eval_micro_recall': 0.2, 'eval_combined_score': 0.13369904055733947, 'eval_runtime': 7.5793, 'eval_samples_per_second': 65.969, 'eval_steps_per_second': 8.312, 'epoch': 4.0}
{'loss': 4.0663, 'learning_rate': 6.824999999999999e-05, 'epoch': 5.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:34,809 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:04, 12.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:34,973 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,097 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:06,  9.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,216 >> Initializing global attention on CLS token...

  8%|▊         | 5/63 [00:00<00:06,  9.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,335 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  8.88it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,455 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,576 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,698 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:06,  8.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,818 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:35,940 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,060 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,181 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,301 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,423 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,553 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,672 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,793 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:36,913 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,033 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,165 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,285 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,405 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,524 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,644 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:05,  6.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,849 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:05,  7.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:37,967 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  7.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,086 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  7.79it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,206 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,325 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,495 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:04,  7.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,614 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:04,  7.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,734 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:04<00:03,  7.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,854 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  7.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:38,974 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,094 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,213 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,333 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,453 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,579 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,704 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:05<00:02,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,826 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:39,947 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,067 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,187 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,308 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,431 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:02,  7.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,593 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  7.74it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,711 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:06<00:01,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,831 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:40,950 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,073 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,193 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  7.96it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,326 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,449 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,569 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,688 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,809 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:41,932 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:42,051 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:42,172 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:42,292 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:42,406 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  2%|▎         | 625/25000 [17:02<10:28:07,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.30it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:38:42,491 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-625
[INFO|configuration_utils.py:461] 2024-01-22 00:38:42,499 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-625/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:38:43,096 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-625/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:38:43,097 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-625/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:38:43,098 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-625/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:38:44,327 >> Initializing global attention on CLS token...
  3%|▎         | 626/25000 [17:05<30:12:49,  4.46s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:45,953 >> Initializing global attention on CLS token...
  3%|▎         | 627/25000 [17:07<24:27:30,  3.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:47,584 >> Initializing global attention on CLS token...
  3%|▎         | 628/25000 [17:08<20:15:41,  2.99s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:49,129 >> Initializing global attention on CLS token...
  3%|▎         | 629/25000 [17:10<17:19:21,  2.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:50,675 >> Initializing global attention on CLS token...
  3%|▎         | 630/25000 [17:12<15:19:31,  2.26s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:52,252 >> Initializing global attention on CLS token...
  3%|▎         | 631/25000 [17:13<13:52:07,  2.05s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:53,798 >> Initializing global attention on CLS token...
  3%|▎         | 632/25000 [17:15<12:52:12,  1.90s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:55,356 >> Initializing global attention on CLS token...
  3%|▎         | 633/25000 [17:16<12:09:49,  1.80s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:56,909 >> Initializing global attention on CLS token...
  3%|▎         | 634/25000 [17:18<11:39:41,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:38:58,460 >> Initializing global attention on CLS token...
  3%|▎         | 635/25000 [17:19<11:18:12,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:00,006 >> Initializing global attention on CLS token...
  3%|▎         | 636/25000 [17:21<11:02:48,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:01,551 >> Initializing global attention on CLS token...
  3%|▎         | 637/25000 [17:22<10:52:14,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:03,096 >> Initializing global attention on CLS token...
  3%|▎         | 638/25000 [17:24<10:47:59,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:04,668 >> Initializing global attention on CLS token...
  3%|▎         | 639/25000 [17:25<10:43:07,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:06,224 >> Initializing global attention on CLS token...
  3%|▎         | 640/25000 [17:27<10:38:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:07,768 >> Initializing global attention on CLS token...
  3%|▎         | 641/25000 [17:29<10:35:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:09,319 >> Initializing global attention on CLS token...
  3%|▎         | 642/25000 [17:30<10:36:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:10,890 >> Initializing global attention on CLS token...
  3%|▎         | 643/25000 [17:32<10:35:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:12,451 >> Initializing global attention on CLS token...
  3%|▎         | 644/25000 [17:33<10:34:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:14,005 >> Initializing global attention on CLS token...
  3%|▎         | 645/25000 [17:35<10:36:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:15,591 >> Initializing global attention on CLS token...
  3%|▎         | 646/25000 [17:36<10:35:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:17,150 >> Initializing global attention on CLS token...
  3%|▎         | 647/25000 [17:38<10:34:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:18,709 >> Initializing global attention on CLS token...
  3%|▎         | 648/25000 [17:40<10:34:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:20,267 >> Initializing global attention on CLS token...
  3%|▎         | 649/25000 [17:41<10:41:12,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:21,888 >> Initializing global attention on CLS token...
  3%|▎         | 650/25000 [17:43<10:36:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:23,433 >> Initializing global attention on CLS token...
  3%|▎         | 651/25000 [17:44<10:33:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:24,978 >> Initializing global attention on CLS token...
  3%|▎         | 652/25000 [17:46<10:32:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:26,534 >> Initializing global attention on CLS token...
  3%|▎         | 653/25000 [17:47<10:34:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:28,105 >> Initializing global attention on CLS token...
  3%|▎         | 654/25000 [17:49<10:33:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:29,657 >> Initializing global attention on CLS token...
  3%|▎         | 655/25000 [17:50<10:30:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:31,195 >> Initializing global attention on CLS token...
  3%|▎         | 656/25000 [17:52<10:29:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:32,745 >> Initializing global attention on CLS token...
  3%|▎         | 657/25000 [17:54<10:29:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:34,300 >> Initializing global attention on CLS token...
  3%|▎         | 658/25000 [17:55<10:29:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:35,846 >> Initializing global attention on CLS token...
  3%|▎         | 659/25000 [17:57<10:28:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:37,392 >> Initializing global attention on CLS token...
  3%|▎         | 660/25000 [17:58<10:28:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:38,945 >> Initializing global attention on CLS token...
  3%|▎         | 661/25000 [18:00<10:29:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:40,498 >> Initializing global attention on CLS token...
  3%|▎         | 662/25000 [18:01<10:27:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:42,036 >> Initializing global attention on CLS token...
  3%|▎         | 663/25000 [18:03<10:28:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:43,596 >> Initializing global attention on CLS token...
  3%|▎         | 664/25000 [18:04<10:29:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:45,148 >> Initializing global attention on CLS token...
  3%|▎         | 665/25000 [18:06<10:28:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:46,694 >> Initializing global attention on CLS token...
  3%|▎         | 666/25000 [18:08<10:27:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:48,238 >> Initializing global attention on CLS token...
  3%|▎         | 667/25000 [18:09<10:27:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:49,784 >> Initializing global attention on CLS token...
  3%|▎         | 668/25000 [18:11<10:27:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:51,330 >> Initializing global attention on CLS token...
  3%|▎         | 669/25000 [18:12<10:28:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:52,884 >> Initializing global attention on CLS token...
  3%|▎         | 670/25000 [18:14<10:27:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:54,429 >> Initializing global attention on CLS token...
  3%|▎         | 671/25000 [18:15<10:29:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:55,990 >> Initializing global attention on CLS token...
  3%|▎         | 672/25000 [18:17<10:28:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:57,538 >> Initializing global attention on CLS token...
  3%|▎         | 673/25000 [18:18<10:29:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:39:59,099 >> Initializing global attention on CLS token...
  3%|▎         | 674/25000 [18:20<10:29:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:00,644 >> Initializing global attention on CLS token...
  3%|▎         | 675/25000 [18:21<10:31:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:02,214 >> Initializing global attention on CLS token...
  3%|▎         | 676/25000 [18:23<10:30:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:03,766 >> Initializing global attention on CLS token...
  3%|▎         | 677/25000 [18:25<10:29:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:05,310 >> Initializing global attention on CLS token...
  3%|▎         | 678/25000 [18:26<10:28:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:06,856 >> Initializing global attention on CLS token...
  3%|▎         | 679/25000 [18:28<10:27:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:08,400 >> Initializing global attention on CLS token...
  3%|▎         | 680/25000 [18:29<10:27:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:09,950 >> Initializing global attention on CLS token...
  3%|▎         | 681/25000 [18:31<10:27:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:11,499 >> Initializing global attention on CLS token...
  3%|▎         | 682/25000 [18:32<10:31:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:13,079 >> Initializing global attention on CLS token...
  3%|▎         | 683/25000 [18:34<10:29:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:14,622 >> Initializing global attention on CLS token...
  3%|▎         | 684/25000 [18:35<10:30:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:16,182 >> Initializing global attention on CLS token...
  3%|▎         | 685/25000 [18:37<10:29:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:17,733 >> Initializing global attention on CLS token...
  3%|▎         | 686/25000 [18:39<10:28:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:19,276 >> Initializing global attention on CLS token...
  3%|▎         | 687/25000 [18:40<10:28:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:20,823 >> Initializing global attention on CLS token...
  3%|▎         | 688/25000 [18:42<10:26:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:22,362 >> Initializing global attention on CLS token...
  3%|▎         | 689/25000 [18:43<10:25:51,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:23,901 >> Initializing global attention on CLS token...
  3%|▎         | 690/25000 [18:45<10:25:27,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:25,443 >> Initializing global attention on CLS token...
  3%|▎         | 691/25000 [18:46<10:25:52,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:26,990 >> Initializing global attention on CLS token...
  3%|▎         | 692/25000 [18:48<10:25:28,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:28,532 >> Initializing global attention on CLS token...
  3%|▎         | 693/25000 [18:49<10:25:08,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:30,076 >> Initializing global attention on CLS token...
  3%|▎         | 694/25000 [18:51<10:25:41,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:31,624 >> Initializing global attention on CLS token...
  3%|▎         | 695/25000 [18:52<10:25:46,  1.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:33,169 >> Initializing global attention on CLS token...
  3%|▎         | 696/25000 [18:54<10:26:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:34,719 >> Initializing global attention on CLS token...
  3%|▎         | 697/25000 [18:56<10:27:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:36,276 >> Initializing global attention on CLS token...
  3%|▎         | 698/25000 [18:57<10:27:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:37,825 >> Initializing global attention on CLS token...
  3%|▎         | 699/25000 [18:59<10:28:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:39,378 >> Initializing global attention on CLS token...
  3%|▎         | 700/25000 [19:00<10:28:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:40,931 >> Initializing global attention on CLS token...
  3%|▎         | 701/25000 [19:02<10:28:00,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:42,481 >> Initializing global attention on CLS token...
  3%|▎         | 702/25000 [19:03<10:27:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:44,027 >> Initializing global attention on CLS token...
  3%|▎         | 703/25000 [19:05<10:28:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:45,586 >> Initializing global attention on CLS token...
  3%|▎         | 704/25000 [19:06<10:28:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:47,142 >> Initializing global attention on CLS token...
  3%|▎         | 705/25000 [19:08<10:27:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:48,688 >> Initializing global attention on CLS token...
  3%|▎         | 706/25000 [19:10<10:29:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:50,268 >> Initializing global attention on CLS token...
  3%|▎         | 707/25000 [19:11<10:30:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:51,817 >> Initializing global attention on CLS token...
  3%|▎         | 708/25000 [19:13<10:29:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:53,361 >> Initializing global attention on CLS token...
  3%|▎         | 709/25000 [19:14<10:28:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:54,910 >> Initializing global attention on CLS token...
  3%|▎         | 710/25000 [19:16<10:29:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:56,470 >> Initializing global attention on CLS token...
  3%|▎         | 711/25000 [19:17<10:28:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:58,015 >> Initializing global attention on CLS token...
  3%|▎         | 712/25000 [19:19<10:27:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:40:59,560 >> Initializing global attention on CLS token...
  3%|▎         | 713/25000 [19:20<10:26:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:01,104 >> Initializing global attention on CLS token...
  3%|▎         | 714/25000 [19:22<10:28:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:02,671 >> Initializing global attention on CLS token...
  3%|▎         | 715/25000 [19:23<10:28:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:04,223 >> Initializing global attention on CLS token...
  3%|▎         | 716/25000 [19:25<10:28:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:05,773 >> Initializing global attention on CLS token...
  3%|▎         | 717/25000 [19:27<10:30:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:07,345 >> Initializing global attention on CLS token...
  3%|▎         | 718/25000 [19:28<10:29:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:08,890 >> Initializing global attention on CLS token...
  3%|▎         | 719/25000 [19:30<10:28:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:10,437 >> Initializing global attention on CLS token...
  3%|▎         | 720/25000 [19:31<10:27:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:11,983 >> Initializing global attention on CLS token...
  3%|▎         | 721/25000 [19:33<10:35:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:13,604 >> Initializing global attention on CLS token...
  3%|▎         | 722/25000 [19:34<10:32:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:15,149 >> Initializing global attention on CLS token...
  3%|▎         | 723/25000 [19:36<10:30:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:16,693 >> Initializing global attention on CLS token...
  3%|▎         | 724/25000 [19:38<10:29:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:18,241 >> Initializing global attention on CLS token...
  3%|▎         | 725/25000 [19:39<10:27:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:19,785 >> Initializing global attention on CLS token...
  3%|▎         | 726/25000 [19:41<10:26:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:21,329 >> Initializing global attention on CLS token...
  3%|▎         | 727/25000 [19:42<10:26:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:22,878 >> Initializing global attention on CLS token...
  3%|▎         | 728/25000 [19:44<10:27:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:24,437 >> Initializing global attention on CLS token...
  3%|▎         | 729/25000 [19:45<10:27:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:25,984 >> Initializing global attention on CLS token...
  3%|▎         | 730/25000 [19:47<10:26:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:27,528 >> Initializing global attention on CLS token...
  3%|▎         | 731/25000 [19:48<10:27:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:29,084 >> Initializing global attention on CLS token...
  3%|▎         | 732/25000 [19:50<10:26:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:30,631 >> Initializing global attention on CLS token...
  3%|▎         | 733/25000 [19:51<10:26:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:32,179 >> Initializing global attention on CLS token...
  3%|▎         | 734/25000 [19:53<10:25:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:33,722 >> Initializing global attention on CLS token...
  3%|▎         | 735/25000 [19:55<10:32:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:35,329 >> Initializing global attention on CLS token...
  3%|▎         | 736/25000 [19:56<10:30:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:36,873 >> Initializing global attention on CLS token...
  3%|▎         | 737/25000 [19:58<10:28:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:38,418 >> Initializing global attention on CLS token...
  3%|▎         | 738/25000 [19:59<10:30:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:39,996 >> Initializing global attention on CLS token...
  3%|▎         | 739/25000 [20:01<10:29:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:41,540 >> Initializing global attention on CLS token...
  3%|▎         | 740/25000 [20:02<10:28:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:43,086 >> Initializing global attention on CLS token...
  3%|▎         | 741/25000 [20:04<10:27:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:44,631 >> Initializing global attention on CLS token...
  3%|▎         | 742/25000 [20:06<10:35:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:46,251 >> Initializing global attention on CLS token...
  3%|▎         | 743/25000 [20:07<10:32:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:47,798 >> Initializing global attention on CLS token...
  3%|▎         | 744/25000 [20:09<10:31:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:49,353 >> Initializing global attention on CLS token...
  3%|▎         | 745/25000 [20:10<10:32:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:50,924 >> Initializing global attention on CLS token...
  3%|▎         | 746/25000 [20:12<10:30:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:52,470 >> Initializing global attention on CLS token...
  3%|▎         | 747/25000 [20:13<10:28:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:54,016 >> Initializing global attention on CLS token...
  3%|▎         | 748/25000 [20:15<10:27:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:55,562 >> Initializing global attention on CLS token...
  3%|▎         | 749/25000 [20:16<10:32:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:41:57,138 >> Initializing global attention on CLS token...
  3%|▎         | 750/25000 [20:18<10:29:00,  1.56s/it]                                                        3%|▎         | 750/25000 [20:18<10:29:00,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:41:58,673 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:41:58,677 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:41:58,677 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:41:58,677 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:58,690 >> Initializing global attention on CLS token...
{'eval_loss': 5.182898044586182, 'eval_accuracy': 0.22, 'eval_macro_f1': 0.04586109276842035, 'eval_macro_precision': 0.03669961667120758, 'eval_macro_recall': 0.08426339285714285, 'eval_micro_f1': 0.22, 'eval_micro_precision': 0.22, 'eval_micro_recall': 0.22, 'eval_combined_score': 0.14954630032811012, 'eval_runtime': 7.8182, 'eval_samples_per_second': 63.954, 'eval_steps_per_second': 8.058, 'epoch': 5.0}
{'loss': 3.7212, 'learning_rate': 6.79e-05, 'epoch': 6.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:58,809 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:58,929 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,048 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,168 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,288 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,408 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,531 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.89it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,651 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.77it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,771 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.64it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:41:59,892 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.57it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,012 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:05,  8.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,132 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,252 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,373 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,494 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,614 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,733 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,852 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:00,972 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,092 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,212 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  6.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,415 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:05,  7.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,534 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:05,  7.58it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,654 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  7.80it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,774 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:01,897 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  7.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,017 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,137 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,257 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,378 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,498 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,617 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,737 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,857 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:02,977 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,098 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,218 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,340 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,459 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,579 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,699 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,820 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:03,940 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,059 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  7.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,211 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  7.91it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,330 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,449 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,569 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,689 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,809 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:04,930 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,050 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,170 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,289 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,409 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,528 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,648 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,767 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:05,887 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:06,006 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:06,123 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:06,237 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  3%|▎         | 750/25000 [20:26<10:29:00,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.42it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:42:06,322 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-750
[INFO|configuration_utils.py:461] 2024-01-22 00:42:06,330 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-750/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:42:07,046 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:42:07,047 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:42:07,048 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-750/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:42:08,127 >> Initializing global attention on CLS token...
  3%|▎         | 751/25000 [20:29<29:40:22,  4.41s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:09,742 >> Initializing global attention on CLS token...
  3%|▎         | 752/25000 [20:31<23:52:54,  3.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:11,286 >> Initializing global attention on CLS token...
  3%|▎         | 753/25000 [20:32<19:51:27,  2.95s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:12,840 >> Initializing global attention on CLS token...
  3%|▎         | 754/25000 [20:34<17:02:51,  2.53s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:14,399 >> Initializing global attention on CLS token...
  3%|▎         | 755/25000 [20:35<15:03:44,  2.24s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:15,946 >> Initializing global attention on CLS token...
  3%|▎         | 756/25000 [20:37<13:39:18,  2.03s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:17,485 >> Initializing global attention on CLS token...
  3%|▎         | 757/25000 [20:38<12:40:38,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:19,028 >> Initializing global attention on CLS token...
  3%|▎         | 758/25000 [20:40<11:59:22,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:20,571 >> Initializing global attention on CLS token...
  3%|▎         | 759/25000 [20:41<11:30:51,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:22,119 >> Initializing global attention on CLS token...
  3%|▎         | 760/25000 [20:43<11:10:41,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:23,660 >> Initializing global attention on CLS token...
  3%|▎         | 761/25000 [20:44<10:56:32,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:25,207 >> Initializing global attention on CLS token...
  3%|▎         | 762/25000 [20:46<10:46:37,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:26,750 >> Initializing global attention on CLS token...
  3%|▎         | 763/25000 [20:48<10:39:54,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:28,295 >> Initializing global attention on CLS token...
  3%|▎         | 764/25000 [20:49<10:36:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:29,849 >> Initializing global attention on CLS token...
  3%|▎         | 765/25000 [20:51<10:32:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:31,393 >> Initializing global attention on CLS token...
  3%|▎         | 766/25000 [20:52<10:29:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:32,938 >> Initializing global attention on CLS token...
  3%|▎         | 767/25000 [20:54<10:27:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:34,481 >> Initializing global attention on CLS token...
  3%|▎         | 768/25000 [20:55<10:27:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:36,037 >> Initializing global attention on CLS token...
  3%|▎         | 769/25000 [20:57<10:26:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:37,581 >> Initializing global attention on CLS token...
  3%|▎         | 770/25000 [20:58<10:25:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:39,124 >> Initializing global attention on CLS token...
  3%|▎         | 771/25000 [21:00<10:24:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:40,668 >> Initializing global attention on CLS token...
  3%|▎         | 772/25000 [21:02<10:28:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:42,243 >> Initializing global attention on CLS token...
  3%|▎         | 773/25000 [21:03<10:27:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:43,789 >> Initializing global attention on CLS token...
  3%|▎         | 774/25000 [21:05<10:26:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:45,341 >> Initializing global attention on CLS token...
  3%|▎         | 775/25000 [21:06<10:33:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:46,950 >> Initializing global attention on CLS token...
  3%|▎         | 776/25000 [21:08<10:30:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:48,496 >> Initializing global attention on CLS token...
  3%|▎         | 777/25000 [21:09<10:28:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:50,040 >> Initializing global attention on CLS token...
  3%|▎         | 778/25000 [21:11<10:26:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:51,582 >> Initializing global attention on CLS token...
  3%|▎         | 779/25000 [21:12<10:35:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:53,209 >> Initializing global attention on CLS token...
  3%|▎         | 780/25000 [21:14<10:31:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:54,752 >> Initializing global attention on CLS token...
  3%|▎         | 781/25000 [21:16<10:29:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:56,300 >> Initializing global attention on CLS token...
  3%|▎         | 782/25000 [21:17<10:27:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:57,844 >> Initializing global attention on CLS token...
  3%|▎         | 783/25000 [21:19<10:29:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:42:59,415 >> Initializing global attention on CLS token...
  3%|▎         | 784/25000 [21:20<10:28:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:00,962 >> Initializing global attention on CLS token...
  3%|▎         | 785/25000 [21:22<10:26:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:02,506 >> Initializing global attention on CLS token...
  3%|▎         | 786/25000 [21:23<10:25:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:04,051 >> Initializing global attention on CLS token...
  3%|▎         | 787/25000 [21:25<10:38:17,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:05,707 >> Initializing global attention on CLS token...
  3%|▎         | 788/25000 [21:27<10:33:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:07,252 >> Initializing global attention on CLS token...
  3%|▎         | 789/25000 [21:28<10:31:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:08,799 >> Initializing global attention on CLS token...
  3%|▎         | 790/25000 [21:30<10:29:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:10,352 >> Initializing global attention on CLS token...
  3%|▎         | 791/25000 [21:31<10:27:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:11,896 >> Initializing global attention on CLS token...
  3%|▎         | 792/25000 [21:33<10:26:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:13,442 >> Initializing global attention on CLS token...
  3%|▎         | 793/25000 [21:34<10:25:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:14,985 >> Initializing global attention on CLS token...
  3%|▎         | 794/25000 [21:36<10:25:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:16,537 >> Initializing global attention on CLS token...
  3%|▎         | 795/25000 [21:37<10:24:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:18,082 >> Initializing global attention on CLS token...
  3%|▎         | 796/25000 [21:39<10:24:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:19,629 >> Initializing global attention on CLS token...
  3%|▎         | 797/25000 [21:40<10:24:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:21,175 >> Initializing global attention on CLS token...
  3%|▎         | 798/25000 [21:42<10:24:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:22,725 >> Initializing global attention on CLS token...
  3%|▎         | 799/25000 [21:44<10:24:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:24,270 >> Initializing global attention on CLS token...
  3%|▎         | 800/25000 [21:45<10:23:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:25,815 >> Initializing global attention on CLS token...
  3%|▎         | 801/25000 [21:47<10:23:32,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:27,359 >> Initializing global attention on CLS token...
  3%|▎         | 802/25000 [21:48<10:30:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:28,967 >> Initializing global attention on CLS token...
  3%|▎         | 803/25000 [21:50<10:28:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:30,512 >> Initializing global attention on CLS token...
  3%|▎         | 804/25000 [21:51<10:27:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:32,061 >> Initializing global attention on CLS token...
  3%|▎         | 805/25000 [21:53<10:28:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:33,627 >> Initializing global attention on CLS token...
  3%|▎         | 806/25000 [21:54<10:27:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:35,173 >> Initializing global attention on CLS token...
  3%|▎         | 807/25000 [21:56<10:27:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:36,731 >> Initializing global attention on CLS token...
  3%|▎         | 808/25000 [21:58<10:25:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:38,277 >> Initializing global attention on CLS token...
  3%|▎         | 809/25000 [21:59<10:31:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:39,874 >> Initializing global attention on CLS token...
  3%|▎         | 810/25000 [22:01<10:29:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:41,423 >> Initializing global attention on CLS token...
  3%|▎         | 811/25000 [22:02<10:32:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:43,008 >> Initializing global attention on CLS token...
  3%|▎         | 812/25000 [22:04<10:30:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:44,559 >> Initializing global attention on CLS token...
  3%|▎         | 813/25000 [22:05<10:28:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:46,105 >> Initializing global attention on CLS token...
  3%|▎         | 814/25000 [22:07<10:27:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:47,658 >> Initializing global attention on CLS token...
  3%|▎         | 815/25000 [22:08<10:25:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:49,204 >> Initializing global attention on CLS token...
  3%|▎         | 816/25000 [22:10<10:25:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:50,750 >> Initializing global attention on CLS token...
  3%|▎         | 817/25000 [22:12<10:24:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:52,297 >> Initializing global attention on CLS token...
  3%|▎         | 818/25000 [22:13<10:25:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:53,856 >> Initializing global attention on CLS token...
  3%|▎         | 819/25000 [22:15<10:24:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:55,400 >> Initializing global attention on CLS token...
  3%|▎         | 820/25000 [22:16<10:26:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:56,965 >> Initializing global attention on CLS token...
  3%|▎         | 821/25000 [22:18<10:25:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:43:58,511 >> Initializing global attention on CLS token...
  3%|▎         | 822/25000 [22:19<10:24:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:00,055 >> Initializing global attention on CLS token...
  3%|▎         | 823/25000 [22:21<10:24:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:01,602 >> Initializing global attention on CLS token...
  3%|▎         | 824/25000 [22:22<10:23:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:03,150 >> Initializing global attention on CLS token...
  3%|▎         | 825/25000 [22:24<10:26:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:04,717 >> Initializing global attention on CLS token...
  3%|▎         | 826/25000 [22:26<10:25:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:06,263 >> Initializing global attention on CLS token...
  3%|▎         | 827/25000 [22:27<10:26:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:07,829 >> Initializing global attention on CLS token...
  3%|▎         | 828/25000 [22:29<10:26:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:09,379 >> Initializing global attention on CLS token...
  3%|▎         | 829/25000 [22:30<10:25:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:10,925 >> Initializing global attention on CLS token...
  3%|▎         | 830/25000 [22:32<10:32:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:12,537 >> Initializing global attention on CLS token...
  3%|▎         | 831/25000 [22:33<10:30:53,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:14,146 >> Initializing global attention on CLS token...
  3%|▎         | 832/25000 [22:35<10:34:56,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:15,695 >> Initializing global attention on CLS token...
  3%|▎         | 833/25000 [22:37<10:31:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:17,240 >> Initializing global attention on CLS token...
  3%|▎         | 834/25000 [22:38<10:30:32,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:18,803 >> Initializing global attention on CLS token...
  3%|▎         | 835/25000 [22:40<10:28:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:20,347 >> Initializing global attention on CLS token...
  3%|▎         | 836/25000 [22:41<10:26:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:21,901 >> Initializing global attention on CLS token...
  3%|▎         | 837/25000 [22:43<10:26:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:23,454 >> Initializing global attention on CLS token...
  3%|▎         | 838/25000 [22:44<10:25:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:25,000 >> Initializing global attention on CLS token...
  3%|▎         | 839/25000 [22:46<10:24:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:26,548 >> Initializing global attention on CLS token...
  3%|▎         | 840/25000 [22:47<10:24:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:28,095 >> Initializing global attention on CLS token...
  3%|▎         | 841/25000 [22:49<10:27:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:29,676 >> Initializing global attention on CLS token...
  3%|▎         | 842/25000 [22:50<10:27:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:31,231 >> Initializing global attention on CLS token...
  3%|▎         | 843/25000 [22:52<10:26:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:32,786 >> Initializing global attention on CLS token...
  3%|▎         | 844/25000 [22:54<10:26:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:34,333 >> Initializing global attention on CLS token...
  3%|▎         | 845/25000 [22:55<10:25:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:35,882 >> Initializing global attention on CLS token...
  3%|▎         | 846/25000 [22:57<10:24:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:37,433 >> Initializing global attention on CLS token...
  3%|▎         | 847/25000 [22:58<10:24:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:38,980 >> Initializing global attention on CLS token...
  3%|▎         | 848/25000 [23:00<10:27:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:40,561 >> Initializing global attention on CLS token...
  3%|▎         | 849/25000 [23:01<10:25:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:42,104 >> Initializing global attention on CLS token...
  3%|▎         | 850/25000 [23:03<10:25:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:43,653 >> Initializing global attention on CLS token...
  3%|▎         | 851/25000 [23:05<10:30:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:45,246 >> Initializing global attention on CLS token...
  3%|▎         | 852/25000 [23:06<10:27:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:46,793 >> Initializing global attention on CLS token...
  3%|▎         | 853/25000 [23:08<10:27:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:48,354 >> Initializing global attention on CLS token...
  3%|▎         | 854/25000 [23:09<10:26:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:49,903 >> Initializing global attention on CLS token...
  3%|▎         | 855/25000 [23:11<10:26:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:51,463 >> Initializing global attention on CLS token...
  3%|▎         | 856/25000 [23:12<10:26:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:53,017 >> Initializing global attention on CLS token...
  3%|▎         | 857/25000 [23:14<10:25:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:54,563 >> Initializing global attention on CLS token...
  3%|▎         | 858/25000 [23:15<10:26:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:56,144 >> Initializing global attention on CLS token...
  3%|▎         | 859/25000 [23:17<10:30:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:57,724 >> Initializing global attention on CLS token...
  3%|▎         | 860/25000 [23:19<10:28:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:44:59,273 >> Initializing global attention on CLS token...
  3%|▎         | 861/25000 [23:20<10:29:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:00,837 >> Initializing global attention on CLS token...
  3%|▎         | 862/25000 [23:22<10:29:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:02,405 >> Initializing global attention on CLS token...
  3%|▎         | 863/25000 [23:23<10:27:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:03,955 >> Initializing global attention on CLS token...
  3%|▎         | 864/25000 [23:25<10:26:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:05,507 >> Initializing global attention on CLS token...
  3%|▎         | 865/25000 [23:26<10:35:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:07,133 >> Initializing global attention on CLS token...
  3%|▎         | 866/25000 [23:28<10:31:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:08,687 >> Initializing global attention on CLS token...
  3%|▎         | 867/25000 [23:29<10:28:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:10,235 >> Initializing global attention on CLS token...
  3%|▎         | 868/25000 [23:31<10:27:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:11,787 >> Initializing global attention on CLS token...
  3%|▎         | 869/25000 [23:33<10:26:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:13,340 >> Initializing global attention on CLS token...
  3%|▎         | 870/25000 [23:34<10:25:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:14,895 >> Initializing global attention on CLS token...
  3%|▎         | 871/25000 [23:36<10:26:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:16,448 >> Initializing global attention on CLS token...
  3%|▎         | 872/25000 [23:37<10:26:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:18,007 >> Initializing global attention on CLS token...
  3%|▎         | 873/25000 [23:39<10:28:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:19,585 >> Initializing global attention on CLS token...
  3%|▎         | 874/25000 [23:40<10:27:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:21,121 >> Initializing global attention on CLS token...
  4%|▎         | 875/25000 [23:42<10:24:26,  1.55s/it]                                                        4%|▎         | 875/25000 [23:42<10:24:26,  1.55s/it][INFO|trainer.py:738] 2024-01-22 00:45:22,654 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:45:22,657 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:45:22,657 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:45:22,658 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:22,689 >> Initializing global attention on CLS token...
{'eval_loss': 5.063812732696533, 'eval_accuracy': 0.22, 'eval_macro_f1': 0.04546510452750877, 'eval_macro_precision': 0.03604240645662287, 'eval_macro_recall': 0.0840316573556797, 'eval_micro_f1': 0.22, 'eval_micro_precision': 0.22, 'eval_micro_recall': 0.22, 'eval_combined_score': 0.14936273833425878, 'eval_runtime': 7.6436, 'eval_samples_per_second': 65.414, 'eval_steps_per_second': 8.242, 'epoch': 6.0}
{'loss': 3.4031, 'learning_rate': 6.754999999999999e-05, 'epoch': 7.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:22,822 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.53it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:22,946 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,068 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,191 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,311 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,434 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,555 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,686 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,807 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:23,926 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,045 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:07,  7.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,257 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  7.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,377 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:06,  7.58it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,496 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:06,  7.80it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,616 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  7.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,743 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:02<00:05,  7.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,862 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:24,981 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,101 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,222 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,350 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,470 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,590 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,710 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:03<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,829 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:25,949 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,071 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,198 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,317 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,437 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,559 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,679 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,802 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:26,924 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,044 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,168 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,287 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,408 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,530 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,649 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,769 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:27,891 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,010 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,132 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,254 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,375 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,496 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,625 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,755 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,875 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:28,996 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,118 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,237 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,366 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,486 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,605 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,725 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:29,845 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  6.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:30,094 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  6.79it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:30,213 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  7.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:30,330 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  7.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:30,448 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  4%|▎         | 875/25000 [23:50<10:24:26,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  7.50it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:45:30,533 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-875
[INFO|configuration_utils.py:461] 2024-01-22 00:45:30,540 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-875/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:45:31,103 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-875/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:45:31,105 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-875/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:45:31,105 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-875/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:45:32,196 >> Initializing global attention on CLS token...
  4%|▎         | 876/25000 [23:53<29:32:22,  4.41s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:33,745 >> Initializing global attention on CLS token...
  4%|▎         | 877/25000 [23:55<23:46:52,  3.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:35,289 >> Initializing global attention on CLS token...
  4%|▎         | 878/25000 [23:56<19:53:21,  2.97s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:36,903 >> Initializing global attention on CLS token...
  4%|▎         | 879/25000 [23:58<17:02:15,  2.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:38,453 >> Initializing global attention on CLS token...
  4%|▎         | 880/25000 [23:59<15:03:37,  2.25s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:40,012 >> Initializing global attention on CLS token...
  4%|▎         | 881/25000 [24:01<13:46:46,  2.06s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:41,620 >> Initializing global attention on CLS token...
  4%|▎         | 882/25000 [24:02<12:44:28,  1.90s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:43,164 >> Initializing global attention on CLS token...
  4%|▎         | 883/25000 [24:04<12:01:29,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:44,709 >> Initializing global attention on CLS token...
  4%|▎         | 884/25000 [24:06<11:31:15,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:46,254 >> Initializing global attention on CLS token...
  4%|▎         | 885/25000 [24:07<11:25:52,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:47,929 >> Initializing global attention on CLS token...
  4%|▎         | 886/25000 [24:09<11:06:23,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:49,474 >> Initializing global attention on CLS token...
  4%|▎         | 887/25000 [24:10<10:53:14,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:51,030 >> Initializing global attention on CLS token...
  4%|▎         | 888/25000 [24:12<10:46:22,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:52,591 >> Initializing global attention on CLS token...
  4%|▎         | 889/25000 [24:13<10:48:33,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:54,219 >> Initializing global attention on CLS token...
  4%|▎         | 890/25000 [24:15<10:41:03,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:55,776 >> Initializing global attention on CLS token...
  4%|▎         | 891/25000 [24:17<10:36:14,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:57,327 >> Initializing global attention on CLS token...
  4%|▎         | 892/25000 [24:18<10:31:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:45:58,874 >> Initializing global attention on CLS token...
  4%|▎         | 893/25000 [24:20<10:35:23,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:00,473 >> Initializing global attention on CLS token...
  4%|▎         | 894/25000 [24:21<10:30:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:02,019 >> Initializing global attention on CLS token...
  4%|▎         | 895/25000 [24:23<10:28:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:03,570 >> Initializing global attention on CLS token...
  4%|▎         | 896/25000 [24:24<10:27:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:05,122 >> Initializing global attention on CLS token...
  4%|▎         | 897/25000 [24:26<10:25:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:06,667 >> Initializing global attention on CLS token...
  4%|▎         | 898/25000 [24:27<10:23:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:08,213 >> Initializing global attention on CLS token...
  4%|▎         | 899/25000 [24:29<10:22:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:09,759 >> Initializing global attention on CLS token...
  4%|▎         | 900/25000 [24:31<10:31:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:11,379 >> Initializing global attention on CLS token...
  4%|▎         | 901/25000 [24:32<10:27:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:12,921 >> Initializing global attention on CLS token...
  4%|▎         | 902/25000 [24:34<10:25:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:14,471 >> Initializing global attention on CLS token...
  4%|▎         | 903/25000 [24:35<10:25:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:16,022 >> Initializing global attention on CLS token...
  4%|▎         | 904/25000 [24:37<10:31:11,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:17,643 >> Initializing global attention on CLS token...
  4%|▎         | 905/25000 [24:38<10:30:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:19,193 >> Initializing global attention on CLS token...
  4%|▎         | 906/25000 [24:40<10:27:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:20,740 >> Initializing global attention on CLS token...
  4%|▎         | 907/25000 [24:42<10:28:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:22,320 >> Initializing global attention on CLS token...
  4%|▎         | 908/25000 [24:43<10:28:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:23,873 >> Initializing global attention on CLS token...
  4%|▎         | 909/25000 [24:45<10:26:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:25,421 >> Initializing global attention on CLS token...
  4%|▎         | 910/25000 [24:46<10:25:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:26,977 >> Initializing global attention on CLS token...
  4%|▎         | 911/25000 [24:48<10:24:57,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:28,529 >> Initializing global attention on CLS token...
  4%|▎         | 912/25000 [24:49<10:23:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:30,074 >> Initializing global attention on CLS token...
  4%|▎         | 913/25000 [24:51<10:22:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:31,616 >> Initializing global attention on CLS token...
  4%|▎         | 914/25000 [24:52<10:21:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:33,161 >> Initializing global attention on CLS token...
  4%|▎         | 915/25000 [24:54<10:29:36,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:34,777 >> Initializing global attention on CLS token...
  4%|▎         | 916/25000 [24:56<10:27:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:36,325 >> Initializing global attention on CLS token...
  4%|▎         | 917/25000 [24:57<10:25:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:37,874 >> Initializing global attention on CLS token...
  4%|▎         | 918/25000 [24:59<10:26:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:39,440 >> Initializing global attention on CLS token...
  4%|▎         | 919/25000 [25:00<10:24:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:40,988 >> Initializing global attention on CLS token...
  4%|▎         | 920/25000 [25:02<10:24:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:42,538 >> Initializing global attention on CLS token...
  4%|▎         | 921/25000 [25:03<10:22:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:44,083 >> Initializing global attention on CLS token...
  4%|▎         | 922/25000 [25:05<10:29:57,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:45,694 >> Initializing global attention on CLS token...
  4%|▎         | 923/25000 [25:07<10:27:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:47,254 >> Initializing global attention on CLS token...
  4%|▎         | 924/25000 [25:08<10:26:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:48,802 >> Initializing global attention on CLS token...
  4%|▎         | 925/25000 [25:10<10:24:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:50,349 >> Initializing global attention on CLS token...
  4%|▎         | 926/25000 [25:11<10:23:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:51,896 >> Initializing global attention on CLS token...
  4%|▎         | 927/25000 [25:13<10:22:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:53,440 >> Initializing global attention on CLS token...
  4%|▎         | 928/25000 [25:14<10:21:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:54,986 >> Initializing global attention on CLS token...
  4%|▎         | 929/25000 [25:16<10:27:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:56,580 >> Initializing global attention on CLS token...
  4%|▎         | 930/25000 [25:17<10:24:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:58,125 >> Initializing global attention on CLS token...
  4%|▎         | 931/25000 [25:19<10:25:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:46:59,691 >> Initializing global attention on CLS token...
  4%|▎         | 932/25000 [25:21<10:28:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:01,277 >> Initializing global attention on CLS token...
  4%|▎         | 933/25000 [25:22<10:26:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:02,823 >> Initializing global attention on CLS token...
  4%|▎         | 934/25000 [25:24<10:24:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:04,370 >> Initializing global attention on CLS token...
  4%|▎         | 935/25000 [25:25<10:22:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:05,913 >> Initializing global attention on CLS token...
  4%|▎         | 936/25000 [25:27<10:29:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:07,522 >> Initializing global attention on CLS token...
  4%|▎         | 937/25000 [25:28<10:26:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:09,068 >> Initializing global attention on CLS token...
  4%|▍         | 938/25000 [25:30<10:24:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:10,613 >> Initializing global attention on CLS token...
  4%|▍         | 939/25000 [25:31<10:23:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:12,171 >> Initializing global attention on CLS token...
  4%|▍         | 940/25000 [25:33<10:24:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:13,721 >> Initializing global attention on CLS token...
  4%|▍         | 941/25000 [25:35<10:22:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:15,261 >> Initializing global attention on CLS token...
  4%|▍         | 942/25000 [25:36<10:22:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:16,827 >> Initializing global attention on CLS token...
  4%|▍         | 943/25000 [25:38<10:23:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:18,378 >> Initializing global attention on CLS token...
  4%|▍         | 944/25000 [25:39<10:22:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:19,922 >> Initializing global attention on CLS token...
  4%|▍         | 945/25000 [25:41<10:21:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:21,473 >> Initializing global attention on CLS token...
  4%|▍         | 946/25000 [25:42<10:22:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:23,025 >> Initializing global attention on CLS token...
  4%|▍         | 947/25000 [25:44<10:22:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:24,581 >> Initializing global attention on CLS token...
  4%|▍         | 948/25000 [25:45<10:21:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:26,126 >> Initializing global attention on CLS token...
  4%|▍         | 949/25000 [25:47<10:21:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:27,676 >> Initializing global attention on CLS token...
  4%|▍         | 950/25000 [25:48<10:22:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:29,233 >> Initializing global attention on CLS token...
  4%|▍         | 951/25000 [25:50<10:21:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:30,782 >> Initializing global attention on CLS token...
  4%|▍         | 952/25000 [25:52<10:21:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:32,328 >> Initializing global attention on CLS token...
  4%|▍         | 953/25000 [25:53<10:21:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:33,877 >> Initializing global attention on CLS token...
  4%|▍         | 954/25000 [25:55<10:20:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:35,421 >> Initializing global attention on CLS token...
  4%|▍         | 955/25000 [25:56<10:20:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:36,967 >> Initializing global attention on CLS token...
  4%|▍         | 956/25000 [25:58<10:19:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:38,514 >> Initializing global attention on CLS token...
  4%|▍         | 957/25000 [25:59<10:20:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:40,071 >> Initializing global attention on CLS token...
  4%|▍         | 958/25000 [26:01<10:21:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:41,626 >> Initializing global attention on CLS token...
  4%|▍         | 959/25000 [26:02<10:22:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:43,184 >> Initializing global attention on CLS token...
  4%|▍         | 960/25000 [26:04<10:22:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:44,735 >> Initializing global attention on CLS token...
  4%|▍         | 961/25000 [26:06<10:25:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:46,311 >> Initializing global attention on CLS token...
  4%|▍         | 962/25000 [26:07<10:23:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:47,858 >> Initializing global attention on CLS token...
  4%|▍         | 963/25000 [26:09<10:21:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:49,403 >> Initializing global attention on CLS token...
  4%|▍         | 964/25000 [26:10<10:21:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:50,954 >> Initializing global attention on CLS token...
  4%|▍         | 965/25000 [26:12<10:28:39,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:52,564 >> Initializing global attention on CLS token...
  4%|▍         | 966/25000 [26:13<10:26:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:54,111 >> Initializing global attention on CLS token...
  4%|▍         | 967/25000 [26:15<10:23:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:55,656 >> Initializing global attention on CLS token...
  4%|▍         | 968/25000 [26:16<10:26:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:57,233 >> Initializing global attention on CLS token...
  4%|▍         | 969/25000 [26:18<10:24:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:47:58,786 >> Initializing global attention on CLS token...
  4%|▍         | 970/25000 [26:20<10:23:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:00,336 >> Initializing global attention on CLS token...
  4%|▍         | 971/25000 [26:21<10:23:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:01,890 >> Initializing global attention on CLS token...
  4%|▍         | 972/25000 [26:23<10:28:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:03,492 >> Initializing global attention on CLS token...
  4%|▍         | 973/25000 [26:24<10:25:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:05,042 >> Initializing global attention on CLS token...
  4%|▍         | 974/25000 [26:26<10:24:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:06,592 >> Initializing global attention on CLS token...
  4%|▍         | 975/25000 [26:27<10:32:03,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:08,222 >> Initializing global attention on CLS token...
  4%|▍         | 976/25000 [26:29<10:29:35,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:09,771 >> Initializing global attention on CLS token...
  4%|▍         | 977/25000 [26:31<10:26:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:11,315 >> Initializing global attention on CLS token...
  4%|▍         | 978/25000 [26:32<10:25:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:12,876 >> Initializing global attention on CLS token...
  4%|▍         | 979/25000 [26:34<10:28:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:14,466 >> Initializing global attention on CLS token...
  4%|▍         | 980/25000 [26:35<10:25:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:16,010 >> Initializing global attention on CLS token...
  4%|▍         | 981/25000 [26:37<10:23:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:17,555 >> Initializing global attention on CLS token...
  4%|▍         | 982/25000 [26:38<10:35:04,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:19,207 >> Initializing global attention on CLS token...
  4%|▍         | 983/25000 [26:40<10:29:48,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:20,752 >> Initializing global attention on CLS token...
  4%|▍         | 984/25000 [26:42<10:27:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:22,312 >> Initializing global attention on CLS token...
  4%|▍         | 985/25000 [26:43<10:25:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:23,859 >> Initializing global attention on CLS token...
  4%|▍         | 986/25000 [26:45<10:26:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:25,432 >> Initializing global attention on CLS token...
  4%|▍         | 987/25000 [26:46<10:24:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:26,979 >> Initializing global attention on CLS token...
  4%|▍         | 988/25000 [26:48<10:22:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:28,524 >> Initializing global attention on CLS token...
  4%|▍         | 989/25000 [26:49<10:22:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:30,166 >> Initializing global attention on CLS token...
  4%|▍         | 990/25000 [26:51<10:34:50,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:31,736 >> Initializing global attention on CLS token...
  4%|▍         | 991/25000 [26:53<10:29:45,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:33,280 >> Initializing global attention on CLS token...
  4%|▍         | 992/25000 [26:54<10:26:19,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:34,826 >> Initializing global attention on CLS token...
  4%|▍         | 993/25000 [26:56<10:28:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:36,409 >> Initializing global attention on CLS token...
  4%|▍         | 994/25000 [26:57<10:25:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:37,955 >> Initializing global attention on CLS token...
  4%|▍         | 995/25000 [26:59<10:24:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:39,508 >> Initializing global attention on CLS token...
  4%|▍         | 996/25000 [27:00<10:22:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:41,197 >> Initializing global attention on CLS token...
  4%|▍         | 997/25000 [27:02<10:38:07,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:42,739 >> Initializing global attention on CLS token...
  4%|▍         | 998/25000 [27:04<10:32:10,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:44,285 >> Initializing global attention on CLS token...
  4%|▍         | 999/25000 [27:05<10:27:51,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:45,810 >> Initializing global attention on CLS token...
  4%|▍         | 1000/25000 [27:07<10:28:34,  1.57s/it]                                                         4%|▍         | 1000/25000 [27:07<10:28:34,  1.57s/it][INFO|trainer.py:738] 2024-01-22 00:48:47,385 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:48:47,389 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:48:47,389 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:48:47,389 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:47,402 >> Initializing global attention on CLS token...
{'eval_loss': 5.048659324645996, 'eval_accuracy': 0.246, 'eval_macro_f1': 0.059219314343063635, 'eval_macro_precision': 0.05316043129404011, 'eval_macro_recall': 0.09759281122917485, 'eval_micro_f1': 0.246, 'eval_micro_precision': 0.246, 'eval_micro_recall': 0.246, 'eval_combined_score': 0.1705675081237541, 'eval_runtime': 7.8731, 'eval_samples_per_second': 63.507, 'eval_steps_per_second': 8.002, 'epoch': 7.0}
{'loss': 3.1056, 'learning_rate': 6.72e-05, 'epoch': 8.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:47,522 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:47,647 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:47,767 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:47,886 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,005 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,125 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,244 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.64it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,385 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,505 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,624 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,744 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,874 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:48,994 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,116 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,245 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,365 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,485 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,607 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,733 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,855 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:49,976 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,101 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,221 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,339 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,461 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,580 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,701 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,823 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:50,941 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,063 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,182 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,303 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,422 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,544 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,674 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,794 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:51,912 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  7.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,063 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:03,  7.89it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,182 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  7.83it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,313 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,432 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.02it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,555 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,675 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,794 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.08it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:52,923 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,042 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,162 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,281 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,403 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,521 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,641 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,766 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:53,886 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,006 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,130 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,248 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,366 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,494 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,612 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,730 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,851 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:54,964 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  4%|▍         | 1000/25000 [27:14<10:28:34,  1.57s/it]
100%|██████████| 63/63 [00:07<00:00,  8.39it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:48:55,095 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1000
[INFO|configuration_utils.py:461] 2024-01-22 00:48:55,104 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1000/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:48:55,721 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:48:55,722 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:48:55,723 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1000/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:48:56,841 >> Initializing global attention on CLS token...
  4%|▍         | 1001/25000 [27:18<29:27:04,  4.42s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:48:58,465 >> Initializing global attention on CLS token...
  4%|▍         | 1002/25000 [27:19<23:42:10,  3.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:00,009 >> Initializing global attention on CLS token...
  4%|▍         | 1003/25000 [27:21<19:40:57,  2.95s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:01,555 >> Initializing global attention on CLS token...
  4%|▍         | 1004/25000 [27:22<16:51:54,  2.53s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:03,099 >> Initializing global attention on CLS token...
  4%|▍         | 1005/25000 [27:24<14:53:21,  2.23s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:04,642 >> Initializing global attention on CLS token...
  4%|▍         | 1006/25000 [27:25<13:30:30,  2.03s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:06,185 >> Initializing global attention on CLS token...
  4%|▍         | 1007/25000 [27:27<12:33:03,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:07,733 >> Initializing global attention on CLS token...
  4%|▍         | 1008/25000 [27:29<11:55:06,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:09,300 >> Initializing global attention on CLS token...
  4%|▍         | 1009/25000 [27:30<11:26:08,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:10,847 >> Initializing global attention on CLS token...
  4%|▍         | 1010/25000 [27:32<11:06:17,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:12,398 >> Initializing global attention on CLS token...
  4%|▍         | 1011/25000 [27:33<10:57:42,  1.65s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:13,994 >> Initializing global attention on CLS token...
  4%|▍         | 1012/25000 [27:35<10:46:16,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:15,543 >> Initializing global attention on CLS token...
  4%|▍         | 1013/25000 [27:36<10:38:09,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:17,092 >> Initializing global attention on CLS token...
  4%|▍         | 1014/25000 [27:38<10:32:22,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:18,640 >> Initializing global attention on CLS token...
  4%|▍         | 1015/25000 [27:39<10:29:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:20,197 >> Initializing global attention on CLS token...
  4%|▍         | 1016/25000 [27:41<10:26:26,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:21,747 >> Initializing global attention on CLS token...
  4%|▍         | 1017/25000 [27:43<10:24:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:23,297 >> Initializing global attention on CLS token...
  4%|▍         | 1018/25000 [27:44<10:23:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:24,855 >> Initializing global attention on CLS token...
  4%|▍         | 1019/25000 [27:46<10:21:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:26,401 >> Initializing global attention on CLS token...
  4%|▍         | 1020/25000 [27:47<10:20:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:27,945 >> Initializing global attention on CLS token...
  4%|▍         | 1021/25000 [27:49<10:19:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:29,488 >> Initializing global attention on CLS token...
  4%|▍         | 1022/25000 [27:50<10:19:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:31,038 >> Initializing global attention on CLS token...
  4%|▍         | 1023/25000 [27:52<10:18:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:32,584 >> Initializing global attention on CLS token...
  4%|▍         | 1024/25000 [27:53<10:18:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:34,127 >> Initializing global attention on CLS token...
  4%|▍         | 1025/25000 [27:55<10:20:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:35,693 >> Initializing global attention on CLS token...
  4%|▍         | 1026/25000 [27:57<10:19:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:37,243 >> Initializing global attention on CLS token...
  4%|▍         | 1027/25000 [27:58<10:18:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:38,785 >> Initializing global attention on CLS token...
  4%|▍         | 1028/25000 [28:00<10:18:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:40,331 >> Initializing global attention on CLS token...
  4%|▍         | 1029/25000 [28:01<10:19:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:41,887 >> Initializing global attention on CLS token...
  4%|▍         | 1030/25000 [28:03<10:18:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:43,430 >> Initializing global attention on CLS token...
  4%|▍         | 1031/25000 [28:04<10:18:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:44,975 >> Initializing global attention on CLS token...
  4%|▍         | 1032/25000 [28:06<10:18:08,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:46,523 >> Initializing global attention on CLS token...
  4%|▍         | 1033/25000 [28:07<10:22:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:48,105 >> Initializing global attention on CLS token...
  4%|▍         | 1034/25000 [28:09<10:20:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:49,649 >> Initializing global attention on CLS token...
  4%|▍         | 1035/25000 [28:10<10:20:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:51,199 >> Initializing global attention on CLS token...
  4%|▍         | 1036/25000 [28:12<10:25:04,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:52,792 >> Initializing global attention on CLS token...
  4%|▍         | 1037/25000 [28:14<10:22:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:54,340 >> Initializing global attention on CLS token...
  4%|▍         | 1038/25000 [28:15<10:21:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:55,889 >> Initializing global attention on CLS token...
  4%|▍         | 1039/25000 [28:17<10:21:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:57,444 >> Initializing global attention on CLS token...
  4%|▍         | 1040/25000 [28:18<10:23:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:49:59,020 >> Initializing global attention on CLS token...
  4%|▍         | 1041/25000 [28:20<10:22:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:00,574 >> Initializing global attention on CLS token...
  4%|▍         | 1042/25000 [28:21<10:22:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:02,135 >> Initializing global attention on CLS token...
  4%|▍         | 1043/25000 [28:23<10:21:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:03,680 >> Initializing global attention on CLS token...
  4%|▍         | 1044/25000 [28:25<10:26:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:05,282 >> Initializing global attention on CLS token...
  4%|▍         | 1045/25000 [28:26<10:24:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:06,832 >> Initializing global attention on CLS token...
  4%|▍         | 1046/25000 [28:28<10:22:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:08,377 >> Initializing global attention on CLS token...
  4%|▍         | 1047/25000 [28:29<10:21:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:09,927 >> Initializing global attention on CLS token...
  4%|▍         | 1048/25000 [28:31<10:20:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:11,482 >> Initializing global attention on CLS token...
  4%|▍         | 1049/25000 [28:32<10:20:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:13,029 >> Initializing global attention on CLS token...
  4%|▍         | 1050/25000 [28:34<10:18:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:14,574 >> Initializing global attention on CLS token...
  4%|▍         | 1051/25000 [28:35<10:18:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:16,123 >> Initializing global attention on CLS token...
  4%|▍         | 1052/25000 [28:37<10:18:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:17,668 >> Initializing global attention on CLS token...
  4%|▍         | 1053/25000 [28:38<10:18:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:19,218 >> Initializing global attention on CLS token...
  4%|▍         | 1054/25000 [28:40<10:18:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:20,775 >> Initializing global attention on CLS token...
  4%|▍         | 1055/25000 [28:42<10:19:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:22,328 >> Initializing global attention on CLS token...
  4%|▍         | 1056/25000 [28:43<10:18:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:23,874 >> Initializing global attention on CLS token...
  4%|▍         | 1057/25000 [28:45<10:20:28,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:25,440 >> Initializing global attention on CLS token...
  4%|▍         | 1058/25000 [28:46<10:20:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:26,991 >> Initializing global attention on CLS token...
  4%|▍         | 1059/25000 [28:48<10:19:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:28,543 >> Initializing global attention on CLS token...
  4%|▍         | 1060/25000 [28:49<10:19:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:30,094 >> Initializing global attention on CLS token...
  4%|▍         | 1061/25000 [28:51<10:19:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:31,646 >> Initializing global attention on CLS token...
  4%|▍         | 1062/25000 [28:52<10:20:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:33,207 >> Initializing global attention on CLS token...
  4%|▍         | 1063/25000 [28:54<10:19:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:34,752 >> Initializing global attention on CLS token...
  4%|▍         | 1064/25000 [28:56<10:19:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:36,306 >> Initializing global attention on CLS token...
  4%|▍         | 1065/25000 [28:57<10:21:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:37,893 >> Initializing global attention on CLS token...
  4%|▍         | 1066/25000 [28:59<10:22:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:39,442 >> Initializing global attention on CLS token...
  4%|▍         | 1067/25000 [29:00<10:20:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:40,990 >> Initializing global attention on CLS token...
  4%|▍         | 1068/25000 [29:02<10:19:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:42,536 >> Initializing global attention on CLS token...
  4%|▍         | 1069/25000 [29:03<10:19:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:44,089 >> Initializing global attention on CLS token...
  4%|▍         | 1070/25000 [29:05<10:18:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:45,634 >> Initializing global attention on CLS token...
  4%|▍         | 1071/25000 [29:06<10:19:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:47,196 >> Initializing global attention on CLS token...
  4%|▍         | 1072/25000 [29:08<10:22:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:48,774 >> Initializing global attention on CLS token...
  4%|▍         | 1073/25000 [29:10<10:21:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:50,331 >> Initializing global attention on CLS token...
  4%|▍         | 1074/25000 [29:11<10:20:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:51,880 >> Initializing global attention on CLS token...
  4%|▍         | 1075/25000 [29:13<10:19:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:53,427 >> Initializing global attention on CLS token...
  4%|▍         | 1076/25000 [29:14<10:20:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:54,986 >> Initializing global attention on CLS token...
  4%|▍         | 1077/25000 [29:16<10:19:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:56,547 >> Initializing global attention on CLS token...
  4%|▍         | 1078/25000 [29:17<10:20:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:58,099 >> Initializing global attention on CLS token...
  4%|▍         | 1079/25000 [29:19<10:22:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:50:59,665 >> Initializing global attention on CLS token...
  4%|▍         | 1080/25000 [29:20<10:19:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:01,206 >> Initializing global attention on CLS token...
  4%|▍         | 1081/25000 [29:22<10:17:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:02,745 >> Initializing global attention on CLS token...
  4%|▍         | 1082/25000 [29:24<10:17:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:04,293 >> Initializing global attention on CLS token...
  4%|▍         | 1083/25000 [29:25<10:17:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:05,842 >> Initializing global attention on CLS token...
  4%|▍         | 1084/25000 [29:27<10:16:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:07,388 >> Initializing global attention on CLS token...
  4%|▍         | 1085/25000 [29:28<10:17:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:08,941 >> Initializing global attention on CLS token...
  4%|▍         | 1086/25000 [29:30<10:33:08,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:10,618 >> Initializing global attention on CLS token...
  4%|▍         | 1087/25000 [29:31<10:28:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:12,169 >> Initializing global attention on CLS token...
  4%|▍         | 1088/25000 [29:33<10:24:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:13,714 >> Initializing global attention on CLS token...
  4%|▍         | 1089/25000 [29:35<10:22:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:15,265 >> Initializing global attention on CLS token...
  4%|▍         | 1090/25000 [29:36<10:22:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:16,831 >> Initializing global attention on CLS token...
  4%|▍         | 1091/25000 [29:38<10:20:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:18,374 >> Initializing global attention on CLS token...
  4%|▍         | 1092/25000 [29:39<10:19:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:19,923 >> Initializing global attention on CLS token...
  4%|▍         | 1093/25000 [29:41<10:21:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:21,495 >> Initializing global attention on CLS token...
  4%|▍         | 1094/25000 [29:42<10:20:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:23,043 >> Initializing global attention on CLS token...
  4%|▍         | 1095/25000 [29:44<10:18:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:24,588 >> Initializing global attention on CLS token...
  4%|▍         | 1096/25000 [29:45<10:18:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:26,141 >> Initializing global attention on CLS token...
  4%|▍         | 1097/25000 [29:47<10:18:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:27,695 >> Initializing global attention on CLS token...
  4%|▍         | 1098/25000 [29:49<10:18:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:29,243 >> Initializing global attention on CLS token...
  4%|▍         | 1099/25000 [29:50<10:17:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:30,788 >> Initializing global attention on CLS token...
  4%|▍         | 1100/25000 [29:52<10:16:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:32,333 >> Initializing global attention on CLS token...
  4%|▍         | 1101/25000 [29:53<10:17:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:33,886 >> Initializing global attention on CLS token...
  4%|▍         | 1102/25000 [29:55<10:16:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:35,431 >> Initializing global attention on CLS token...
  4%|▍         | 1103/25000 [29:56<10:17:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:36,983 >> Initializing global attention on CLS token...
  4%|▍         | 1104/25000 [29:58<10:16:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:38,529 >> Initializing global attention on CLS token...
  4%|▍         | 1105/25000 [29:59<10:19:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:40,104 >> Initializing global attention on CLS token...
  4%|▍         | 1106/25000 [30:01<10:19:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:41,654 >> Initializing global attention on CLS token...
  4%|▍         | 1107/25000 [30:02<10:17:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:43,198 >> Initializing global attention on CLS token...
  4%|▍         | 1108/25000 [30:04<10:18:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:44,752 >> Initializing global attention on CLS token...
  4%|▍         | 1109/25000 [30:06<10:17:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:46,297 >> Initializing global attention on CLS token...
  4%|▍         | 1110/25000 [30:07<10:16:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:47,843 >> Initializing global attention on CLS token...
  4%|▍         | 1111/25000 [30:09<10:17:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:49,397 >> Initializing global attention on CLS token...
  4%|▍         | 1112/25000 [30:10<10:19:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:50,961 >> Initializing global attention on CLS token...
  4%|▍         | 1113/25000 [30:12<10:19:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:52,520 >> Initializing global attention on CLS token...
  4%|▍         | 1114/25000 [30:13<10:18:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:54,069 >> Initializing global attention on CLS token...
  4%|▍         | 1115/25000 [30:15<10:19:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:55,629 >> Initializing global attention on CLS token...
  4%|▍         | 1116/25000 [30:16<10:19:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:57,189 >> Initializing global attention on CLS token...
  4%|▍         | 1117/25000 [30:18<10:19:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:51:58,745 >> Initializing global attention on CLS token...
  4%|▍         | 1118/25000 [30:20<10:18:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:00,294 >> Initializing global attention on CLS token...
  4%|▍         | 1119/25000 [30:21<10:17:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:01,842 >> Initializing global attention on CLS token...
  4%|▍         | 1120/25000 [30:23<10:18:22,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:03,399 >> Initializing global attention on CLS token...
  4%|▍         | 1121/25000 [30:24<10:18:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:04,951 >> Initializing global attention on CLS token...
  4%|▍         | 1122/25000 [30:26<10:17:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:06,497 >> Initializing global attention on CLS token...
  4%|▍         | 1123/25000 [30:27<10:17:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:08,047 >> Initializing global attention on CLS token...
  4%|▍         | 1124/25000 [30:29<10:16:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:09,575 >> Initializing global attention on CLS token...
  4%|▍         | 1125/25000 [30:30<10:13:45,  1.54s/it]                                                         4%|▍         | 1125/25000 [30:30<10:13:45,  1.54s/it][INFO|trainer.py:738] 2024-01-22 00:52:11,099 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:52:11,102 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:52:11,102 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:52:11,102 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,114 >> Initializing global attention on CLS token...
{'eval_loss': 5.0568318367004395, 'eval_accuracy': 0.244, 'eval_macro_f1': 0.061208591796212485, 'eval_macro_precision': 0.05195457238396017, 'eval_macro_recall': 0.10120408545727136, 'eval_micro_f1': 0.244, 'eval_micro_precision': 0.244, 'eval_micro_recall': 0.244, 'eval_combined_score': 0.17005246423392056, 'eval_runtime': 7.655, 'eval_samples_per_second': 65.317, 'eval_steps_per_second': 8.23, 'epoch': 8.0}
{'loss': 2.8184, 'learning_rate': 6.684999999999999e-05, 'epoch': 9.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,254 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.75it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,374 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,508 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05,  9.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,630 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,755 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  8.98it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:11,881 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.79it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,001 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,120 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.54it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,243 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,366 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  7.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,537 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  7.64it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,660 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  7.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,783 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:06,  7.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:12,903 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,025 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,147 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:02<00:05,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,269 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,387 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,509 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.05it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,639 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,760 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:13,882 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,001 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,120 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,252 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,370 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,490 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,608 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,737 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,860 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:14,978 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,098 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,217 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,339 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,463 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,586 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,705 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,824 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:15,944 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,064 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,187 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,310 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,429 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,552 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,671 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,792 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:16,912 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,032 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,151 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,270 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,389 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,511 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,631 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,752 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:17,872 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  7.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,038 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  7.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,159 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  7.85it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,279 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,398 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,518 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,637 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:18,751 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  4%|▍         | 1125/25000 [30:38<10:13:45,  1.54s/it]
100%|██████████| 63/63 [00:07<00:00,  8.26it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:52:18,835 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1125
[INFO|configuration_utils.py:461] 2024-01-22 00:52:18,842 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1125/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:52:19,419 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1125/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:52:19,420 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1125/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:52:19,420 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1125/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:52:20,507 >> Initializing global attention on CLS token...
  5%|▍         | 1126/25000 [30:41<28:54:36,  4.36s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:22,052 >> Initializing global attention on CLS token...
  5%|▍         | 1127/25000 [30:43<23:19:25,  3.52s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:23,601 >> Initializing global attention on CLS token...
  5%|▍         | 1128/25000 [30:44<19:22:54,  2.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:25,137 >> Initializing global attention on CLS token...
  5%|▍         | 1129/25000 [30:46<16:38:33,  2.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:26,687 >> Initializing global attention on CLS token...
  5%|▍         | 1130/25000 [30:47<14:43:14,  2.22s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:28,231 >> Initializing global attention on CLS token...
  5%|▍         | 1131/25000 [30:49<13:23:27,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:29,783 >> Initializing global attention on CLS token...
  5%|▍         | 1132/25000 [30:51<12:27:33,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:31,333 >> Initializing global attention on CLS token...
  5%|▍         | 1133/25000 [30:52<11:47:22,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:32,879 >> Initializing global attention on CLS token...
  5%|▍         | 1134/25000 [30:54<11:20:17,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:34,428 >> Initializing global attention on CLS token...
  5%|▍         | 1135/25000 [30:55<11:06:36,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:36,024 >> Initializing global attention on CLS token...
  5%|▍         | 1136/25000 [30:57<10:51:22,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:37,573 >> Initializing global attention on CLS token...
  5%|▍         | 1137/25000 [30:58<10:40:49,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:39,122 >> Initializing global attention on CLS token...
  5%|▍         | 1138/25000 [31:00<10:34:38,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:40,694 >> Initializing global attention on CLS token...
  5%|▍         | 1139/25000 [31:02<10:31:17,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:42,247 >> Initializing global attention on CLS token...
  5%|▍         | 1140/25000 [31:03<10:25:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:43,801 >> Initializing global attention on CLS token...
  5%|▍         | 1141/25000 [31:05<10:23:54,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:45,348 >> Initializing global attention on CLS token...
  5%|▍         | 1142/25000 [31:06<10:25:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:46,933 >> Initializing global attention on CLS token...
  5%|▍         | 1143/25000 [31:08<10:23:08,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:48,486 >> Initializing global attention on CLS token...
  5%|▍         | 1144/25000 [31:09<10:20:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:50,031 >> Initializing global attention on CLS token...
  5%|▍         | 1145/25000 [31:11<10:18:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:51,578 >> Initializing global attention on CLS token...
  5%|▍         | 1146/25000 [31:12<10:19:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:53,144 >> Initializing global attention on CLS token...
  5%|▍         | 1147/25000 [31:14<10:18:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:54,691 >> Initializing global attention on CLS token...
  5%|▍         | 1148/25000 [31:15<10:17:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:56,235 >> Initializing global attention on CLS token...
  5%|▍         | 1149/25000 [31:17<10:28:38,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:57,880 >> Initializing global attention on CLS token...
  5%|▍         | 1150/25000 [31:19<10:24:08,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:52:59,430 >> Initializing global attention on CLS token...
  5%|▍         | 1151/25000 [31:20<10:21:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:00,990 >> Initializing global attention on CLS token...
  5%|▍         | 1152/25000 [31:22<10:20:56,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:02,539 >> Initializing global attention on CLS token...
  5%|▍         | 1153/25000 [31:23<10:20:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:04,102 >> Initializing global attention on CLS token...
  5%|▍         | 1154/25000 [31:25<10:20:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:05,661 >> Initializing global attention on CLS token...
  5%|▍         | 1155/25000 [31:26<10:19:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:07,213 >> Initializing global attention on CLS token...
  5%|▍         | 1156/25000 [31:28<10:18:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:08,765 >> Initializing global attention on CLS token...
  5%|▍         | 1157/25000 [31:30<10:21:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:10,343 >> Initializing global attention on CLS token...
  5%|▍         | 1158/25000 [31:31<10:19:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:11,891 >> Initializing global attention on CLS token...
  5%|▍         | 1159/25000 [31:33<10:18:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:13,442 >> Initializing global attention on CLS token...
  5%|▍         | 1160/25000 [31:34<10:17:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:14,993 >> Initializing global attention on CLS token...
  5%|▍         | 1161/25000 [31:36<10:26:07,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:16,618 >> Initializing global attention on CLS token...
  5%|▍         | 1162/25000 [31:37<10:22:52,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:18,169 >> Initializing global attention on CLS token...
  5%|▍         | 1163/25000 [31:39<10:20:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:19,715 >> Initializing global attention on CLS token...
  5%|▍         | 1164/25000 [31:41<10:24:38,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:21,312 >> Initializing global attention on CLS token...
  5%|▍         | 1165/25000 [31:42<10:22:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:22,865 >> Initializing global attention on CLS token...
  5%|▍         | 1166/25000 [31:44<10:23:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:24,456 >> Initializing global attention on CLS token...
  5%|▍         | 1167/25000 [31:45<10:22:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:26,004 >> Initializing global attention on CLS token...
  5%|▍         | 1168/25000 [31:47<10:20:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:27,555 >> Initializing global attention on CLS token...
  5%|▍         | 1169/25000 [31:48<10:18:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:29,103 >> Initializing global attention on CLS token...
  5%|▍         | 1170/25000 [31:50<10:19:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:30,665 >> Initializing global attention on CLS token...
  5%|▍         | 1171/25000 [31:51<10:17:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:32,209 >> Initializing global attention on CLS token...
  5%|▍         | 1172/25000 [31:53<10:22:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:33,803 >> Initializing global attention on CLS token...
  5%|▍         | 1173/25000 [31:55<10:19:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:35,347 >> Initializing global attention on CLS token...
  5%|▍         | 1174/25000 [31:56<10:18:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:36,896 >> Initializing global attention on CLS token...
  5%|▍         | 1175/25000 [31:58<10:24:24,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:38,506 >> Initializing global attention on CLS token...
  5%|▍         | 1176/25000 [31:59<10:22:31,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:40,061 >> Initializing global attention on CLS token...
  5%|▍         | 1177/25000 [32:01<10:20:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:41,611 >> Initializing global attention on CLS token...
  5%|▍         | 1178/25000 [32:02<10:18:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:43,158 >> Initializing global attention on CLS token...
  5%|▍         | 1179/25000 [32:04<10:17:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:44,710 >> Initializing global attention on CLS token...
  5%|▍         | 1180/25000 [32:06<10:17:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:46,262 >> Initializing global attention on CLS token...
  5%|▍         | 1181/25000 [32:07<10:16:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:47,807 >> Initializing global attention on CLS token...
  5%|▍         | 1182/25000 [32:09<10:15:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:49,354 >> Initializing global attention on CLS token...
  5%|▍         | 1183/25000 [32:10<10:17:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:50,921 >> Initializing global attention on CLS token...
  5%|▍         | 1184/25000 [32:12<10:15:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:52,465 >> Initializing global attention on CLS token...
  5%|▍         | 1185/25000 [32:13<10:15:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:54,013 >> Initializing global attention on CLS token...
  5%|▍         | 1186/25000 [32:15<10:16:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:55,576 >> Initializing global attention on CLS token...
  5%|▍         | 1187/25000 [32:16<10:16:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:57,124 >> Initializing global attention on CLS token...
  5%|▍         | 1188/25000 [32:18<10:16:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:53:58,679 >> Initializing global attention on CLS token...
  5%|▍         | 1189/25000 [32:19<10:15:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:00,228 >> Initializing global attention on CLS token...
  5%|▍         | 1190/25000 [32:21<10:18:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:01,805 >> Initializing global attention on CLS token...
  5%|▍         | 1191/25000 [32:23<10:17:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:03,351 >> Initializing global attention on CLS token...
  5%|▍         | 1192/25000 [32:24<10:15:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:04,896 >> Initializing global attention on CLS token...
  5%|▍         | 1193/25000 [32:26<10:15:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:06,444 >> Initializing global attention on CLS token...
  5%|▍         | 1194/25000 [32:27<10:17:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:08,013 >> Initializing global attention on CLS token...
  5%|▍         | 1195/25000 [32:29<10:16:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:09,561 >> Initializing global attention on CLS token...
  5%|▍         | 1196/25000 [32:30<10:15:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:11,106 >> Initializing global attention on CLS token...
  5%|▍         | 1197/25000 [32:32<10:22:10,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:12,710 >> Initializing global attention on CLS token...
  5%|▍         | 1198/25000 [32:34<10:18:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:14,253 >> Initializing global attention on CLS token...
  5%|▍         | 1199/25000 [32:35<10:16:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:15,795 >> Initializing global attention on CLS token...
  5%|▍         | 1200/25000 [32:37<10:15:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:17,344 >> Initializing global attention on CLS token...
  5%|▍         | 1201/25000 [32:38<10:15:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:18,895 >> Initializing global attention on CLS token...
  5%|▍         | 1202/25000 [32:40<10:14:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:20,441 >> Initializing global attention on CLS token...
  5%|▍         | 1203/25000 [32:41<10:14:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:21,985 >> Initializing global attention on CLS token...
  5%|▍         | 1204/25000 [32:43<10:17:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:23,557 >> Initializing global attention on CLS token...
  5%|▍         | 1205/25000 [32:44<10:15:26,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:25,101 >> Initializing global attention on CLS token...
  5%|▍         | 1206/25000 [32:46<10:14:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:26,647 >> Initializing global attention on CLS token...
  5%|▍         | 1207/25000 [32:47<10:14:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:28,199 >> Initializing global attention on CLS token...
  5%|▍         | 1208/25000 [32:49<10:15:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:29,761 >> Initializing global attention on CLS token...
  5%|▍         | 1209/25000 [32:51<10:14:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:31,304 >> Initializing global attention on CLS token...
  5%|▍         | 1210/25000 [32:52<10:14:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:32,849 >> Initializing global attention on CLS token...
  5%|▍         | 1211/25000 [32:54<10:14:56,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:34,404 >> Initializing global attention on CLS token...
  5%|▍         | 1212/25000 [32:55<10:14:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:35,948 >> Initializing global attention on CLS token...
  5%|▍         | 1213/25000 [32:57<10:14:32,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:37,501 >> Initializing global attention on CLS token...
  5%|▍         | 1214/25000 [32:58<10:13:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:39,046 >> Initializing global attention on CLS token...
  5%|▍         | 1215/25000 [33:00<10:15:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:40,611 >> Initializing global attention on CLS token...
  5%|▍         | 1216/25000 [33:01<10:14:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:42,159 >> Initializing global attention on CLS token...
  5%|▍         | 1217/25000 [33:03<10:14:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:43,712 >> Initializing global attention on CLS token...
  5%|▍         | 1218/25000 [33:05<10:15:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:45,263 >> Initializing global attention on CLS token...
  5%|▍         | 1219/25000 [33:06<10:14:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:46,805 >> Initializing global attention on CLS token...
  5%|▍         | 1220/25000 [33:08<10:16:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:48,371 >> Initializing global attention on CLS token...
  5%|▍         | 1221/25000 [33:09<10:15:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:49,919 >> Initializing global attention on CLS token...
  5%|▍         | 1222/25000 [33:11<10:15:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:51,473 >> Initializing global attention on CLS token...
  5%|▍         | 1223/25000 [33:12<10:15:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:53,024 >> Initializing global attention on CLS token...
  5%|▍         | 1224/25000 [33:14<10:14:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:54,570 >> Initializing global attention on CLS token...
  5%|▍         | 1225/25000 [33:15<10:15:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:56,127 >> Initializing global attention on CLS token...
  5%|▍         | 1226/25000 [33:17<10:14:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:57,676 >> Initializing global attention on CLS token...
  5%|▍         | 1227/25000 [33:18<10:14:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:54:59,223 >> Initializing global attention on CLS token...
  5%|▍         | 1228/25000 [33:20<10:13:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:00,769 >> Initializing global attention on CLS token...
  5%|▍         | 1229/25000 [33:22<10:14:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:02,325 >> Initializing global attention on CLS token...
  5%|▍         | 1230/25000 [33:23<10:15:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:03,882 >> Initializing global attention on CLS token...
  5%|▍         | 1231/25000 [33:25<10:15:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:05,439 >> Initializing global attention on CLS token...
  5%|▍         | 1232/25000 [33:26<10:15:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:06,988 >> Initializing global attention on CLS token...
  5%|▍         | 1233/25000 [33:28<10:14:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:08,535 >> Initializing global attention on CLS token...
  5%|▍         | 1234/25000 [33:29<10:14:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:10,090 >> Initializing global attention on CLS token...
  5%|▍         | 1235/25000 [33:31<10:14:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:11,638 >> Initializing global attention on CLS token...
  5%|▍         | 1236/25000 [33:32<10:13:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:13,186 >> Initializing global attention on CLS token...
  5%|▍         | 1237/25000 [33:34<10:13:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:14,730 >> Initializing global attention on CLS token...
  5%|▍         | 1238/25000 [33:36<10:12:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:16,275 >> Initializing global attention on CLS token...
  5%|▍         | 1239/25000 [33:37<10:12:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:17,824 >> Initializing global attention on CLS token...
  5%|▍         | 1240/25000 [33:39<10:12:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:19,372 >> Initializing global attention on CLS token...
  5%|▍         | 1241/25000 [33:40<10:12:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:20,918 >> Initializing global attention on CLS token...
  5%|▍         | 1242/25000 [33:42<10:12:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:22,467 >> Initializing global attention on CLS token...
  5%|▍         | 1243/25000 [33:43<10:16:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:24,048 >> Initializing global attention on CLS token...
  5%|▍         | 1244/25000 [33:45<10:15:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:25,594 >> Initializing global attention on CLS token...
  5%|▍         | 1245/25000 [33:46<10:15:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:27,150 >> Initializing global attention on CLS token...
  5%|▍         | 1246/25000 [33:48<10:18:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:28,730 >> Initializing global attention on CLS token...
  5%|▍         | 1247/25000 [33:50<10:17:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:30,282 >> Initializing global attention on CLS token...
  5%|▍         | 1248/25000 [33:51<10:15:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:31,829 >> Initializing global attention on CLS token...
  5%|▍         | 1249/25000 [33:53<10:14:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:33,356 >> Initializing global attention on CLS token...
  5%|▌         | 1250/25000 [33:54<10:18:48,  1.56s/it]                                                         5%|▌         | 1250/25000 [33:54<10:18:48,  1.56s/it][INFO|trainer.py:738] 2024-01-22 00:55:34,943 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:55:34,947 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:55:34,947 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:55:34,947 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:34,959 >> Initializing global attention on CLS token...
{'eval_loss': 5.067880630493164, 'eval_accuracy': 0.246, 'eval_macro_f1': 0.0659608853325303, 'eval_macro_precision': 0.05991064796009851, 'eval_macro_recall': 0.10261338620328767, 'eval_micro_f1': 0.246, 'eval_micro_precision': 0.246, 'eval_micro_recall': 0.246, 'eval_combined_score': 0.1732121313565595, 'eval_runtime': 7.7308, 'eval_samples_per_second': 64.676, 'eval_steps_per_second': 8.149, 'epoch': 9.0}
{'loss': 2.556, 'learning_rate': 6.649999999999999e-05, 'epoch': 10.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,079 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.81it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,248 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,367 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:06,  9.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,488 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,607 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  8.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,727 >> Initializing global attention on CLS token...

 11%|█         | 7/63 [00:00<00:06,  8.67it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,846 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.58it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:35,967 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:01<00:06,  8.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,086 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.48it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,204 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,324 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,444 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,564 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,683 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,803 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:36,922 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,041 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,161 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,281 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,401 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,523 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,643 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,762 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:37,880 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,003 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,122 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,241 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,360 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,482 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,601 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,720 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,841 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:38,960 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,079 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,211 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.12it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,334 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,453 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,572 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,691 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,810 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:39,929 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,048 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,168 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,290 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,408 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,528 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,652 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,774 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:40,894 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,016 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,139 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,258 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,378 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,496 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,617 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,736 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,855 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:41,974 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:42,094 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:42,214 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:42,331 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:42,445 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  5%|▌         | 1250/25000 [34:02<10:18:48,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.42it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:55:42,530 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1250
[INFO|configuration_utils.py:461] 2024-01-22 00:55:42,538 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1250/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:55:43,101 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:55:43,102 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:55:43,102 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1250/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:55:44,171 >> Initializing global attention on CLS token...
  5%|▌         | 1251/25000 [34:05<28:32:01,  4.33s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:45,733 >> Initializing global attention on CLS token...
  5%|▌         | 1252/25000 [34:07<23:01:36,  3.49s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:47,276 >> Initializing global attention on CLS token...
  5%|▌         | 1253/25000 [34:08<19:10:14,  2.91s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:48,818 >> Initializing global attention on CLS token...
  5%|▌         | 1254/25000 [34:10<16:29:19,  2.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:50,370 >> Initializing global attention on CLS token...
  5%|▌         | 1255/25000 [34:11<14:36:17,  2.21s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:51,918 >> Initializing global attention on CLS token...
  5%|▌         | 1256/25000 [34:13<13:17:12,  2.01s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:53,466 >> Initializing global attention on CLS token...
  5%|▌         | 1257/25000 [34:14<12:21:49,  1.87s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:55,015 >> Initializing global attention on CLS token...
  5%|▌         | 1258/25000 [34:16<11:42:53,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:56,562 >> Initializing global attention on CLS token...
  5%|▌         | 1259/25000 [34:17<11:15:11,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:58,105 >> Initializing global attention on CLS token...
  5%|▌         | 1260/25000 [34:19<10:56:05,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:55:59,650 >> Initializing global attention on CLS token...
  5%|▌         | 1261/25000 [34:20<10:42:35,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:01,195 >> Initializing global attention on CLS token...
  5%|▌         | 1262/25000 [34:22<10:36:09,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:02,766 >> Initializing global attention on CLS token...
  5%|▌         | 1263/25000 [34:24<10:29:31,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:04,318 >> Initializing global attention on CLS token...
  5%|▌         | 1264/25000 [34:25<10:24:21,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:05,867 >> Initializing global attention on CLS token...
  5%|▌         | 1265/25000 [34:27<10:21:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:07,419 >> Initializing global attention on CLS token...
  5%|▌         | 1266/25000 [34:28<10:42:24,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:09,167 >> Initializing global attention on CLS token...
  5%|▌         | 1267/25000 [34:30<10:33:13,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:10,714 >> Initializing global attention on CLS token...
  5%|▌         | 1268/25000 [34:32<10:26:32,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:12,259 >> Initializing global attention on CLS token...
  5%|▌         | 1269/25000 [34:33<10:22:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:13,812 >> Initializing global attention on CLS token...
  5%|▌         | 1270/25000 [34:35<10:19:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:15,366 >> Initializing global attention on CLS token...
  5%|▌         | 1271/25000 [34:36<10:18:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:16,917 >> Initializing global attention on CLS token...
  5%|▌         | 1272/25000 [34:38<10:29:41,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:18,583 >> Initializing global attention on CLS token...
  5%|▌         | 1273/25000 [34:39<10:24:39,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:20,125 >> Initializing global attention on CLS token...
  5%|▌         | 1274/25000 [34:41<10:20:40,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:21,671 >> Initializing global attention on CLS token...
  5%|▌         | 1275/25000 [34:42<10:17:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:23,219 >> Initializing global attention on CLS token...
  5%|▌         | 1276/25000 [34:44<10:18:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:24,789 >> Initializing global attention on CLS token...
  5%|▌         | 1277/25000 [34:46<10:16:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:26,334 >> Initializing global attention on CLS token...
  5%|▌         | 1278/25000 [34:47<10:14:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:27,879 >> Initializing global attention on CLS token...
  5%|▌         | 1279/25000 [34:49<10:13:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:29,420 >> Initializing global attention on CLS token...
  5%|▌         | 1280/25000 [34:50<10:13:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:30,972 >> Initializing global attention on CLS token...
  5%|▌         | 1281/25000 [34:52<10:13:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:32,526 >> Initializing global attention on CLS token...
  5%|▌         | 1282/25000 [34:53<10:13:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:34,075 >> Initializing global attention on CLS token...
  5%|▌         | 1283/25000 [34:55<10:16:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:35,651 >> Initializing global attention on CLS token...
  5%|▌         | 1284/25000 [34:56<10:17:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:37,220 >> Initializing global attention on CLS token...
  5%|▌         | 1285/25000 [34:58<10:14:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:38,760 >> Initializing global attention on CLS token...
  5%|▌         | 1286/25000 [35:00<10:14:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:40,316 >> Initializing global attention on CLS token...
  5%|▌         | 1287/25000 [35:01<10:15:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:41,879 >> Initializing global attention on CLS token...
  5%|▌         | 1288/25000 [35:03<10:15:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:43,439 >> Initializing global attention on CLS token...
  5%|▌         | 1289/25000 [35:04<10:13:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:44,982 >> Initializing global attention on CLS token...
  5%|▌         | 1290/25000 [35:06<10:13:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:46,545 >> Initializing global attention on CLS token...
  5%|▌         | 1291/25000 [35:07<10:14:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:48,089 >> Initializing global attention on CLS token...
  5%|▌         | 1292/25000 [35:09<10:13:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:49,635 >> Initializing global attention on CLS token...
  5%|▌         | 1293/25000 [35:11<10:19:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:51,232 >> Initializing global attention on CLS token...
  5%|▌         | 1294/25000 [35:12<10:16:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:52,779 >> Initializing global attention on CLS token...
  5%|▌         | 1295/25000 [35:14<10:14:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:54,322 >> Initializing global attention on CLS token...
  5%|▌         | 1296/25000 [35:15<10:12:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:55,862 >> Initializing global attention on CLS token...
  5%|▌         | 1297/25000 [35:17<10:11:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:57,416 >> Initializing global attention on CLS token...
  5%|▌         | 1298/25000 [35:18<10:12:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:56:58,961 >> Initializing global attention on CLS token...
  5%|▌         | 1299/25000 [35:20<10:11:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:00,506 >> Initializing global attention on CLS token...
  5%|▌         | 1300/25000 [35:21<10:12:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:02,056 >> Initializing global attention on CLS token...
  5%|▌         | 1301/25000 [35:23<10:10:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:03,599 >> Initializing global attention on CLS token...
  5%|▌         | 1302/25000 [35:24<10:10:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:05,143 >> Initializing global attention on CLS token...
  5%|▌         | 1303/25000 [35:26<10:10:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:06,689 >> Initializing global attention on CLS token...
  5%|▌         | 1304/25000 [35:28<10:12:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:08,248 >> Initializing global attention on CLS token...
  5%|▌         | 1305/25000 [35:29<10:11:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:09,794 >> Initializing global attention on CLS token...
  5%|▌         | 1306/25000 [35:31<10:11:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:11,338 >> Initializing global attention on CLS token...
  5%|▌         | 1307/25000 [35:32<10:12:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:12,900 >> Initializing global attention on CLS token...
  5%|▌         | 1308/25000 [35:34<10:12:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:14,450 >> Initializing global attention on CLS token...
  5%|▌         | 1309/25000 [35:35<10:12:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:16,002 >> Initializing global attention on CLS token...
  5%|▌         | 1310/25000 [35:37<10:11:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:17,562 >> Initializing global attention on CLS token...
  5%|▌         | 1311/25000 [35:38<10:13:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:19,108 >> Initializing global attention on CLS token...
  5%|▌         | 1312/25000 [35:40<10:12:07,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:20,654 >> Initializing global attention on CLS token...
  5%|▌         | 1313/25000 [35:41<10:11:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:22,200 >> Initializing global attention on CLS token...
  5%|▌         | 1314/25000 [35:43<10:13:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:23,766 >> Initializing global attention on CLS token...
  5%|▌         | 1315/25000 [35:45<10:13:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:25,316 >> Initializing global attention on CLS token...
  5%|▌         | 1316/25000 [35:46<10:12:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:26,862 >> Initializing global attention on CLS token...
  5%|▌         | 1317/25000 [35:48<10:11:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:28,419 >> Initializing global attention on CLS token...
  5%|▌         | 1318/25000 [35:49<10:16:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:29,998 >> Initializing global attention on CLS token...
  5%|▌         | 1319/25000 [35:51<10:14:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:31,542 >> Initializing global attention on CLS token...
  5%|▌         | 1320/25000 [35:52<10:12:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:33,088 >> Initializing global attention on CLS token...
  5%|▌         | 1321/25000 [35:54<10:14:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:34,656 >> Initializing global attention on CLS token...
  5%|▌         | 1322/25000 [35:55<10:14:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:36,210 >> Initializing global attention on CLS token...
  5%|▌         | 1323/25000 [35:57<10:14:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:37,764 >> Initializing global attention on CLS token...
  5%|▌         | 1324/25000 [35:59<10:12:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:39,311 >> Initializing global attention on CLS token...
  5%|▌         | 1325/25000 [36:00<10:15:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:40,889 >> Initializing global attention on CLS token...
  5%|▌         | 1326/25000 [36:02<10:13:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:42,435 >> Initializing global attention on CLS token...
  5%|▌         | 1327/25000 [36:03<10:12:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:43,978 >> Initializing global attention on CLS token...
  5%|▌         | 1328/25000 [36:05<10:12:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:45,529 >> Initializing global attention on CLS token...
  5%|▌         | 1329/25000 [36:06<10:11:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:47,078 >> Initializing global attention on CLS token...
  5%|▌         | 1330/25000 [36:08<10:11:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:48,632 >> Initializing global attention on CLS token...
  5%|▌         | 1331/25000 [36:09<10:13:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:50,203 >> Initializing global attention on CLS token...
  5%|▌         | 1332/25000 [36:11<10:18:12,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:51,791 >> Initializing global attention on CLS token...
  5%|▌         | 1333/25000 [36:13<10:15:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:53,335 >> Initializing global attention on CLS token...
  5%|▌         | 1334/25000 [36:14<10:14:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:54,889 >> Initializing global attention on CLS token...
  5%|▌         | 1335/25000 [36:16<10:17:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:56,473 >> Initializing global attention on CLS token...
  5%|▌         | 1336/25000 [36:17<10:15:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:58,025 >> Initializing global attention on CLS token...
  5%|▌         | 1337/25000 [36:19<10:13:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:57:59,570 >> Initializing global attention on CLS token...
  5%|▌         | 1338/25000 [36:20<10:12:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:01,113 >> Initializing global attention on CLS token...
  5%|▌         | 1339/25000 [36:22<10:15:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:02,690 >> Initializing global attention on CLS token...
  5%|▌         | 1340/25000 [36:24<10:14:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:04,242 >> Initializing global attention on CLS token...
  5%|▌         | 1341/25000 [36:25<10:12:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:05,786 >> Initializing global attention on CLS token...
  5%|▌         | 1342/25000 [36:27<10:12:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:07,341 >> Initializing global attention on CLS token...
  5%|▌         | 1343/25000 [36:28<10:12:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:08,891 >> Initializing global attention on CLS token...
  5%|▌         | 1344/25000 [36:30<10:12:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:10,449 >> Initializing global attention on CLS token...
  5%|▌         | 1345/25000 [36:31<10:12:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:12,004 >> Initializing global attention on CLS token...
  5%|▌         | 1346/25000 [36:33<10:14:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:13,573 >> Initializing global attention on CLS token...
  5%|▌         | 1347/25000 [36:34<10:15:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:15,138 >> Initializing global attention on CLS token...
  5%|▌         | 1348/25000 [36:36<10:13:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:16,687 >> Initializing global attention on CLS token...
  5%|▌         | 1349/25000 [36:38<10:20:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:18,299 >> Initializing global attention on CLS token...
  5%|▌         | 1350/25000 [36:39<10:16:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:19,843 >> Initializing global attention on CLS token...
  5%|▌         | 1351/25000 [36:41<10:14:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:21,386 >> Initializing global attention on CLS token...
  5%|▌         | 1352/25000 [36:42<10:20:29,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:22,994 >> Initializing global attention on CLS token...
  5%|▌         | 1353/25000 [36:44<10:17:46,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:24,550 >> Initializing global attention on CLS token...
  5%|▌         | 1354/25000 [36:45<10:15:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:26,096 >> Initializing global attention on CLS token...
  5%|▌         | 1355/25000 [36:47<10:13:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:27,642 >> Initializing global attention on CLS token...
  5%|▌         | 1356/25000 [36:48<10:12:25,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:29,189 >> Initializing global attention on CLS token...
  5%|▌         | 1357/25000 [36:50<10:11:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:30,733 >> Initializing global attention on CLS token...
  5%|▌         | 1358/25000 [36:52<10:10:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:32,280 >> Initializing global attention on CLS token...
  5%|▌         | 1359/25000 [36:53<10:10:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:33,831 >> Initializing global attention on CLS token...
  5%|▌         | 1360/25000 [36:55<10:10:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:35,377 >> Initializing global attention on CLS token...
  5%|▌         | 1361/25000 [36:56<10:09:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:36,922 >> Initializing global attention on CLS token...
  5%|▌         | 1362/25000 [36:58<10:09:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:38,471 >> Initializing global attention on CLS token...
  5%|▌         | 1363/25000 [36:59<10:09:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:40,018 >> Initializing global attention on CLS token...
  5%|▌         | 1364/25000 [37:01<10:09:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:41,565 >> Initializing global attention on CLS token...
  5%|▌         | 1365/25000 [37:02<10:09:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:43,112 >> Initializing global attention on CLS token...
  5%|▌         | 1366/25000 [37:04<10:09:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:44,660 >> Initializing global attention on CLS token...
  5%|▌         | 1367/25000 [37:05<10:10:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:46,215 >> Initializing global attention on CLS token...
  5%|▌         | 1368/25000 [37:07<10:10:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:47,763 >> Initializing global attention on CLS token...
  5%|▌         | 1369/25000 [37:09<10:10:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:49,315 >> Initializing global attention on CLS token...
  5%|▌         | 1370/25000 [37:10<10:13:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:50,893 >> Initializing global attention on CLS token...
  5%|▌         | 1371/25000 [37:12<10:12:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:52,440 >> Initializing global attention on CLS token...
  5%|▌         | 1372/25000 [37:13<10:11:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:53,984 >> Initializing global attention on CLS token...
  5%|▌         | 1373/25000 [37:15<10:10:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:55,529 >> Initializing global attention on CLS token...
  5%|▌         | 1374/25000 [37:16<10:09:52,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:58:57,056 >> Initializing global attention on CLS token...
  6%|▌         | 1375/25000 [37:18<10:07:19,  1.54s/it]                                                         6%|▌         | 1375/25000 [37:18<10:07:19,  1.54s/it][INFO|trainer.py:738] 2024-01-22 00:58:58,586 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 00:58:58,589 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 00:58:58,589 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 00:58:58,589 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:58,601 >> Initializing global attention on CLS token...
{'eval_loss': 5.030808448791504, 'eval_accuracy': 0.258, 'eval_macro_f1': 0.0750140395764499, 'eval_macro_precision': 0.0651918608877002, 'eval_macro_recall': 0.11209866901624681, 'eval_micro_f1': 0.258, 'eval_micro_precision': 0.258, 'eval_micro_recall': 0.258, 'eval_combined_score': 0.18347208135434243, 'eval_runtime': 7.5814, 'eval_samples_per_second': 65.951, 'eval_steps_per_second': 8.31, 'epoch': 10.0}
{'loss': 2.3055, 'learning_rate': 6.615e-05, 'epoch': 11.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:58,721 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.83it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:58,841 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:58,960 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.55it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,079 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,198 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,319 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,438 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.99it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,557 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,676 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.73it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,795 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:58:59,930 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,053 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,172 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,291 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,420 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,538 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,657 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,776 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:00,895 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,014 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,133 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,252 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,371 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,490 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,609 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,728 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,856 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:01,975 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,094 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,213 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,332 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,451 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,571 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,689 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,808 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:02,927 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,046 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:02,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,164 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,283 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,402 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,522 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,641 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,760 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.40it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,879 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:03,998 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,116 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,235 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,354 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,472 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,596 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:05<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,715 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,834 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:04,953 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,073 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,191 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.41it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,310 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,432 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,551 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:06<00:00,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,669 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  7.52it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,833 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  7.78it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:05,949 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.01it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:06,063 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  6%|▌         | 1375/25000 [37:25<10:07:19,  1.54s/it]
100%|██████████| 63/63 [00:07<00:00,  8.01it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 00:59:06,147 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1375
[INFO|configuration_utils.py:461] 2024-01-22 00:59:06,154 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1375/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 00:59:06,750 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1375/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 00:59:06,751 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1375/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 00:59:06,751 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1375/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 00:59:07,913 >> Initializing global attention on CLS token...
  6%|▌         | 1376/25000 [37:29<28:27:05,  4.34s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:09,456 >> Initializing global attention on CLS token...
  6%|▌         | 1377/25000 [37:30<22:57:19,  3.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:11,001 >> Initializing global attention on CLS token...
  6%|▌         | 1378/25000 [37:32<19:12:43,  2.93s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:12,598 >> Initializing global attention on CLS token...
  6%|▌         | 1379/25000 [37:33<16:29:56,  2.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:14,148 >> Initializing global attention on CLS token...
  6%|▌         | 1380/25000 [37:35<14:36:07,  2.23s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:15,699 >> Initializing global attention on CLS token...
  6%|▌         | 1381/25000 [37:37<13:17:07,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:17,256 >> Initializing global attention on CLS token...
  6%|▌         | 1382/25000 [37:38<12:21:30,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:18,810 >> Initializing global attention on CLS token...
  6%|▌         | 1383/25000 [37:40<11:41:19,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:20,354 >> Initializing global attention on CLS token...
  6%|▌         | 1384/25000 [37:41<11:13:14,  1.71s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:21,898 >> Initializing global attention on CLS token...
  6%|▌         | 1385/25000 [37:43<10:54:33,  1.66s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:23,451 >> Initializing global attention on CLS token...
  6%|▌         | 1386/25000 [37:44<10:40:36,  1.63s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:24,996 >> Initializing global attention on CLS token...
  6%|▌         | 1387/25000 [37:46<10:30:40,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:26,539 >> Initializing global attention on CLS token...
  6%|▌         | 1388/25000 [37:47<10:23:49,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:28,084 >> Initializing global attention on CLS token...
  6%|▌         | 1389/25000 [37:49<10:21:00,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:29,643 >> Initializing global attention on CLS token...
  6%|▌         | 1390/25000 [37:50<10:17:33,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:31,195 >> Initializing global attention on CLS token...
  6%|▌         | 1391/25000 [37:52<10:15:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:32,743 >> Initializing global attention on CLS token...
  6%|▌         | 1392/25000 [37:54<10:15:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:34,309 >> Initializing global attention on CLS token...
  6%|▌         | 1393/25000 [37:55<10:13:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:35,855 >> Initializing global attention on CLS token...
  6%|▌         | 1394/25000 [37:57<10:12:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:37,408 >> Initializing global attention on CLS token...
  6%|▌         | 1395/25000 [37:58<10:11:21,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:38,958 >> Initializing global attention on CLS token...
  6%|▌         | 1396/25000 [38:00<10:12:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:40,524 >> Initializing global attention on CLS token...
  6%|▌         | 1397/25000 [38:01<10:12:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:42,075 >> Initializing global attention on CLS token...
  6%|▌         | 1398/25000 [38:03<10:12:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:43,632 >> Initializing global attention on CLS token...
  6%|▌         | 1399/25000 [38:04<10:11:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:45,179 >> Initializing global attention on CLS token...
  6%|▌         | 1400/25000 [38:06<10:16:03,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:46,775 >> Initializing global attention on CLS token...
  6%|▌         | 1401/25000 [38:08<10:13:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:48,319 >> Initializing global attention on CLS token...
  6%|▌         | 1402/25000 [38:09<10:11:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:49,863 >> Initializing global attention on CLS token...
  6%|▌         | 1403/25000 [38:11<10:13:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:51,437 >> Initializing global attention on CLS token...
  6%|▌         | 1404/25000 [38:12<10:11:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:52,982 >> Initializing global attention on CLS token...
  6%|▌         | 1405/25000 [38:14<10:10:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:54,529 >> Initializing global attention on CLS token...
  6%|▌         | 1406/25000 [38:15<10:12:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:56,107 >> Initializing global attention on CLS token...
  6%|▌         | 1407/25000 [38:17<10:19:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:57,709 >> Initializing global attention on CLS token...
  6%|▌         | 1408/25000 [38:19<10:15:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 00:59:59,269 >> Initializing global attention on CLS token...
  6%|▌         | 1409/25000 [38:20<10:15:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:00,817 >> Initializing global attention on CLS token...
  6%|▌         | 1410/25000 [38:22<10:16:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:02,391 >> Initializing global attention on CLS token...
  6%|▌         | 1411/25000 [38:23<10:13:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:03,938 >> Initializing global attention on CLS token...
  6%|▌         | 1412/25000 [38:25<10:14:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:05,503 >> Initializing global attention on CLS token...
  6%|▌         | 1413/25000 [38:26<10:12:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:07,051 >> Initializing global attention on CLS token...
  6%|▌         | 1414/25000 [38:28<10:11:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:08,605 >> Initializing global attention on CLS token...
  6%|▌         | 1415/25000 [38:29<10:12:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:10,161 >> Initializing global attention on CLS token...
  6%|▌         | 1416/25000 [38:31<10:10:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:11,756 >> Initializing global attention on CLS token...
  6%|▌         | 1417/25000 [38:33<10:15:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:13,305 >> Initializing global attention on CLS token...
  6%|▌         | 1418/25000 [38:34<10:17:47,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:14,890 >> Initializing global attention on CLS token...
  6%|▌         | 1419/25000 [38:36<10:15:23,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:16,441 >> Initializing global attention on CLS token...
  6%|▌         | 1420/25000 [38:37<10:13:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:17,988 >> Initializing global attention on CLS token...
  6%|▌         | 1421/25000 [38:39<10:12:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:19,541 >> Initializing global attention on CLS token...
  6%|▌         | 1422/25000 [38:40<10:11:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:21,091 >> Initializing global attention on CLS token...
  6%|▌         | 1423/25000 [38:42<10:10:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:22,647 >> Initializing global attention on CLS token...
  6%|▌         | 1424/25000 [38:43<10:10:44,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:24,197 >> Initializing global attention on CLS token...
  6%|▌         | 1425/25000 [38:45<10:12:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:25,763 >> Initializing global attention on CLS token...
  6%|▌         | 1426/25000 [38:47<10:11:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:27,313 >> Initializing global attention on CLS token...
  6%|▌         | 1427/25000 [38:48<10:10:50,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:28,869 >> Initializing global attention on CLS token...
  6%|▌         | 1428/25000 [38:50<10:10:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:30,419 >> Initializing global attention on CLS token...
  6%|▌         | 1429/25000 [38:51<10:20:31,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:32,057 >> Initializing global attention on CLS token...
  6%|▌         | 1430/25000 [38:53<10:16:55,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:33,606 >> Initializing global attention on CLS token...
  6%|▌         | 1431/25000 [38:54<10:14:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:35,156 >> Initializing global attention on CLS token...
  6%|▌         | 1432/25000 [38:56<10:13:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:36,712 >> Initializing global attention on CLS token...
  6%|▌         | 1433/25000 [38:58<10:12:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:38,262 >> Initializing global attention on CLS token...
  6%|▌         | 1434/25000 [38:59<10:10:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:39,811 >> Initializing global attention on CLS token...
  6%|▌         | 1435/25000 [39:01<10:10:18,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:41,361 >> Initializing global attention on CLS token...
  6%|▌         | 1436/25000 [39:02<10:10:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:42,917 >> Initializing global attention on CLS token...
  6%|▌         | 1437/25000 [39:04<10:10:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:44,468 >> Initializing global attention on CLS token...
  6%|▌         | 1438/25000 [39:05<10:11:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:46,034 >> Initializing global attention on CLS token...
  6%|▌         | 1439/25000 [39:07<10:10:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:47,587 >> Initializing global attention on CLS token...
  6%|▌         | 1440/25000 [39:08<10:13:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:49,167 >> Initializing global attention on CLS token...
  6%|▌         | 1441/25000 [39:10<10:20:41,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:50,786 >> Initializing global attention on CLS token...
  6%|▌         | 1442/25000 [39:12<10:20:12,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:52,362 >> Initializing global attention on CLS token...
  6%|▌         | 1443/25000 [39:13<10:19:29,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:53,939 >> Initializing global attention on CLS token...
  6%|▌         | 1444/25000 [39:15<10:17:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:55,495 >> Initializing global attention on CLS token...
  6%|▌         | 1445/25000 [39:16<10:16:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:57,061 >> Initializing global attention on CLS token...
  6%|▌         | 1446/25000 [39:18<10:14:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:00:58,612 >> Initializing global attention on CLS token...
  6%|▌         | 1447/25000 [39:19<10:12:42,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:00,166 >> Initializing global attention on CLS token...
  6%|▌         | 1448/25000 [39:21<10:11:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:01,717 >> Initializing global attention on CLS token...
  6%|▌         | 1449/25000 [39:23<10:10:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:03,266 >> Initializing global attention on CLS token...
  6%|▌         | 1450/25000 [39:24<10:09:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:04,816 >> Initializing global attention on CLS token...
  6%|▌         | 1451/25000 [39:26<10:10:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:06,379 >> Initializing global attention on CLS token...
  6%|▌         | 1452/25000 [39:27<10:09:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:07,927 >> Initializing global attention on CLS token...
  6%|▌         | 1453/25000 [39:29<10:09:19,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:09,475 >> Initializing global attention on CLS token...
  6%|▌         | 1454/25000 [39:30<10:09:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:11,029 >> Initializing global attention on CLS token...
  6%|▌         | 1455/25000 [39:32<10:08:46,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:12,577 >> Initializing global attention on CLS token...
  6%|▌         | 1456/25000 [39:33<10:09:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:14,135 >> Initializing global attention on CLS token...
  6%|▌         | 1457/25000 [39:35<10:09:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:15,685 >> Initializing global attention on CLS token...
  6%|▌         | 1458/25000 [39:37<10:09:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:17,248 >> Initializing global attention on CLS token...
  6%|▌         | 1459/25000 [39:38<10:09:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:18,797 >> Initializing global attention on CLS token...
  6%|▌         | 1460/25000 [39:40<10:09:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:20,348 >> Initializing global attention on CLS token...
  6%|▌         | 1461/25000 [39:41<10:09:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:21,902 >> Initializing global attention on CLS token...
  6%|▌         | 1462/25000 [39:43<10:10:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:23,462 >> Initializing global attention on CLS token...
  6%|▌         | 1463/25000 [39:44<10:09:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:25,015 >> Initializing global attention on CLS token...
  6%|▌         | 1464/25000 [39:46<10:10:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:26,579 >> Initializing global attention on CLS token...
  6%|▌         | 1465/25000 [39:47<10:11:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:28,142 >> Initializing global attention on CLS token...
  6%|▌         | 1466/25000 [39:49<10:10:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:29,693 >> Initializing global attention on CLS token...
  6%|▌         | 1467/25000 [39:51<10:11:05,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:31,254 >> Initializing global attention on CLS token...
  6%|▌         | 1468/25000 [39:52<10:10:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:32,804 >> Initializing global attention on CLS token...
  6%|▌         | 1469/25000 [39:54<10:09:28,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:34,354 >> Initializing global attention on CLS token...
  6%|▌         | 1470/25000 [39:55<10:09:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:35,913 >> Initializing global attention on CLS token...
  6%|▌         | 1471/25000 [39:57<10:11:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:37,478 >> Initializing global attention on CLS token...
  6%|▌         | 1472/25000 [39:58<10:10:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:39,032 >> Initializing global attention on CLS token...
  6%|▌         | 1473/25000 [40:00<10:09:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:40,582 >> Initializing global attention on CLS token...
  6%|▌         | 1474/25000 [40:01<10:11:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:42,151 >> Initializing global attention on CLS token...
  6%|▌         | 1475/25000 [40:03<10:13:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:43,728 >> Initializing global attention on CLS token...
  6%|▌         | 1476/25000 [40:05<10:11:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:45,280 >> Initializing global attention on CLS token...
  6%|▌         | 1477/25000 [40:06<10:10:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:46,833 >> Initializing global attention on CLS token...
  6%|▌         | 1478/25000 [40:08<10:14:25,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:48,420 >> Initializing global attention on CLS token...
  6%|▌         | 1479/25000 [40:09<10:12:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:49,979 >> Initializing global attention on CLS token...
  6%|▌         | 1480/25000 [40:11<10:12:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:51,532 >> Initializing global attention on CLS token...
  6%|▌         | 1481/25000 [40:12<10:11:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:53,088 >> Initializing global attention on CLS token...
  6%|▌         | 1482/25000 [40:14<10:16:14,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:54,687 >> Initializing global attention on CLS token...
  6%|▌         | 1483/25000 [40:16<10:15:37,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:56,255 >> Initializing global attention on CLS token...
  6%|▌         | 1484/25000 [40:17<10:13:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:57,806 >> Initializing global attention on CLS token...
  6%|▌         | 1485/25000 [40:19<10:11:38,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:01:59,357 >> Initializing global attention on CLS token...
  6%|▌         | 1486/25000 [40:20<10:20:50,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:00,997 >> Initializing global attention on CLS token...
  6%|▌         | 1487/25000 [40:22<10:17:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:02,548 >> Initializing global attention on CLS token...
  6%|▌         | 1488/25000 [40:23<10:14:22,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:04,097 >> Initializing global attention on CLS token...
  6%|▌         | 1489/25000 [40:25<10:12:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:05,653 >> Initializing global attention on CLS token...
  6%|▌         | 1490/25000 [40:26<10:10:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:07,202 >> Initializing global attention on CLS token...
  6%|▌         | 1491/25000 [40:28<10:09:10,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:08,744 >> Initializing global attention on CLS token...
  6%|▌         | 1492/25000 [40:30<10:07:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:10,291 >> Initializing global attention on CLS token...
  6%|▌         | 1493/25000 [40:31<10:10:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:11,867 >> Initializing global attention on CLS token...
  6%|▌         | 1494/25000 [40:33<10:09:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:13,430 >> Initializing global attention on CLS token...
  6%|▌         | 1495/25000 [40:34<10:10:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:14,980 >> Initializing global attention on CLS token...
  6%|▌         | 1496/25000 [40:36<10:10:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:16,541 >> Initializing global attention on CLS token...
  6%|▌         | 1497/25000 [40:37<10:11:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:18,111 >> Initializing global attention on CLS token...
  6%|▌         | 1498/25000 [40:39<10:11:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:19,666 >> Initializing global attention on CLS token...
  6%|▌         | 1499/25000 [40:40<10:10:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:21,201 >> Initializing global attention on CLS token...
  6%|▌         | 1500/25000 [40:42<10:11:24,  1.56s/it]                                                         6%|▌         | 1500/25000 [40:42<10:11:24,  1.56s/it][INFO|trainer.py:738] 2024-01-22 01:02:22,771 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:02:22,773 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:02:22,774 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:02:22,774 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:22,786 >> Initializing global attention on CLS token...
{'eval_loss': 4.902622222900391, 'eval_accuracy': 0.266, 'eval_macro_f1': 0.08570205344663137, 'eval_macro_precision': 0.07662027471670162, 'eval_macro_recall': 0.12173846763620842, 'eval_micro_f1': 0.266, 'eval_micro_precision': 0.266, 'eval_micro_recall': 0.266, 'eval_combined_score': 0.1925801136856488, 'eval_runtime': 7.5563, 'eval_samples_per_second': 66.17, 'eval_steps_per_second': 8.337, 'epoch': 11.0}
{'loss': 2.0597, 'learning_rate': 6.579999999999999e-05, 'epoch': 12.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:22,907 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,028 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,147 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,268 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,387 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,508 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,627 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,747 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.68it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,875 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.57it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:23,996 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.51it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,116 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,236 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,357 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.38it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,477 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,599 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,728 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,849 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:24,971 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,092 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,213 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,339 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:05,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,471 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:05,  7.96it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,593 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,715 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,844 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.03it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:25,965 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,091 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,211 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,333 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,453 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,578 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  7.88it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,715 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  7.87it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,841 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  7.97it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:26,963 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.03it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,085 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,206 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,334 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,454 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.15it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,575 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,695 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,819 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  8.21it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:27,939 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,059 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  6.94it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,257 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  7.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,377 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  7.57it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,498 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:02,  7.77it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,618 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  7.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,739 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,859 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:06<00:01,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:28,978 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,099 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,218 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,338 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,458 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,578 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,698 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,819 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:07<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:29,940 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:30,062 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:30,182 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:30,300 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:30,415 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  6%|▌         | 1500/25000 [40:50<10:11:24,  1.56s/it]
100%|██████████| 63/63 [00:07<00:00,  8.35it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:02:30,504 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1500
[INFO|configuration_utils.py:461] 2024-01-22 01:02:30,513 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1500/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:02:31,104 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:02:31,105 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:02:31,105 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1500/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 01:02:32,224 >> Initializing global attention on CLS token...
  6%|▌         | 1501/25000 [40:53<28:54:00,  4.43s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:33,900 >> Initializing global attention on CLS token...
  6%|▌         | 1502/25000 [40:55<23:15:12,  3.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:35,447 >> Initializing global attention on CLS token...
  6%|▌         | 1503/25000 [40:56<19:18:40,  2.96s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:36,996 >> Initializing global attention on CLS token...
  6%|▌         | 1504/25000 [40:58<16:33:50,  2.54s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:38,554 >> Initializing global attention on CLS token...
  6%|▌         | 1505/25000 [40:59<14:37:37,  2.24s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:40,102 >> Initializing global attention on CLS token...
  6%|▌         | 1506/25000 [41:01<13:16:42,  2.03s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:41,655 >> Initializing global attention on CLS token...
  6%|▌         | 1507/25000 [41:02<12:19:28,  1.89s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:43,203 >> Initializing global attention on CLS token...
  6%|▌         | 1508/25000 [41:04<11:39:16,  1.79s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:44,749 >> Initializing global attention on CLS token...
  6%|▌         | 1509/25000 [41:06<11:12:58,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:46,312 >> Initializing global attention on CLS token...
  6%|▌         | 1510/25000 [41:07<10:53:09,  1.67s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:47,862 >> Initializing global attention on CLS token...
  6%|▌         | 1511/25000 [41:09<10:41:16,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:49,429 >> Initializing global attention on CLS token...
  6%|▌         | 1512/25000 [41:10<10:31:03,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:50,981 >> Initializing global attention on CLS token...
  6%|▌         | 1513/25000 [41:12<10:23:35,  1.59s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:52,530 >> Initializing global attention on CLS token...
  6%|▌         | 1514/25000 [41:13<10:18:37,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:54,081 >> Initializing global attention on CLS token...
  6%|▌         | 1515/25000 [41:15<10:16:27,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:55,643 >> Initializing global attention on CLS token...
  6%|▌         | 1516/25000 [41:16<10:13:20,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:57,191 >> Initializing global attention on CLS token...
  6%|▌         | 1517/25000 [41:18<10:11:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:02:58,741 >> Initializing global attention on CLS token...
  6%|▌         | 1518/25000 [41:20<10:09:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:00,288 >> Initializing global attention on CLS token...
  6%|▌         | 1519/25000 [41:21<10:09:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:01,847 >> Initializing global attention on CLS token...
  6%|▌         | 1520/25000 [41:23<10:10:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:03,410 >> Initializing global attention on CLS token...
  6%|▌         | 1521/25000 [41:24<10:08:55,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:04,957 >> Initializing global attention on CLS token...
  6%|▌         | 1522/25000 [41:26<10:09:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:06,518 >> Initializing global attention on CLS token...
  6%|▌         | 1523/25000 [41:27<10:08:35,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:08,069 >> Initializing global attention on CLS token...
  6%|▌         | 1524/25000 [41:29<10:08:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:09,620 >> Initializing global attention on CLS token...
  6%|▌         | 1525/25000 [41:30<10:07:40,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:11,170 >> Initializing global attention on CLS token...
  6%|▌         | 1526/25000 [41:32<10:07:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:12,726 >> Initializing global attention on CLS token...
  6%|▌         | 1527/25000 [41:34<10:06:53,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:14,271 >> Initializing global attention on CLS token...
  6%|▌         | 1528/25000 [41:35<10:07:00,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:15,825 >> Initializing global attention on CLS token...
  6%|▌         | 1529/25000 [41:37<10:09:10,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:17,402 >> Initializing global attention on CLS token...
  6%|▌         | 1530/25000 [41:38<10:09:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:18,953 >> Initializing global attention on CLS token...
  6%|▌         | 1531/25000 [41:40<10:08:01,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:20,500 >> Initializing global attention on CLS token...
  6%|▌         | 1532/25000 [41:41<10:07:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:22,048 >> Initializing global attention on CLS token...
  6%|▌         | 1533/25000 [41:43<10:09:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:23,629 >> Initializing global attention on CLS token...
  6%|▌         | 1534/25000 [41:44<10:10:32,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:25,188 >> Initializing global attention on CLS token...
  6%|▌         | 1535/25000 [41:46<10:09:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:26,741 >> Initializing global attention on CLS token...
  6%|▌         | 1536/25000 [41:48<10:09:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:28,297 >> Initializing global attention on CLS token...
  6%|▌         | 1537/25000 [41:49<10:13:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:29,893 >> Initializing global attention on CLS token...
  6%|▌         | 1538/25000 [41:51<10:11:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:31,445 >> Initializing global attention on CLS token...
  6%|▌         | 1539/25000 [41:52<10:09:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:32,994 >> Initializing global attention on CLS token...
  6%|▌         | 1540/25000 [41:54<10:11:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:34,567 >> Initializing global attention on CLS token...
  6%|▌         | 1541/25000 [41:55<10:09:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:36,120 >> Initializing global attention on CLS token...
  6%|▌         | 1542/25000 [41:57<10:08:59,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:37,671 >> Initializing global attention on CLS token...
  6%|▌         | 1543/25000 [41:58<10:07:48,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:39,220 >> Initializing global attention on CLS token...
  6%|▌         | 1544/25000 [42:00<10:08:37,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:40,781 >> Initializing global attention on CLS token...
  6%|▌         | 1545/25000 [42:02<10:08:12,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:42,342 >> Initializing global attention on CLS token...
  6%|▌         | 1546/25000 [42:03<10:09:14,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:43,899 >> Initializing global attention on CLS token...
  6%|▌         | 1547/25000 [42:05<10:09:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:45,460 >> Initializing global attention on CLS token...
  6%|▌         | 1548/25000 [42:06<10:12:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:47,048 >> Initializing global attention on CLS token...
  6%|▌         | 1549/25000 [42:08<10:11:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:48,602 >> Initializing global attention on CLS token...
  6%|▌         | 1550/25000 [42:09<10:09:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:50,152 >> Initializing global attention on CLS token...
  6%|▌         | 1551/25000 [42:11<10:10:24,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:51,719 >> Initializing global attention on CLS token...
  6%|▌         | 1552/25000 [42:13<10:10:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:53,277 >> Initializing global attention on CLS token...
  6%|▌         | 1553/25000 [42:14<10:10:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:54,845 >> Initializing global attention on CLS token...
  6%|▌         | 1554/25000 [42:16<10:10:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:56,406 >> Initializing global attention on CLS token...
  6%|▌         | 1555/25000 [42:17<10:09:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:57,958 >> Initializing global attention on CLS token...
  6%|▌         | 1556/25000 [42:19<10:07:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:03:59,502 >> Initializing global attention on CLS token...
  6%|▌         | 1557/25000 [42:20<10:06:23,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:01,046 >> Initializing global attention on CLS token...
  6%|▌         | 1558/25000 [42:22<10:06:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:02,598 >> Initializing global attention on CLS token...
  6%|▌         | 1559/25000 [42:23<10:06:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:04,154 >> Initializing global attention on CLS token...
  6%|▌         | 1560/25000 [42:25<10:05:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:05,703 >> Initializing global attention on CLS token...
  6%|▌         | 1561/25000 [42:27<10:06:30,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:07,259 >> Initializing global attention on CLS token...
  6%|▌         | 1562/25000 [42:28<10:06:42,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:08,812 >> Initializing global attention on CLS token...
  6%|▋         | 1563/25000 [42:30<10:06:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:10,361 >> Initializing global attention on CLS token...
  6%|▋         | 1564/25000 [42:31<10:05:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:11,912 >> Initializing global attention on CLS token...
  6%|▋         | 1565/25000 [42:33<10:12:07,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:13,516 >> Initializing global attention on CLS token...
  6%|▋         | 1566/25000 [42:34<10:10:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:15,067 >> Initializing global attention on CLS token...
  6%|▋         | 1567/25000 [42:36<10:08:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:16,617 >> Initializing global attention on CLS token...
  6%|▋         | 1568/25000 [42:37<10:09:58,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:18,193 >> Initializing global attention on CLS token...
  6%|▋         | 1569/25000 [42:39<10:09:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:19,744 >> Initializing global attention on CLS token...
  6%|▋         | 1570/25000 [42:41<10:08:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:21,296 >> Initializing global attention on CLS token...
  6%|▋         | 1571/25000 [42:42<10:08:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:22,861 >> Initializing global attention on CLS token...
  6%|▋         | 1572/25000 [42:44<10:10:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:24,429 >> Initializing global attention on CLS token...
  6%|▋         | 1573/25000 [42:45<10:09:46,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:25,988 >> Initializing global attention on CLS token...
  6%|▋         | 1574/25000 [42:47<10:08:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:27,536 >> Initializing global attention on CLS token...
  6%|▋         | 1575/25000 [42:48<10:07:11,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:29,085 >> Initializing global attention on CLS token...
  6%|▋         | 1576/25000 [42:50<10:07:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:30,642 >> Initializing global attention on CLS token...
  6%|▋         | 1577/25000 [42:51<10:06:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:32,193 >> Initializing global attention on CLS token...
  6%|▋         | 1578/25000 [42:53<10:08:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:33,759 >> Initializing global attention on CLS token...
  6%|▋         | 1579/25000 [42:55<10:07:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:35,308 >> Initializing global attention on CLS token...
  6%|▋         | 1580/25000 [42:56<10:06:04,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:36,855 >> Initializing global attention on CLS token...
  6%|▋         | 1581/25000 [42:58<10:05:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:38,407 >> Initializing global attention on CLS token...
  6%|▋         | 1582/25000 [42:59<10:06:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:39,960 >> Initializing global attention on CLS token...
  6%|▋         | 1583/25000 [43:01<10:05:37,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:41,507 >> Initializing global attention on CLS token...
  6%|▋         | 1584/25000 [43:02<10:04:35,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:43,050 >> Initializing global attention on CLS token...
  6%|▋         | 1585/25000 [43:04<10:04:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:44,599 >> Initializing global attention on CLS token...
  6%|▋         | 1586/25000 [43:05<10:05:06,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:46,152 >> Initializing global attention on CLS token...
  6%|▋         | 1587/25000 [43:07<10:04:00,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:47,694 >> Initializing global attention on CLS token...
  6%|▋         | 1588/25000 [43:09<10:03:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:49,236 >> Initializing global attention on CLS token...
  6%|▋         | 1589/25000 [43:10<10:03:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:50,785 >> Initializing global attention on CLS token...
  6%|▋         | 1590/25000 [43:12<10:03:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:52,334 >> Initializing global attention on CLS token...
  6%|▋         | 1591/25000 [43:13<10:03:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:53,883 >> Initializing global attention on CLS token...
  6%|▋         | 1592/25000 [43:15<10:04:38,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:55,438 >> Initializing global attention on CLS token...
  6%|▋         | 1593/25000 [43:16<10:05:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:56,992 >> Initializing global attention on CLS token...
  6%|▋         | 1594/25000 [43:18<10:05:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:04:58,551 >> Initializing global attention on CLS token...
  6%|▋         | 1595/25000 [43:19<10:05:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:00,099 >> Initializing global attention on CLS token...
  6%|▋         | 1596/25000 [43:21<10:12:00,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:01,708 >> Initializing global attention on CLS token...
  6%|▋         | 1597/25000 [43:23<10:09:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:03,259 >> Initializing global attention on CLS token...
  6%|▋         | 1598/25000 [43:24<10:08:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:04,809 >> Initializing global attention on CLS token...
  6%|▋         | 1599/25000 [43:26<10:09:20,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:06,378 >> Initializing global attention on CLS token...
  6%|▋         | 1600/25000 [43:27<10:07:30,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:07,925 >> Initializing global attention on CLS token...
  6%|▋         | 1601/25000 [43:29<10:07:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:09,479 >> Initializing global attention on CLS token...
  6%|▋         | 1602/25000 [43:30<10:08:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:11,041 >> Initializing global attention on CLS token...
  6%|▋         | 1603/25000 [43:32<10:06:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:12,590 >> Initializing global attention on CLS token...
  6%|▋         | 1604/25000 [43:33<10:05:47,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:14,140 >> Initializing global attention on CLS token...
  6%|▋         | 1605/25000 [43:35<10:05:57,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:15,695 >> Initializing global attention on CLS token...
  6%|▋         | 1606/25000 [43:37<10:06:11,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:17,251 >> Initializing global attention on CLS token...
  6%|▋         | 1607/25000 [43:38<10:05:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:18,799 >> Initializing global attention on CLS token...
  6%|▋         | 1608/25000 [43:40<10:05:27,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:20,353 >> Initializing global attention on CLS token...
  6%|▋         | 1609/25000 [43:41<10:05:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:21,908 >> Initializing global attention on CLS token...
  6%|▋         | 1610/25000 [43:43<10:05:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:23,459 >> Initializing global attention on CLS token...
  6%|▋         | 1611/25000 [43:44<10:06:02,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:25,018 >> Initializing global attention on CLS token...
  6%|▋         | 1612/25000 [43:46<10:05:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:26,567 >> Initializing global attention on CLS token...
  6%|▋         | 1613/25000 [43:47<10:06:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:28,133 >> Initializing global attention on CLS token...
  6%|▋         | 1614/25000 [43:49<10:07:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:29,694 >> Initializing global attention on CLS token...
  6%|▋         | 1615/25000 [43:51<10:06:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:31,245 >> Initializing global attention on CLS token...
  6%|▋         | 1616/25000 [43:52<10:06:18,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:32,799 >> Initializing global attention on CLS token...
  6%|▋         | 1617/25000 [43:54<10:05:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:34,347 >> Initializing global attention on CLS token...
  6%|▋         | 1618/25000 [43:55<10:04:54,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:35,896 >> Initializing global attention on CLS token...
  6%|▋         | 1619/25000 [43:57<10:05:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:37,458 >> Initializing global attention on CLS token...
  6%|▋         | 1620/25000 [43:58<10:07:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:39,024 >> Initializing global attention on CLS token...
  6%|▋         | 1621/25000 [44:00<10:06:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:40,573 >> Initializing global attention on CLS token...
  6%|▋         | 1622/25000 [44:01<10:05:36,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:42,125 >> Initializing global attention on CLS token...
  6%|▋         | 1623/25000 [44:03<10:07:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:43,696 >> Initializing global attention on CLS token...
  6%|▋         | 1624/25000 [44:05<10:06:41,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:45,228 >> Initializing global attention on CLS token...
  6%|▋         | 1625/25000 [44:06<10:04:06,  1.55s/it]                                                         6%|▋         | 1625/25000 [44:06<10:04:06,  1.55s/it][INFO|trainer.py:738] 2024-01-22 01:05:46,766 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:05:46,769 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:05:46,769 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:05:46,769 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:46,781 >> Initializing global attention on CLS token...
{'eval_loss': 4.966553688049316, 'eval_accuracy': 0.276, 'eval_macro_f1': 0.09980470137346992, 'eval_macro_precision': 0.09135815464152414, 'eval_macro_recall': 0.13275703367020464, 'eval_micro_f1': 0.276, 'eval_micro_precision': 0.276, 'eval_micro_recall': 0.276, 'eval_combined_score': 0.20398855566931412, 'eval_runtime': 7.7258, 'eval_samples_per_second': 64.719, 'eval_steps_per_second': 8.155, 'epoch': 12.0}
{'loss': 1.8384, 'learning_rate': 6.544999999999999e-05, 'epoch': 13.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:46,901 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.83it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,021 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,139 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.56it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,259 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,383 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,503 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,622 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.92it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,742 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.79it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,862 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.65it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:47,985 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:06,  8.54it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,105 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:06,  8.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,225 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:06,  7.92it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,379 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:06,  7.92it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,500 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.04it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,619 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,738 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,859 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:48,979 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,099 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,218 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.29it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,339 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,459 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,578 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,699 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,819 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:03<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:49,939 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.34it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,058 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,183 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,302 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:04,  8.17it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,428 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,548 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,669 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,790 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:04<00:03,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:50,912 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,033 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,154 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,276 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,446 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:03,  7.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,566 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:03,  7.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,686 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  7.82it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,807 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:05<00:02,  7.95it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:51,928 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.06it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,047 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.14it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,168 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,287 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,407 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,527 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,647 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,766 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:52,885 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,004 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,124 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,243 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,365 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,485 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,604 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  7.96it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,743 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.09it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,862 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:07<00:00,  8.16it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:53,982 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:54,102 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:54,220 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:54,334 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                       
                                               [A  6%|▋         | 1625/25000 [44:14<10:04:06,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.33it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:05:54,419 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1625
[INFO|configuration_utils.py:461] 2024-01-22 01:05:54,426 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1625/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:05:54,974 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1625/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:05:54,976 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1625/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:05:54,976 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1625/special_tokens_map.json
[INFO|modeling_longformer.py:1920] 2024-01-22 01:05:56,048 >> Initializing global attention on CLS token...
  7%|▋         | 1626/25000 [44:17<28:06:51,  4.33s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:57,599 >> Initializing global attention on CLS token...
  7%|▋         | 1627/25000 [44:18<22:44:43,  3.50s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:05:59,177 >> Initializing global attention on CLS token...
  7%|▋         | 1628/25000 [44:20<18:57:02,  2.92s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:00,728 >> Initializing global attention on CLS token...
  7%|▋         | 1629/25000 [44:22<16:16:21,  2.51s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:02,269 >> Initializing global attention on CLS token...
  7%|▋         | 1630/25000 [44:23<14:23:27,  2.22s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:03,810 >> Initializing global attention on CLS token...
  7%|▋         | 1631/25000 [44:25<13:04:55,  2.02s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:05,359 >> Initializing global attention on CLS token...
  7%|▋         | 1632/25000 [44:26<12:10:16,  1.88s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:06,906 >> Initializing global attention on CLS token...
  7%|▋         | 1633/25000 [44:28<11:32:02,  1.78s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:08,454 >> Initializing global attention on CLS token...
  7%|▋         | 1634/25000 [44:29<11:11:02,  1.72s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:10,051 >> Initializing global attention on CLS token...
  7%|▋         | 1635/25000 [44:31<10:53:12,  1.68s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:11,621 >> Initializing global attention on CLS token...
  7%|▋         | 1636/25000 [44:32<10:38:25,  1.64s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:13,174 >> Initializing global attention on CLS token...
  7%|▋         | 1637/25000 [44:34<10:27:49,  1.61s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:14,747 >> Initializing global attention on CLS token...
  7%|▋         | 1638/25000 [44:36<10:30:18,  1.62s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:16,357 >> Initializing global attention on CLS token...
  7%|▋         | 1639/25000 [44:37<10:21:59,  1.60s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:17,905 >> Initializing global attention on CLS token...
  7%|▋         | 1640/25000 [44:39<10:16:13,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:19,452 >> Initializing global attention on CLS token...
  7%|▋         | 1641/25000 [44:40<10:14:57,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:21,025 >> Initializing global attention on CLS token...
  7%|▋         | 1642/25000 [44:42<10:11:43,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:22,576 >> Initializing global attention on CLS token...
  7%|▋         | 1643/25000 [44:43<10:09:01,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:24,125 >> Initializing global attention on CLS token...
  7%|▋         | 1644/25000 [44:45<10:10:41,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:25,701 >> Initializing global attention on CLS token...
  7%|▋         | 1645/25000 [44:47<10:09:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:27,260 >> Initializing global attention on CLS token...
  7%|▋         | 1646/25000 [44:48<10:08:02,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:28,816 >> Initializing global attention on CLS token...
  7%|▋         | 1647/25000 [44:50<10:06:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:30,364 >> Initializing global attention on CLS token...
  7%|▋         | 1648/25000 [44:51<10:06:06,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:31,921 >> Initializing global attention on CLS token...
  7%|▋         | 1649/25000 [44:53<10:05:15,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:33,471 >> Initializing global attention on CLS token...
  7%|▋         | 1650/25000 [44:54<10:07:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:35,044 >> Initializing global attention on CLS token...
  7%|▋         | 1651/25000 [44:56<10:07:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:36,605 >> Initializing global attention on CLS token...
  7%|▋         | 1652/25000 [44:57<10:06:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:38,152 >> Initializing global attention on CLS token...
  7%|▋         | 1653/25000 [44:59<10:05:07,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:39,703 >> Initializing global attention on CLS token...
  7%|▋         | 1654/25000 [45:01<10:04:09,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:41,250 >> Initializing global attention on CLS token...
  7%|▋         | 1655/25000 [45:02<10:03:55,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:42,801 >> Initializing global attention on CLS token...
  7%|▋         | 1656/25000 [45:04<10:03:17,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:44,350 >> Initializing global attention on CLS token...
  7%|▋         | 1657/25000 [45:05<10:03:05,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:45,898 >> Initializing global attention on CLS token...
  7%|▋         | 1658/25000 [45:07<10:04:33,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:47,461 >> Initializing global attention on CLS token...
  7%|▋         | 1659/25000 [45:08<10:03:39,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:49,007 >> Initializing global attention on CLS token...
  7%|▋         | 1660/25000 [45:10<10:04:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:50,569 >> Initializing global attention on CLS token...
  7%|▋         | 1661/25000 [45:11<10:04:16,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:52,119 >> Initializing global attention on CLS token...
  7%|▋         | 1662/25000 [45:13<10:04:24,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:53,674 >> Initializing global attention on CLS token...
  7%|▋         | 1663/25000 [45:14<10:04:13,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:55,226 >> Initializing global attention on CLS token...
  7%|▋         | 1664/25000 [45:16<10:04:12,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:56,780 >> Initializing global attention on CLS token...
  7%|▋         | 1665/25000 [45:18<10:08:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:58,366 >> Initializing global attention on CLS token...
  7%|▋         | 1666/25000 [45:19<10:06:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:06:59,918 >> Initializing global attention on CLS token...
  7%|▋         | 1667/25000 [45:21<10:05:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:01,469 >> Initializing global attention on CLS token...
  7%|▋         | 1668/25000 [45:22<10:04:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:03,018 >> Initializing global attention on CLS token...
  7%|▋         | 1669/25000 [45:24<10:06:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:04,593 >> Initializing global attention on CLS token...
  7%|▋         | 1670/25000 [45:25<10:06:29,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:06,151 >> Initializing global attention on CLS token...
  7%|▋         | 1671/25000 [45:27<10:06:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:07,710 >> Initializing global attention on CLS token...
  7%|▋         | 1672/25000 [45:29<10:05:49,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:09,264 >> Initializing global attention on CLS token...
  7%|▋         | 1673/25000 [45:30<10:05:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:10,816 >> Initializing global attention on CLS token...
  7%|▋         | 1674/25000 [45:32<10:05:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:12,373 >> Initializing global attention on CLS token...
  7%|▋         | 1675/25000 [45:33<10:08:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:13,952 >> Initializing global attention on CLS token...
  7%|▋         | 1676/25000 [45:35<10:06:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:15,503 >> Initializing global attention on CLS token...
  7%|▋         | 1677/25000 [45:36<10:04:51,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:17,052 >> Initializing global attention on CLS token...
  7%|▋         | 1678/25000 [45:38<10:05:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:18,612 >> Initializing global attention on CLS token...
  7%|▋         | 1679/25000 [45:39<10:06:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:20,180 >> Initializing global attention on CLS token...
  7%|▋         | 1680/25000 [45:41<10:05:09,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:21,730 >> Initializing global attention on CLS token...
  7%|▋         | 1681/25000 [45:43<10:04:43,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:23,295 >> Initializing global attention on CLS token...
  7%|▋         | 1682/25000 [45:44<10:06:22,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:24,853 >> Initializing global attention on CLS token...
  7%|▋         | 1683/25000 [45:46<10:05:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:26,406 >> Initializing global attention on CLS token...
  7%|▋         | 1684/25000 [45:47<10:06:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:27,969 >> Initializing global attention on CLS token...
  7%|▋         | 1685/25000 [45:49<10:04:54,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:29,519 >> Initializing global attention on CLS token...
  7%|▋         | 1686/25000 [45:50<10:04:36,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:31,074 >> Initializing global attention on CLS token...
  7%|▋         | 1687/25000 [45:52<10:03:31,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:32,620 >> Initializing global attention on CLS token...
  7%|▋         | 1688/25000 [45:53<10:03:29,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:34,172 >> Initializing global attention on CLS token...
  7%|▋         | 1689/25000 [45:55<10:04:27,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:35,735 >> Initializing global attention on CLS token...
  7%|▋         | 1690/25000 [45:57<10:03:45,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:37,285 >> Initializing global attention on CLS token...
  7%|▋         | 1691/25000 [45:58<10:03:34,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:38,838 >> Initializing global attention on CLS token...
  7%|▋         | 1692/25000 [46:00<10:02:51,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:40,385 >> Initializing global attention on CLS token...
  7%|▋         | 1693/25000 [46:01<10:05:16,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:41,958 >> Initializing global attention on CLS token...
  7%|▋         | 1694/25000 [46:03<10:04:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:43,510 >> Initializing global attention on CLS token...
  7%|▋         | 1695/25000 [46:04<10:03:41,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:45,060 >> Initializing global attention on CLS token...
  7%|▋         | 1696/25000 [46:06<10:05:04,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:46,627 >> Initializing global attention on CLS token...
  7%|▋         | 1697/25000 [46:07<10:05:31,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:48,189 >> Initializing global attention on CLS token...
  7%|▋         | 1698/25000 [46:09<10:05:21,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:49,747 >> Initializing global attention on CLS token...
  7%|▋         | 1699/25000 [46:11<10:04:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:51,298 >> Initializing global attention on CLS token...
  7%|▋         | 1700/25000 [46:12<10:09:15,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:52,897 >> Initializing global attention on CLS token...
  7%|▋         | 1701/25000 [46:14<10:09:01,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:54,462 >> Initializing global attention on CLS token...
  7%|▋         | 1702/25000 [46:15<10:06:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:56,013 >> Initializing global attention on CLS token...
  7%|▋         | 1703/25000 [46:17<10:05:52,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:57,567 >> Initializing global attention on CLS token...
  7%|▋         | 1704/25000 [46:18<10:04:34,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:07:59,117 >> Initializing global attention on CLS token...
  7%|▋         | 1705/25000 [46:20<10:07:26,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:00,699 >> Initializing global attention on CLS token...
  7%|▋         | 1706/25000 [46:22<10:05:45,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:02,249 >> Initializing global attention on CLS token...
  7%|▋         | 1707/25000 [46:23<10:07:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:03,822 >> Initializing global attention on CLS token...
  7%|▋         | 1708/25000 [46:25<10:05:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:05,372 >> Initializing global attention on CLS token...
  7%|▋         | 1709/25000 [46:26<10:04:13,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:06,921 >> Initializing global attention on CLS token...
  7%|▋         | 1710/25000 [46:28<10:04:50,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:08,483 >> Initializing global attention on CLS token...
  7%|▋         | 1711/25000 [46:29<10:03:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:10,032 >> Initializing global attention on CLS token...
  7%|▋         | 1712/25000 [46:31<10:02:58,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:11,581 >> Initializing global attention on CLS token...
  7%|▋         | 1713/25000 [46:32<10:02:43,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:13,139 >> Initializing global attention on CLS token...
  7%|▋         | 1714/25000 [46:34<10:03:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:14,689 >> Initializing global attention on CLS token...
  7%|▋         | 1715/25000 [46:36<10:02:20,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:16,237 >> Initializing global attention on CLS token...
  7%|▋         | 1716/25000 [46:37<10:01:49,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:17,784 >> Initializing global attention on CLS token...
  7%|▋         | 1717/25000 [46:39<10:02:03,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:19,337 >> Initializing global attention on CLS token...
  7%|▋         | 1718/25000 [46:40<10:02:14,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:20,891 >> Initializing global attention on CLS token...
  7%|▋         | 1719/25000 [46:42<10:04:17,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:22,461 >> Initializing global attention on CLS token...
  7%|▋         | 1720/25000 [46:43<10:04:08,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:24,017 >> Initializing global attention on CLS token...
  7%|▋         | 1721/25000 [46:45<10:03:23,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:25,580 >> Initializing global attention on CLS token...
  7%|▋         | 1722/25000 [46:46<10:04:48,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:27,135 >> Initializing global attention on CLS token...
  7%|▋         | 1723/25000 [46:48<10:04:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:28,688 >> Initializing global attention on CLS token...
  7%|▋         | 1724/25000 [46:50<10:05:25,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:30,261 >> Initializing global attention on CLS token...
  7%|▋         | 1725/25000 [46:51<10:05:39,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:31,819 >> Initializing global attention on CLS token...
  7%|▋         | 1726/25000 [46:53<10:08:49,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:33,408 >> Initializing global attention on CLS token...
  7%|▋         | 1727/25000 [46:54<10:08:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:34,977 >> Initializing global attention on CLS token...
  7%|▋         | 1728/25000 [46:56<10:07:50,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:36,540 >> Initializing global attention on CLS token...
  7%|▋         | 1729/25000 [46:57<10:08:02,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:38,109 >> Initializing global attention on CLS token...
  7%|▋         | 1730/25000 [46:59<10:06:58,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:39,668 >> Initializing global attention on CLS token...
  7%|▋         | 1731/25000 [47:01<10:10:21,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:41,262 >> Initializing global attention on CLS token...
  7%|▋         | 1732/25000 [47:02<10:08:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:42,817 >> Initializing global attention on CLS token...
  7%|▋         | 1733/25000 [47:04<10:05:44,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:44,379 >> Initializing global attention on CLS token...
  7%|▋         | 1734/25000 [47:05<10:12:30,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:45,985 >> Initializing global attention on CLS token...
  7%|▋         | 1735/25000 [47:07<10:09:09,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:47,536 >> Initializing global attention on CLS token...
  7%|▋         | 1736/25000 [47:08<10:13:01,  1.58s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:49,141 >> Initializing global attention on CLS token...
  7%|▋         | 1737/25000 [47:10<10:10:05,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:50,697 >> Initializing global attention on CLS token...
  7%|▋         | 1738/25000 [47:12<10:08:56,  1.57s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:52,261 >> Initializing global attention on CLS token...
  7%|▋         | 1739/25000 [47:13<10:06:40,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:53,813 >> Initializing global attention on CLS token...
  7%|▋         | 1740/25000 [47:15<10:04:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:55,359 >> Initializing global attention on CLS token...
  7%|▋         | 1741/25000 [47:16<10:04:19,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:56,917 >> Initializing global attention on CLS token...
  7%|▋         | 1742/25000 [47:18<10:03:00,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:08:58,464 >> Initializing global attention on CLS token...
  7%|▋         | 1743/25000 [47:19<10:04:33,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:00,033 >> Initializing global attention on CLS token...
  7%|▋         | 1744/25000 [47:21<10:02:47,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:01,583 >> Initializing global attention on CLS token...
  7%|▋         | 1745/25000 [47:22<10:03:28,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:03,140 >> Initializing global attention on CLS token...
  7%|▋         | 1746/25000 [47:24<10:03:03,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:04,694 >> Initializing global attention on CLS token...
  7%|▋         | 1747/25000 [47:26<10:02:15,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:06,243 >> Initializing global attention on CLS token...
  7%|▋         | 1748/25000 [47:27<10:02:53,  1.56s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:07,803 >> Initializing global attention on CLS token...
  7%|▋         | 1749/25000 [47:29<10:01:59,  1.55s/it][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:09,331 >> Initializing global attention on CLS token...
  7%|▋         | 1750/25000 [47:30<9:59:34,  1.55s/it]                                                         7%|▋         | 1750/25000 [47:30<9:59:34,  1.55s/it][INFO|trainer.py:738] 2024-01-22 01:09:10,866 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:09:10,869 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:09:10,869 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:09:10,869 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:10,881 >> Initializing global attention on CLS token...
{'eval_loss': 4.979498863220215, 'eval_accuracy': 0.28, 'eval_macro_f1': 0.08850502221606428, 'eval_macro_precision': 0.07988100160336664, 'eval_macro_recall': 0.12349747362822444, 'eval_micro_f1': 0.28, 'eval_micro_precision': 0.28, 'eval_micro_recall': 0.28, 'eval_combined_score': 0.20169764249252223, 'eval_runtime': 7.6482, 'eval_samples_per_second': 65.375, 'eval_steps_per_second': 8.237, 'epoch': 13.0}
{'loss': 1.6319, 'learning_rate': 6.51e-05, 'epoch': 14.0}

  0%|          | 0/63 [00:00<?, ?it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,001 >> Initializing global attention on CLS token...

  3%|▎         | 2/63 [00:00<00:03, 16.81it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,120 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,249 >> Initializing global attention on CLS token...

  6%|▋         | 4/63 [00:00<00:05, 10.25it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,369 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,495 >> Initializing global attention on CLS token...

 10%|▉         | 6/63 [00:00<00:06,  9.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,612 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,729 >> Initializing global attention on CLS token...

 13%|█▎        | 8/63 [00:00<00:06,  8.93it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,846 >> Initializing global attention on CLS token...

 14%|█▍        | 9/63 [00:00<00:06,  8.84it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:11,963 >> Initializing global attention on CLS token...

 16%|█▌        | 10/63 [00:01<00:06,  8.76it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,080 >> Initializing global attention on CLS token...

 17%|█▋        | 11/63 [00:01<00:05,  8.71it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,197 >> Initializing global attention on CLS token...

 19%|█▉        | 12/63 [00:01<00:05,  8.67it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,314 >> Initializing global attention on CLS token...

 21%|██        | 13/63 [00:01<00:05,  8.63it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,431 >> Initializing global attention on CLS token...

 22%|██▏       | 14/63 [00:01<00:05,  8.46it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,557 >> Initializing global attention on CLS token...

 24%|██▍       | 15/63 [00:01<00:05,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,674 >> Initializing global attention on CLS token...

 25%|██▌       | 16/63 [00:01<00:05,  8.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,791 >> Initializing global attention on CLS token...

 27%|██▋       | 17/63 [00:01<00:05,  8.49it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:12,909 >> Initializing global attention on CLS token...

 29%|██▊       | 18/63 [00:02<00:05,  8.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,026 >> Initializing global attention on CLS token...

 30%|███       | 19/63 [00:02<00:05,  8.50it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,152 >> Initializing global attention on CLS token...

 32%|███▏      | 20/63 [00:02<00:05,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,270 >> Initializing global attention on CLS token...

 33%|███▎      | 21/63 [00:02<00:05,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,388 >> Initializing global attention on CLS token...

 35%|███▍      | 22/63 [00:02<00:04,  8.42it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,505 >> Initializing global attention on CLS token...

 37%|███▋      | 23/63 [00:02<00:04,  8.43it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,623 >> Initializing global attention on CLS token...

 38%|███▊      | 24/63 [00:02<00:04,  8.45it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,741 >> Initializing global attention on CLS token...

 40%|███▉      | 25/63 [00:02<00:04,  8.47it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,859 >> Initializing global attention on CLS token...

 41%|████▏     | 26/63 [00:02<00:04,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:13,983 >> Initializing global attention on CLS token...

 43%|████▎     | 27/63 [00:03<00:04,  8.36it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,103 >> Initializing global attention on CLS token...

 44%|████▍     | 28/63 [00:03<00:04,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,222 >> Initializing global attention on CLS token...

 46%|████▌     | 29/63 [00:03<00:04,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,341 >> Initializing global attention on CLS token...

 48%|████▊     | 30/63 [00:03<00:03,  8.37it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,461 >> Initializing global attention on CLS token...

 49%|████▉     | 31/63 [00:03<00:03,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,579 >> Initializing global attention on CLS token...

 51%|█████     | 32/63 [00:03<00:03,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,698 >> Initializing global attention on CLS token...

 52%|█████▏    | 33/63 [00:03<00:03,  8.39it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,817 >> Initializing global attention on CLS token...

 54%|█████▍    | 34/63 [00:03<00:03,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:14,944 >> Initializing global attention on CLS token...

 56%|█████▌    | 35/63 [00:04<00:03,  8.10it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,072 >> Initializing global attention on CLS token...

 57%|█████▋    | 36/63 [00:04<00:03,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,190 >> Initializing global attention on CLS token...

 59%|█████▊    | 37/63 [00:04<00:03,  8.11it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,317 >> Initializing global attention on CLS token...

 60%|██████    | 38/63 [00:04<00:03,  8.19it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,436 >> Initializing global attention on CLS token...

 62%|██████▏   | 39/63 [00:04<00:02,  8.18it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,560 >> Initializing global attention on CLS token...

 63%|██████▎   | 40/63 [00:04<00:02,  8.22it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,680 >> Initializing global attention on CLS token...

 65%|██████▌   | 41/63 [00:04<00:02,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,800 >> Initializing global attention on CLS token...

 67%|██████▋   | 42/63 [00:04<00:02,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:15,919 >> Initializing global attention on CLS token...

 68%|██████▊   | 43/63 [00:05<00:02,  8.31it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,039 >> Initializing global attention on CLS token...

 70%|██████▉   | 44/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,158 >> Initializing global attention on CLS token...

 71%|███████▏  | 45/63 [00:05<00:02,  8.35it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,277 >> Initializing global attention on CLS token...

 73%|███████▎  | 46/63 [00:05<00:02,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,399 >> Initializing global attention on CLS token...

 75%|███████▍  | 47/63 [00:05<00:01,  8.32it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,518 >> Initializing global attention on CLS token...

 76%|███████▌  | 48/63 [00:05<00:01,  8.33it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,639 >> Initializing global attention on CLS token...

 78%|███████▊  | 49/63 [00:05<00:01,  8.23it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,763 >> Initializing global attention on CLS token...

 79%|███████▉  | 50/63 [00:05<00:01,  8.26it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:16,884 >> Initializing global attention on CLS token...

 81%|████████  | 51/63 [00:06<00:01,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,005 >> Initializing global attention on CLS token...

 83%|████████▎ | 52/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,125 >> Initializing global attention on CLS token...

 84%|████████▍ | 53/63 [00:06<00:01,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,246 >> Initializing global attention on CLS token...

 86%|████████▌ | 54/63 [00:06<00:01,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,367 >> Initializing global attention on CLS token...

 87%|████████▋ | 55/63 [00:06<00:00,  8.30it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,486 >> Initializing global attention on CLS token...

 89%|████████▉ | 56/63 [00:06<00:00,  8.20it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,611 >> Initializing global attention on CLS token...

 90%|█████████ | 57/63 [00:06<00:00,  8.24it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,731 >> Initializing global attention on CLS token...

 92%|█████████▏| 58/63 [00:06<00:00,  8.28it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,851 >> Initializing global attention on CLS token...

 94%|█████████▎| 59/63 [00:06<00:00,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:17,983 >> Initializing global attention on CLS token...

 95%|█████████▌| 60/63 [00:07<00:00,  8.07it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:18,106 >> Initializing global attention on CLS token...

 97%|█████████▋| 61/63 [00:07<00:00,  8.13it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:18,223 >> Initializing global attention on CLS token...

 98%|█████████▊| 62/63 [00:07<00:00,  8.27it/s][A[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:18,337 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                      
                                               [A  7%|▋         | 1750/25000 [47:38<9:59:34,  1.55s/it]
100%|██████████| 63/63 [00:07<00:00,  8.27it/s][A
                                               [A[INFO|trainer.py:2881] 2024-01-22 01:09:18,422 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1750
[INFO|configuration_utils.py:461] 2024-01-22 01:09:18,429 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1750/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:09:18,993 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:09:18,995 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:09:18,995 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1750/special_tokens_map.json
[INFO|trainer.py:1955] 2024-01-22 01:09:20,044 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2139] 2024-01-22 01:09:20,044 >> Loading best model from /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/checkpoint-1375 (score: 4.902622222900391).
                                                        7%|▋         | 1750/25000 [47:39<9:59:34,  1.55s/it]  7%|▋         | 1750/25000 [47:39<10:33:16,  1.63s/it]
[INFO|trainer.py:2881] 2024-01-22 01:09:20,153 >> Saving model checkpoint to /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7
[INFO|configuration_utils.py:461] 2024-01-22 01:09:20,160 >> Configuration saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/config.json
[INFO|modeling_utils.py:2193] 2024-01-22 01:09:20,766 >> Model weights saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-22 01:09:20,768 >> tokenizer config file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-22 01:09:20,768 >> Special tokens file saved in /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/special_tokens_map.json
{'eval_loss': 5.042281150817871, 'eval_accuracy': 0.276, 'eval_macro_f1': 0.10057895576645577, 'eval_macro_precision': 0.09627648046398046, 'eval_macro_recall': 0.12210149835796386, 'eval_micro_f1': 0.276, 'eval_micro_precision': 0.276, 'eval_micro_recall': 0.276, 'eval_combined_score': 0.20327956208405715, 'eval_runtime': 7.5511, 'eval_samples_per_second': 66.215, 'eval_steps_per_second': 8.343, 'epoch': 14.0}
{'train_runtime': 2866.1609, 'train_samples_per_second': 279.119, 'train_steps_per_second': 8.722, 'train_loss': 3.4773580409458704, 'epoch': 14.0}
***** train metrics *****
  epoch                    =       14.0
  train_loss               =     3.4774
  train_runtime            = 0:47:46.16
  train_samples            =       4000
  train_samples_per_second =    279.119
  train_steps_per_second   =      8.722
01/22/2024 01:09:20 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:738] 2024-01-22 01:09:20,777 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:09:20,779 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-22 01:09:20,780 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:09:20,780 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:20,799 >> Initializing global attention on CLS token...
  0%|          | 0/63 [00:00<?, ?it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:20,969 >> Initializing global attention on CLS token...
  3%|▎         | 2/63 [00:00<00:04, 13.06it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,123 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,243 >> Initializing global attention on CLS token...
  6%|▋         | 4/63 [00:00<00:06,  9.82it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,362 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,481 >> Initializing global attention on CLS token...
 10%|▉         | 6/63 [00:00<00:06,  9.10it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,601 >> Initializing global attention on CLS token...
 11%|█         | 7/63 [00:00<00:06,  8.91it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,720 >> Initializing global attention on CLS token...
 13%|█▎        | 8/63 [00:00<00:06,  8.77it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,840 >> Initializing global attention on CLS token...
 14%|█▍        | 9/63 [00:00<00:06,  8.67it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:21,958 >> Initializing global attention on CLS token...
 16%|█▌        | 10/63 [00:01<00:06,  8.59it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,078 >> Initializing global attention on CLS token...
 17%|█▋        | 11/63 [00:01<00:06,  8.49it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,199 >> Initializing global attention on CLS token...
 19%|█▉        | 12/63 [00:01<00:06,  8.46it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,318 >> Initializing global attention on CLS token...
 21%|██        | 13/63 [00:01<00:05,  8.42it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,438 >> Initializing global attention on CLS token...
 22%|██▏       | 14/63 [00:01<00:05,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,557 >> Initializing global attention on CLS token...
 24%|██▍       | 15/63 [00:01<00:05,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,676 >> Initializing global attention on CLS token...
 25%|██▌       | 16/63 [00:01<00:05,  8.39it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,797 >> Initializing global attention on CLS token...
 27%|██▋       | 17/63 [00:01<00:05,  8.19it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:22,925 >> Initializing global attention on CLS token...
 29%|██▊       | 18/63 [00:02<00:05,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,045 >> Initializing global attention on CLS token...
 30%|███       | 19/63 [00:02<00:05,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,164 >> Initializing global attention on CLS token...
 32%|███▏      | 20/63 [00:02<00:05,  8.23it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,288 >> Initializing global attention on CLS token...
 33%|███▎      | 21/63 [00:02<00:05,  8.22it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,444 >> Initializing global attention on CLS token...
 35%|███▍      | 22/63 [00:02<00:05,  7.57it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,566 >> Initializing global attention on CLS token...
 37%|███▋      | 23/63 [00:02<00:05,  7.79it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,686 >> Initializing global attention on CLS token...
 38%|███▊      | 24/63 [00:02<00:04,  7.94it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,806 >> Initializing global attention on CLS token...
 40%|███▉      | 25/63 [00:02<00:04,  8.05it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:23,926 >> Initializing global attention on CLS token...
 41%|████▏     | 26/63 [00:03<00:04,  8.13it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,046 >> Initializing global attention on CLS token...
 43%|████▎     | 27/63 [00:03<00:04,  8.18it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,167 >> Initializing global attention on CLS token...
 44%|████▍     | 28/63 [00:03<00:04,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,286 >> Initializing global attention on CLS token...
 46%|████▌     | 29/63 [00:03<00:04,  8.28it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,405 >> Initializing global attention on CLS token...
 48%|████▊     | 30/63 [00:03<00:03,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,525 >> Initializing global attention on CLS token...
 49%|████▉     | 31/63 [00:03<00:03,  8.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,647 >> Initializing global attention on CLS token...
 51%|█████     | 32/63 [00:03<00:03,  8.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,767 >> Initializing global attention on CLS token...
 52%|█████▏    | 33/63 [00:03<00:03,  8.31it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:24,886 >> Initializing global attention on CLS token...
 54%|█████▍    | 34/63 [00:04<00:03,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,005 >> Initializing global attention on CLS token...
 56%|█████▌    | 35/63 [00:04<00:03,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,124 >> Initializing global attention on CLS token...
 57%|█████▋    | 36/63 [00:04<00:03,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,244 >> Initializing global attention on CLS token...
 59%|█████▊    | 37/63 [00:04<00:03,  8.09it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,377 >> Initializing global attention on CLS token...
 60%|██████    | 38/63 [00:04<00:03,  8.17it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,497 >> Initializing global attention on CLS token...
 62%|██████▏   | 39/63 [00:04<00:02,  8.22it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,617 >> Initializing global attention on CLS token...
 63%|██████▎   | 40/63 [00:04<00:02,  8.27it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,736 >> Initializing global attention on CLS token...
 65%|██████▌   | 41/63 [00:04<00:02,  8.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,856 >> Initializing global attention on CLS token...
 67%|██████▋   | 42/63 [00:05<00:02,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:25,979 >> Initializing global attention on CLS token...
 68%|██████▊   | 43/63 [00:05<00:02,  8.26it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,099 >> Initializing global attention on CLS token...
 70%|██████▉   | 44/63 [00:05<00:02,  7.14it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,284 >> Initializing global attention on CLS token...
 71%|███████▏  | 45/63 [00:05<00:02,  7.46it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,403 >> Initializing global attention on CLS token...
 73%|███████▎  | 46/63 [00:05<00:02,  7.71it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,523 >> Initializing global attention on CLS token...
 75%|███████▍  | 47/63 [00:05<00:02,  7.89it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,643 >> Initializing global attention on CLS token...
 76%|███████▌  | 48/63 [00:05<00:01,  8.03it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,763 >> Initializing global attention on CLS token...
 78%|███████▊  | 49/63 [00:05<00:01,  8.13it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:26,882 >> Initializing global attention on CLS token...
 79%|███████▉  | 50/63 [00:06<00:01,  8.20it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,001 >> Initializing global attention on CLS token...
 81%|████████  | 51/63 [00:06<00:01,  8.27it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,120 >> Initializing global attention on CLS token...
 83%|████████▎ | 52/63 [00:06<00:01,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,238 >> Initializing global attention on CLS token...
 84%|████████▍ | 53/63 [00:06<00:01,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,357 >> Initializing global attention on CLS token...
 86%|████████▌ | 54/63 [00:06<00:01,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,476 >> Initializing global attention on CLS token...
 87%|████████▋ | 55/63 [00:06<00:00,  8.38it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,595 >> Initializing global attention on CLS token...
 89%|████████▉ | 56/63 [00:06<00:00,  8.39it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,714 >> Initializing global attention on CLS token...
 90%|█████████ | 57/63 [00:06<00:00,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,832 >> Initializing global attention on CLS token...
 92%|█████████▏| 58/63 [00:06<00:00,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:27,951 >> Initializing global attention on CLS token...
 94%|█████████▎| 59/63 [00:07<00:00,  8.42it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,069 >> Initializing global attention on CLS token...
 95%|█████████▌| 60/63 [00:07<00:00,  8.42it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,188 >> Initializing global attention on CLS token...
 97%|█████████▋| 61/63 [00:07<00:00,  8.42it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,304 >> Initializing global attention on CLS token...
 98%|█████████▊| 62/63 [00:07<00:00,  8.47it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,418 >> Initializing global attention on CLS token...
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/local/xiaowang/anaconda3/envs/LJP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
100%|██████████| 63/63 [00:07<00:00,  8.36it/s]
***** eval metrics *****
  epoch                   =       14.0
  eval_accuracy           =      0.266
  eval_combined_score     =     0.1926
  eval_loss               =     4.9026
  eval_macro_f1           =     0.0857
  eval_macro_precision    =     0.0766
  eval_macro_recall       =     0.1217
  eval_micro_f1           =      0.266
  eval_micro_precision    =      0.266
  eval_micro_recall       =      0.266
  eval_runtime            = 0:00:07.71
  eval_samples            =        500
  eval_samples_per_second =     64.768
  eval_steps_per_second   =      8.161
01/22/2024 01:09:28 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:738] 2024-01-22 01:09:28,504 >> The following columns in the test set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement. If relevant_article, defendant_ls, imprisonment, sentence, fact, verdict, defendant, accusation, article_content, province, date, defendant_judgement are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-01-22 01:09:28,506 >> ***** Running Prediction *****
[INFO|trainer.py:3160] 2024-01-22 01:09:28,506 >>   Num examples = 500
[INFO|trainer.py:3163] 2024-01-22 01:09:28,506 >>   Batch size = 8
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,518 >> Initializing global attention on CLS token...
  0%|          | 0/63 [00:00<?, ?it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,636 >> Initializing global attention on CLS token...
  3%|▎         | 2/63 [00:00<00:03, 16.96it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,755 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:28,893 >> Initializing global attention on CLS token...
  6%|▋         | 4/63 [00:00<00:05,  9.93it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,015 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,135 >> Initializing global attention on CLS token...
 10%|▉         | 6/63 [00:00<00:06,  9.09it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,257 >> Initializing global attention on CLS token...
[INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,376 >> Initializing global attention on CLS token...
 13%|█▎        | 8/63 [00:00<00:06,  8.80it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,495 >> Initializing global attention on CLS token...
 14%|█▍        | 9/63 [00:00<00:06,  8.71it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,614 >> Initializing global attention on CLS token...
 16%|█▌        | 10/63 [00:01<00:06,  8.64it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,732 >> Initializing global attention on CLS token...
 17%|█▋        | 11/63 [00:01<00:06,  8.59it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,851 >> Initializing global attention on CLS token...
 19%|█▉        | 12/63 [00:01<00:05,  8.55it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:29,969 >> Initializing global attention on CLS token...
 21%|██        | 13/63 [00:01<00:05,  8.51it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,088 >> Initializing global attention on CLS token...
 22%|██▏       | 14/63 [00:01<00:05,  8.48it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,208 >> Initializing global attention on CLS token...
 24%|██▍       | 15/63 [00:01<00:05,  8.46it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,326 >> Initializing global attention on CLS token...
 25%|██▌       | 16/63 [00:01<00:05,  8.46it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,493 >> Initializing global attention on CLS token...
 27%|██▋       | 17/63 [00:01<00:06,  7.52it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,614 >> Initializing global attention on CLS token...
 29%|██▊       | 18/63 [00:02<00:05,  7.74it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,733 >> Initializing global attention on CLS token...
 30%|███       | 19/63 [00:02<00:05,  7.93it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,852 >> Initializing global attention on CLS token...
 32%|███▏      | 20/63 [00:02<00:05,  8.07it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:30,971 >> Initializing global attention on CLS token...
 33%|███▎      | 21/63 [00:02<00:05,  8.17it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,090 >> Initializing global attention on CLS token...
 35%|███▍      | 22/63 [00:02<00:04,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,209 >> Initializing global attention on CLS token...
 37%|███▋      | 23/63 [00:02<00:04,  8.29it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,327 >> Initializing global attention on CLS token...
 38%|███▊      | 24/63 [00:02<00:04,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,447 >> Initializing global attention on CLS token...
 40%|███▉      | 25/63 [00:02<00:04,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,607 >> Initializing global attention on CLS token...
 41%|████▏     | 26/63 [00:03<00:04,  7.58it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,726 >> Initializing global attention on CLS token...
 43%|████▎     | 27/63 [00:03<00:04,  7.81it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,845 >> Initializing global attention on CLS token...
 44%|████▍     | 28/63 [00:03<00:04,  7.98it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:31,964 >> Initializing global attention on CLS token...
 46%|████▌     | 29/63 [00:03<00:04,  8.11it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,083 >> Initializing global attention on CLS token...
 48%|████▊     | 30/63 [00:03<00:04,  8.17it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,203 >> Initializing global attention on CLS token...
 49%|████▉     | 31/63 [00:03<00:03,  8.18it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,326 >> Initializing global attention on CLS token...
 51%|█████     | 32/63 [00:03<00:03,  8.24it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,444 >> Initializing global attention on CLS token...
 52%|█████▏    | 33/63 [00:03<00:03,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,563 >> Initializing global attention on CLS token...
 54%|█████▍    | 34/63 [00:04<00:03,  8.31it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,684 >> Initializing global attention on CLS token...
 56%|█████▌    | 35/63 [00:04<00:03,  8.32it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,802 >> Initializing global attention on CLS token...
 57%|█████▋    | 36/63 [00:04<00:03,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:32,921 >> Initializing global attention on CLS token...
 59%|█████▊    | 37/63 [00:04<00:03,  8.37it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,040 >> Initializing global attention on CLS token...
 60%|██████    | 38/63 [00:04<00:02,  8.38it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,159 >> Initializing global attention on CLS token...
 62%|██████▏   | 39/63 [00:04<00:02,  8.39it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,278 >> Initializing global attention on CLS token...
 63%|██████▎   | 40/63 [00:04<00:02,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,397 >> Initializing global attention on CLS token...
 65%|██████▌   | 41/63 [00:04<00:02,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,516 >> Initializing global attention on CLS token...
 67%|██████▋   | 42/63 [00:04<00:02,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,634 >> Initializing global attention on CLS token...
 68%|██████▊   | 43/63 [00:05<00:02,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,753 >> Initializing global attention on CLS token...
 70%|██████▉   | 44/63 [00:05<00:02,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,873 >> Initializing global attention on CLS token...
 71%|███████▏  | 45/63 [00:05<00:02,  8.39it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:33,992 >> Initializing global attention on CLS token...
 73%|███████▎  | 46/63 [00:05<00:02,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,111 >> Initializing global attention on CLS token...
 75%|███████▍  | 47/63 [00:05<00:01,  8.40it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,230 >> Initializing global attention on CLS token...
 76%|███████▌  | 48/63 [00:05<00:01,  7.78it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,380 >> Initializing global attention on CLS token...
 78%|███████▊  | 49/63 [00:05<00:01,  7.96it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,499 >> Initializing global attention on CLS token...
 79%|███████▉  | 50/63 [00:05<00:01,  8.09it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,618 >> Initializing global attention on CLS token...
 81%|████████  | 51/63 [00:06<00:01,  8.19it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,737 >> Initializing global attention on CLS token...
 83%|████████▎ | 52/63 [00:06<00:01,  8.26it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,856 >> Initializing global attention on CLS token...
 84%|████████▍ | 53/63 [00:06<00:01,  8.30it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:34,974 >> Initializing global attention on CLS token...
 86%|████████▌ | 54/63 [00:06<00:01,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,094 >> Initializing global attention on CLS token...
 87%|████████▋ | 55/63 [00:06<00:00,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,213 >> Initializing global attention on CLS token...
 89%|████████▉ | 56/63 [00:06<00:00,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,333 >> Initializing global attention on CLS token...
 90%|█████████ | 57/63 [00:06<00:00,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,453 >> Initializing global attention on CLS token...
 92%|█████████▏| 58/63 [00:06<00:00,  8.34it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,572 >> Initializing global attention on CLS token...
 94%|█████████▎| 59/63 [00:07<00:00,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,692 >> Initializing global attention on CLS token...
 95%|█████████▌| 60/63 [00:07<00:00,  8.36it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,811 >> Initializing global attention on CLS token...
 97%|█████████▋| 61/63 [00:07<00:00,  8.35it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:35,929 >> Initializing global attention on CLS token...
 98%|█████████▊| 62/63 [00:07<00:00,  8.41it/s][INFO|modeling_longformer.py:1920] 2024-01-22 01:09:36,043 >> Initializing global attention on CLS token...
100%|██████████| 63/63 [00:07<00:00,  8.42it/s]
01/22/2024 01:09:36 - INFO - __main__ - ***** Predict results *****
01/22/2024 01:09:36 - INFO - __main__ - Predict results saved at /local/xiaowang/LJP Task/baseline_output/Task2/Data4/lawformer_lr7/thunlp_Lawformer_predict_results.txt
[INFO|modelcard.py:452] 2024-01-22 01:09:37,162 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.266}]}
wandb: - 0.028 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb: | 0.028 MB of 0.028 MB uploadedwandb: / 0.028 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.646 MB uploadedwandb: \ 0.028 MB of 0.646 MB uploadedwandb: | 0.646 MB of 0.646 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▃▃▄▅▅▆▆▆▇▇███▇
wandb:            eval/combined_score ▁▃▃▄▅▅▆▆▆▇▇███▇
wandb:                      eval/loss █▅▄▃▃▂▂▂▂▂▁▁▁▂▁
wandb:                  eval/macro_f1 ▁▂▂▃▄▄▅▅▅▆▇█▇█▇
wandb:           eval/macro_precision ▁▂▂▃▃▃▄▄▅▅▆█▇█▆
wandb:              eval/macro_recall ▁▃▃▄▅▅▆▆▆▇▇█▇▇▇
wandb:                  eval/micro_f1 ▁▃▃▄▅▅▆▆▆▇▇███▇
wandb:           eval/micro_precision ▁▃▃▄▅▅▆▆▆▇▇███▇
wandb:              eval/micro_recall ▁▃▃▄▅▅▆▆▆▇▇███▇
wandb:                   eval/runtime ▂▅▂▂▇▃█▃▅▂▁▅▃▁▅
wandb:        eval/samples_per_second ▇▄▇▇▂▆▁▆▄▇█▄▆█▄
wandb:          eval/steps_per_second ▇▄▇▇▂▆▁▆▄▇█▄▆█▄
wandb:                    train/epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████
wandb:              train/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████
wandb:            train/learning_rate █▇▇▆▆▅▅▄▄▃▃▂▂▁
wandb:                     train/loss █▇▆▅▅▄▄▃▃▂▂▂▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.266
wandb:            eval/combined_score 0.19258
wandb:                      eval/loss 4.90262
wandb:                  eval/macro_f1 0.0857
wandb:           eval/macro_precision 0.07662
wandb:              eval/macro_recall 0.12174
wandb:                  eval/micro_f1 0.266
wandb:           eval/micro_precision 0.266
wandb:              eval/micro_recall 0.266
wandb:                   eval/runtime 7.7199
wandb:        eval/samples_per_second 64.768
wandb:          eval/steps_per_second 8.161
wandb:                    train/epoch 14.0
wandb:              train/global_step 1750
wandb:            train/learning_rate 7e-05
wandb:                     train/loss 1.6319
wandb:               train/total_flos 1.8605629980672e+16
wandb:               train/train_loss 3.47736
wandb:            train/train_runtime 2866.1609
wandb: train/train_samples_per_second 279.119
wandb:   train/train_steps_per_second 8.722
wandb: 
wandb: 🚀 View run lawformer_data4_t2_bs32_lr7e-5 at: https://wandb.ai/loss4wang/LJP_baselines_task2/runs/gv2w0i3e
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240122_002135-gv2w0i3e/logs
